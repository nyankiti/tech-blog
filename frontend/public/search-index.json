[{"slug":"2025-02-26-next-flexsearch","title":"flexsearch を用いて Next.js でサイト内検索を実装する","tags":["tech","poem"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n本ブログは Next.js で Markdwon をソースとしたブログなのですが、 サイト内検索が欲しくなったので追加しました。 \n\nNextra が提供しているサイト内検索が flexsearch を利用しているとのことなので、 nextra の実装を参考にしながら自分のブログにも実装してみます。 \nhttps://nextra.site/docs/guide/search\n\nhttps://github.com/shuding/nextra\n\nビルドの際に Markdwon から簡易的にインデックスを作成し、 ブラウザで完結するサイト内検索ができれば良いなと思い色々頑張ったので、 その記録を残します。 \n\nFlexsearch とは\n\nhttps://github.com/nextapps-de/flexsearch\n\njavascript で動作する高速の全文検索ライブラリです。 Node.js に加えてブラウザでも動作するので、 ビルドの際にインデックスを作成し、 ブラウザで完結するサイト内検索が可能になります。 \n\nFlexsearch の特徴\n","date":"2025-02-26T10:34:20.000Z"},{"slug":"2025-02-26-next-mdx-bundler-blog","title":"Next.js と markdown のブログで contentlayer から mdx-bundler に移行した話","tags":["tech","next","contentlayer","mdx","markdown"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\n本ブログで利用していた contentlayer がメンテナンスを停止したので、 mdx-bundler へ移行しました。 \n\n現状の記事管理では mdx を使っているわけではないのですが、 これを機に表現の幅を広げられたら嬉しいなと思い mdx-bundler を選定しました。 \n\napp ディレクトリ配下ではない場所で markdown ファイルを管理しているため、 Next.js 公式にある mdx の利用方法はあまり参考にならず、 自前でゴリゴリ実装する羽目になりましたがなんとか実現できたのでその記録を残します。 \n\ncontentlayer から移行の際の注意点 にあるように、 contentlayer っぽい作りをそのまま利用することはできないといのでこれから実施する人は注意してください。 \n\n実装紹介\n\n記事一覧取得部分\n\n記事一覧は markdown の frontmatter の情報のみが必要なため、 mdx-bundler は利用せず、 gray-matter を用いて記事の情報を取得しました。 \n\nbundleMDX の設定\n\nmarkdown 内で画像を利用しているため、 remarkMdxImages の適用と、 esbuildOptions にて、 Next.js の public フォルダに画像を配置するように設定しました。 \n\n記事詳細ページ\n\napp/blog/\\[slug]/page.tsx\n\napp/blog/\\[slug]/MDXComponent.tsx\n\nこちらを参考に、 code のハイライト等のクライアント側での custom components を利用できるような設定を実施しています。 \n\ncontentlayer から移行の際の注意点\n\ncontentlayer の場合は、 .contentlayer 配下に markdown ファイルの情報をまとめた json を作成し、 ビルド生成物としてバンドルしてしまうので、 SSR の際にファイルシステムへアクセスせずとも記事情報を取得することで来ていました。 \n\nmdx-bundler のみで同じような機能を実装しようとすると、 SSR の際にファイルアクセスしてエラーが出るので、 SSG のみにしておくか、 ビルド時にファイルを読み込むようにしておく必要があるということです。 \n\n自分の場合、 md ファイルを読み込む必要がある /blog/\\[slug]/page.tsx については以下の設定を適用し、 SSG のみにする対応としました。 \n\nhttps://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config\n\n⚠️ fetch で revalidate を用いている ISR は利用できません。 file system 参照でエラーになると思うので削除してください。 自分は fetch の revalidate している部分があることに忘れて 1 日ほどハマりました。 。 。 \n\n@next/mdx を利用しなかった理由\n\nブログ記事を別の markdown のみのリポジトリで管理していたからです。 \n@next/mdx の場合、 Next.js のお作法に従って mdx ファイルを app フォルダ配下に配置する必要があり、 ブログ記事管理とフロントエンドの実装が疎結合になるのが気に入りませんでした。 \n\n参考: https://nextjs.org/docs/pages/building-your-application/configuring/mdx\n\nnext-mdx-remote を利用しなかった理由\n\nesbuild を dependency に含めてしまって,mdx 内の import を解決してくれるのが嬉しいです。 \nmarkdown で記事を管理するリポジトリに Component を配置することができるので、 記事の管理をフロントエンドの実装に依存させることなく管理できます。 \n\n最後に\n\nmdx-bundler では以下のようにわかりやすい感じで markdown に Bookmark(Linkcard)を配置することができて良い感じだなと🎉\n\n表示結果↓\n\n\nmdx に移行する以前は、 Bookmark(Linkcard)を表示するために以下のようななんちゃっての unified プラグインを作って頑張っていましたが、 とてもシンプルになってとてもほっこりです。 \n\ncontentlayer と mdx-bundler は同じ役割ではないので、 厳密には移行とは言えないと注意されそうなので補足しておくと、 contentlayer 利用時のレンダリングには react-markdwon を利用していました。 \n","date":"2025-02-26T10:44:36.000Z"},{"slug":"2025-03-01-my-dotfiles","title":"dotfiles の育成 in 2025【mise, sheldon, raycast など】","tags":["tech","dotfiles","mise","sheldon","raycast","fzf","shell"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\nPC をリプレイス機会があったので dotfiles を整理しました。 \n\nある程度調査をして、 自分が使いこなせる範囲の最低限整えるための dotfiles といった感じです。 \n「rust 製のツールで揃えてとにかく早い環境を作る」みたいないなこだわりも特にありません。 \n\nリポジトリは以下になります。 \nhttps://github.com/nyankiti/dotfiles\n\n2023 時点からの変化\n\nmise\n\nあらゆる言語を管理できるツールを利用たことがなかったのでこれを機に入門しました。 \nグローバルに用いる実行環境については全て mise へ集約できるようにしました。 \n\nそれぞれの言語で個別で PATH を通していたものが以下で集約され、 .zshrc がとてもすっきりしました。 \n\nzinit => Sheldon\n\nSheldon が早い & toml ファイルで設定を管理できると噂を聞いていたのでこれを機に入門しました。 \n\nzinit が不安なので sheldon へ移行したら zsh の起動が 50ms と更に速くなった 等を見ると遅延読み込みの設定を書いていました。 興味のある人はこちらも試すと良いと思います。 \n\n自分は上記の遅延読み込みを入れると kube-ps1 の$PROMPT のカスタマイズが初回から読み込めないという問題があったので利用しませんでした。 \n\npure は最初に入れた zsh theme で思い入れがあるのかずっと利用しています。 \n\nalfred, clipy => raycast\n\nraycast は 1 年以上使っていましたが、 dotfiles に落とし込めていなかったので追加しました。 \nraycast の設定のエクスポート/インポートができるようで、 詳細は以下を参照ください。 通常の設定に加えてスニペットなどの情報も引き継げるようです。 \n\n\n個人的には raycast は alfred の完全上位互換と捉えており、 とても重宝しています。 \n\npeco => fzf\n\npeco で特に不満はなかったのですが、 色々と fzf の方が優れている気はしていたので乗り換えました。 \nといっても現状以下のコマンド履歴検索を利用している程度です。 もっと色々カスタマイズできるはずなので、 これから色々試していきたいです。 \n\n利用例↓\nalt text\n\n.gitconfig\n\n.gitconfig による以下の設定を dotfiles に追加。 git pull した時の merge strategy の指定がないぞと言われるあのワーニングへの対策です\n\nGitHub Actions で再現性の担保\n\nmacos-latest 環境にて setup.sh と source .zshrc が正常終了することを GitHub Actions で担保しました。 \n\n最後に\n\nmise, sheldon, fzf は初めて触ることもあり、 結局 1 日くらい格闘していました。 \nCursor, Warp 等の生成 AI 周りのツールの環境整備も dotfiles に落とし込みたかったのですが、 今回は定番どころを押さえていくだけで時間がすごいかかったのでまた別の機会にしたいと思います。 \n\n今回メインで参考にさせていただいた記事です🙏\n\n2024 年度 わたしの dotfiles を紹介します\n\nhttps://github.com/ayuukumakuma/dotfiles\n\nMac の環境を dotfiles でセットアップしてみた改\n\ndotfiles で再構築可能なターミナル環境構築を目指してみた\n","date":"2025-03-01T02:31:36.000Z"},{"slug":"2025-03-02-open-ai-batch-api","title":"Open AI の Batch API を利用して embedding を 50%割引で実施する【node.js】","tags":["tech","node.js","openai","prisma"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nOpenAI の Batch API とは\n\nOpenAI へのリクエストを非同期で処理し、 コストを削減するための機能です。 通常の API と比較して 50%割引となっており、 コストパフォーマンスに優れています。 \n\n複数のリクエストを JSON Lines（.jsonl）形式のファイルとして書き出して送信し、 最大 24 時間以内に結果の .jsonl ファイルを取得することができます。 \n\n公式 Doc では以下のようなユースケースが紹介されています。 \n\n評価の実行\n\n大規模データセットの分類\n\nコンテンツの embedding\n\n今回は RAG を組む際のデータの embedding に利用しました。 \n基本的にはリアルタイム性が必要のない利用であればどのような用途でも利用可能だと思います。 \n\nBatch API の利用手順は以下\n\n入力データの作成: 処理したいプロンプトを JSON Lines（.jsonl）形式のファイルにまとめます。 \n\nファイルのアップロード: 作成した.jsonl ファイルを OpenAI にアップロードします。 \n\nバッチジョブの開始: アップロードしたファイルに対してバッチ処理を実行するジョブを作成します。 \n\n結果の取得: 処理が完了したら、 結果をダウンロードして確認します。 \n\n今回は node.js を用いてリクエスト用のファイルを作成し、 結果を受け取って DB に保存するまでの処理を紹介しようと思います。 \n\nやったことまとめ\n\nリクエストをまとめた.jsonl ファイルを作成する node.js スクリプトを作成\n\nopenAI の Batch API ダッシュボードに.jsonl ファイルをアップロード\n\n処理が完了するまで最大 24 時間待機\n\nopenAI の Batch API ダッシュボードから結果の.jsonl ファイルをダウンロード\n\nnode.js スクリプトで結果.jsonl ファイルを読み込み、 結果を postgres に格納\n\nなぜ node.js を用いたか？ \n\nnode.js を利用したアプリの一環で作成したバッチ処理だったためです。 prisma クライアントの生の SQL を実行するための queryRawTyped を利用しているので、 どの言語、 DB クライアントを用いても同じだと思います。 \n\nPython の環境がある場合は Python でやってしまった方が良いと思います。 \n\nなぜ openAI クライアントの batchs を利用しないのか？ \n\n最初はリクエスト/レスポンスのファイルの中身を見ながら利用したなと感じたからです。 そのうち openAI クライアント利用に移行すると思います。 openAI クライアントを利用するとアプリケーション側では batchId を管理するだけで良くなりそうです。 \n\n利用モデル\n\n2025/02/22 時点で最もコスパの良さそうな text-embedding-3-small を利用しました。 \n\nhttps://openai.com/ja-JP/index/new-embedding-models-and-api-updates/\n\n実装紹介\n\nリクエストをまとめた.jsonl ファイルを作成する node.js スクリプトを作成\n\n作成されたファイル\n\nBatch API ダッシュボードに.jsonl ファイルをアップロード\n\nhttps://platform.openai.com/batches/\n\nalt text\n\nembedding に利用するので Endpoint は /v1/embeddings を選択する必要があります。 Completion window は現在 24 hours しか選択できないようです。 \n\nアップロード後、 処理が完了するまで最大 24 時間待ちます。 \n\nファイルの大きさやタイミングによると思いますが、 自分は 3 時間かかることもあればが、 15 分で終わることもあありました。 \n\nBatch API ダッシュボードから結果の.jsonl ファイルをダウンロード\n\n処理が完了すると以下のような UI となり「Download output」より結果をダウンロードできます\n\nalt text\n\nダウンロードされたファイル\n\nnode.js スクリプトでダウンロードした.jsonl ファイルを読み込み、 結果を postgres に格納\n\nembeddingBulkInsert の SQL は以下です。 \n\nPrisma の TypedSQL を利用する際は以下が参考になります。 利用する前に prisma generate --sql を実行する必要があります。 \n","date":"2025-03-01T18:36:53.000Z"},{"slug":"apache-pulsar-catchup","title":"Apache Pulsar をローカルで立ち上げ、 Spring for Apache Pulsar を動作確認するメモ","tags":["tech","java","pulsar"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nApache Pulsar とは\n\nリアルタイムのデータストリーミングとメッセージングを扱うためのオープンソースの分散メッセージングシステム。 Apache Kafka に比べ、 柔軟性や拡張性が求められるユースケースに適しています。 マルチテナントの運用やスケーラブルなストレージ管理が必要な場合に特に有利。 \n\nトピック名が階層化されておりマルチテナントしやすい（プロパティ/ネームスペース/デスティネーション でトピック名が階層化されている）\n\nまた、 ジオレプリケーション機能が備わっている。 \n\nspring プロジェクトでは、 2022 年から apache pulsar のサポートを開始している https://spring.io/blog/2022/08/16/introducing-experimental-spring-support-for-apache-pulsar\n\n参考\n\n公式 doc\n\nApache Pulsar のマネージメントサービスである Astra Streaming を提供する DataStax 者の方による記事。 メッセージング及びストリーミングテクノロジーの概観から始まり、 Apache Kafka との違いを取り上げながら Apache Pulsar についてわかりやすく解説されている。 \n\nYahoo!デベロッパーネットワークが公開している以下のスライド群\nhttps://www.docswell.com/tag/Apache Pulsar\n\nbaeldung の解説\nhttps://www.baeldung.com/apache-pulsar\nhttps://www.baeldung.com/spring-boot-apache-pulsar\n\nApache Pulsar をローカルで立ち上げ、 Spring for Apache Pulsar を動作確認する\n\npulsar を single node で立ち上げる\n\n公式の Apache Pulsar distribution をダウンロードして解凍\n\n解凍された プロジェクトに移動し、 standalone pulsar を start する\n\n参考: https://pulsar.apache.org/docs/4.0.x/getting-started-standalone/\n\nトピックを作成する\n\n※ pulsar は自動的に存在しないトピックを作成する機能があるが、 あえて明示的に前もってトピックを作成している\n\nspring の pulsar client でメッセージの produce/consume する\n\nspring initializr より、 Dependencies に Spring for Apache Pulsar を選択してプロジェクトを作成する\n\n※ pulsar はデフォルトの localhost:6650 で実行されている想定のため、 追加の設定はしていない\n\n以下コマンドでメッセージを手動で produce することで動作確認できる\n","date":"2024-11-30T00:00:00.000Z"},{"slug":"create-kubernetes-operator","title":"k8s 入門者が超シンプルな Kubernetes Operator を作って k8s の理解を深める","tags":["kubernetes","tech"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\nkubernetes の理解が甘い気がすると漠然と上司に相談したところ、 Kubernetes Operator の自作を勧められたので奮闘した記録です。 \n\n現在の Kubernetes についてのレベル感としては「 Kubernetes 完全ガイド を辞書代わりに必死に調べながらなんとかな既にある yaml ファイルのチューニングっぽいことができる。 」くらいです。 \n\napi-server があり、 etcd という永続化層があり、 kubectl すると api-server が叩かれているということはなんとなく知っているが実態としてどのように pod たちが管理しているかはあまりわかっていないという状況でした。 \n\n今思うと、 新卒になって買った技術書の中で最も Kubernetes 完全ガイド を開いている回数が多い気がする（開いている回数は多いが通読して理解しているわけではない🤪）\n\nKubernetes Operator とは\n\nkubernetes が持つ制御ループという仕組みを使った拡張機能。 公式 Doc の Operator Pattern の部分に説明がある。 \n\n制御ループ (Control Loop, Reconcile Loop とも呼ばれる)\n\n常に制御の対象となるものを監視し、 理想状態に近づける仕組み。 \n\nKubernetes Controller 内で使われる仕組み。 （Kubernetes Controller はリソースを監視して、 登録された理想状態に近づける責務を持っている。  https://kubernetes.io/ja/docs/concepts/architecture/controller/）\n\n「監視 → 分析 → アクション」のサイクルを繰り返し、 現実状態（Actual State, Current State）を理想状態（Desired State）に近づける\n\nロボット工学やオートメーション分野で使われている\n\nController とは\n\nController は Controller Manager の中に束ねられている\n\nController は一つ以上のリソースを監視\n\nController は理想状態と現実状態を近づける責務を持つ\n\n理想状態: API server から取得した Kubernetes オブジェクトの spec フィールド\n\n現実状態: API server から取得できる実際の状態\n\nKubernetes アーキテクチャにおける Controller Manager(c-m)の立ち位置を以下で参考にできます。 etcd に保存されている理想状態に関する情報や、 実際の Node の情報を API server を通じて取得している。 \n\nalt text\n\n出展: https://kubernetes.io/docs/concepts/overview/components/\n\nbuild-in controller: Controller Manager の中にデフォルトで含まれている Controller。 Deployment, ReplicaSet, Pod などの build-in Resource を管理している\n\nkubectl の内部の動き\n\nkuectl コマンドを実行する\nkubectl create deploy  test —image=nginx —replicas=1\n\nコマンドを API server に対する POST リクエストに変換する\nPOST apis/apps/v1/namaspaces/default/deployments\n\nAPI server が deployment のインスタンスを作成し、 etcd に保存する（API server 自体は保存するだけで ReplicaSet の起動等、 その先の動作を行わない）\n\nDeployment Controller の制御ループにより、 新たに deployment インスタンスが作成されたことを検知し、 Deployment Controller が新たな deployment インスタンスに対する ReplicaSet を作成する\n\nReplicaSet Controller の制御ループにより、 新たに replicaSet インスタンスが作成されたことを検知し、 ReplicaSet Controller が新たな replicaSet に対する Pod を作成する\n\nKubernetes Operator とは（結論）\n\nCustom Resource と Custom Controller を自分で作成し、 Kubernete のコンセプトに則って自分のロジックを実現する拡張機能。 以下の 2 step で作成することができる。 \n\nAPI の拡張: Custom Resource の定義・追加（ここで作成する Custom Resource Definition は CRD と略される）\n\n制御ループの追加: Custom Controller の追加\n\nOperator Hub\n\nhttps://operatorhub.io/\n\n様々な Kubernetes Operator が公開されている。 prometheus operator 等、 皆さんがよくお世話になっている Operator についてお調べることができる。 \n\nKubernetes Operator を使う手順\n\nCDR と Custom Controller の deploy をすることで Kubernetes Operator が使えるようになる。 \n\nKubernetes Operator インストール時にインストールされるリソースは以下。 \n\nCosutmResorceDefinition\n\nDeployment(Controller は Deplyment で実行されることが多い)\n\nController へ必要な権限を与えるためのリソース（RBAC）。 ServiceAccount, Role(Cluster Role), RoleBiding(Cluster RoleBiding)\n\n簡単な Kubernetes Operator を作ってみる\n\nCustom Resource を作成する\n\nsample.crd.yaml\n\nsample.yaml\n\nSample crd を作成し、 インスタンスを作成する\n\nCustom Controller を作成する\n\n簡単な Controller のため、 1 秒に一回制御ループを実行し、 custom resource のオブジェクトの一覧を表示する機能だけを持ちます。 \n\nNext Action\n\n今回作成した Kubernetes Operator には以下のような課題があります。 \n\ncustom resoruce オブジェクトが作成された場合に実施する、 pod 作成などのメインロジックが存在しない。 \n\nFor ループは API server への負荷となってしまう\n\nCRD と main.go 内で定義した Sample struct の整合性担保ができていない\n\nField のデフォルト値やバリデーションがない\n\nTest コードがない\n\nKubernetes クラスタ上で動かすためには、 API Server へのアクセス管理（RBAC）の考慮が必要\n\n…\n\n⇒ これらは全ての Kubernetes Operator を作る際の共通の課題なので、 解決のためのライブラリが提供されている。 ⇒ kubebuilder などを利用した本格的な Kubernetes Operator 作成が始まる。 \n\nまとめ\n\nなんとなく kubernetes の世界観がわかってきました。 今のところアプリケーションレイヤーが本業なのでこれ以上は一旦踏み込めないなと感じています。 業務でよく利用されている prometheus や argo-rollouts などの kubernetes operator のドキュメント等が読みやすくなったのがとても良いなと思います。 \n\n参考\n\nオペレーターパターン\n\nKubernetes Operator 超入門/Kubernetes\\_Operator\\_Introduction\n\nOSC2020 Online Fukuoka Kubernetes Operator Intuition\n\n以下の動画は超絶おすすめです。 本記事もこちらの動画の流れに沿った内容となっています m\n\nrepository: https://github.com/nakamasato/kubernetes-operator-basics/tree/main\n","date":"2024-12-16T00:00:00.000Z"},{"slug":"exec-py-by-uv-on-github-actions","title":"uv でプロジェクト管理している Python スクリプトを github actions で実行する","tags":["tech","python","uv","GithubActions"],"content":"uv でプロジェクト管理している Python スクリプトを GitHub Actions で実行する環境を整えたので紹介します。 \n\nGithub Actions は無料でちょっとしたスクリプトの定期実行にとても便利なので重宝しています。 \n\n今回は ChatGPT api を定期的に叩くタスクを想定して、 OpenAI の API Key を Github Secrets に格納して参照することろまで解説します。 \n\nプロジェクト概要\n\nディレクトリ構造\n\npyproject.toml\n\nmain.py\n\n利用する setup テンプレート\n\nuv の開発元である Astral 社が以下で github actions 用の setup を提供しているのでこちらを利用します。 \n\nhttps://github.com/astral-sh/setup-uv\n\nワークフローyaml 全体\n\nコード全体は以下になります。 \n\nactions/checkout step でコードを clone し、  astral-sh/setup-uv step を入れることで簡単に uv 環境を整えることができます。 \n\nastral-sh/setup-uv を用いている場合、 GitHub Actions 上で python 環境を整える際によく利用する actions/setup-python を step に加える必要はないようです。 \nPython のバージョンを指定してたい場合は、 uv コマンド経由で指定することが可能です。 \n\n無事実行することができました\n\nalt text\n\n最後に\n\n雑談ですが uv の環境を整えていて思ったこと↓\n\n以下のようにあるので node.js プロジェクトにおける npm install 的な step を踏まなくて良いのは便利だなと\n\nWhen used in a project, the project environment will be created and updated before invoking the command.\n\nhttps://docs.astral.sh/uv/reference/cli/#uv-run\n","date":"2025-01-11T00:00:00.000Z"},{"slug":"ir-system-book","title":"【読書感想】『検索システム ― 実務者のための開発改善ガイドブック』メモ、 感想","tags":["bookreview","tech"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\n検索システムは、 現代のアプリケーションやウェブサービスにおいて不可欠な要素です。 SNS や EC サイトをはじめ、 大部分の web サービスには検索システムが実装されており、 効率的な検索体験を提供することは、 ユーザーの満足度や利便性に直結します。 \n\n今回は、 『検索システム ― 実務者のための開発改善ガイドブック』を読み、 その内容をまとめ、 私が感じたことを共有したいと思います。 本書は実務に即したアプローチで、 検索システムの設計や改善方法について解説しており、 開発者目線でとても有益な内容が詰まっています。 \n\n読書時のメモ\n\n第 1 章 イントロダクション\n\nRDB へのクエリをそのまま検索システムとして使おうとすると生じる「キーワードの表記揺」、 「ランキング」、 「レイテンシ」、 「フレーズ検索」について触れ、 全文検索エンジンの導入となる章。 \n\n検索の方式\n\n逐次方式: 検索対象から文字列のマッチングでキーワードを探し出す単純な手法。 unix の grep は逐次方式の検索。 \n\n牽引方式: データベースを構築する段階で「キーワードごとに対象となるドキュメントを並べた形式のデータ」を牽引として用意し、 （この牽引を転置インデックスと呼ぶ）転置インデックスを利用して高速にドキュメントを探す手法\n\n転置インデックスはデータ構造の一つ。 転置インデックスを格納して、 クエリで問い合わせるという性質上、 検索エンジンはデータベースの一種と言える。 \n\n検索機能を作成するために考慮する必要があること\n\nデータを検索エンジンにどのように格納するか（データモデリングやインデクシング）\n\nユーザーに検索機能をどのように使ってもらうか（UI/UX や検索を支援するための機能）\n\n検索機能に関係する多様な要素をどうやって管理していくか（プロジェクトマネジメント）\n\nさらに、 検索の歴史と検索システムの全体像と構成要素について前触れがある。 \n検索システムの構成要素として以下が紹介される。 \n\n検索エンジン\n\nインデクサ: 検索対象のデータの取得、 適切な形式への加工、 インデックスへの登録を担う\n\nクエリプロセッサ: ユーザーによって入力されたクエリの前処理を行う\n\nクエリポストプロセッサ: 検索結果の後加工を行う\n\n「データ活用基盤」および「適合性コントローラー（Relevance Controller）」: より良い検索を実現するため、 検索結果に対するユーザーのフィードバック、 サービス提供側のニーズに基づいて検索システムを最適化する仕組み\n\n（可能ならばここに図を置きたい）\n\n2 章から続くいくつかの章は「現代の検索エンジンがどのような課題を意識して実装されているか」を解説する章となる。 \n\n第 2 章 検索エンジンの仕組み\n\n主題: 「そもそも全文検索エンジンがどのようなソフトウェアなのか、 特に全文検索エンジンのデータ構造として最も広く使われている転置インデックスを利用した検索の大まかなフローを解説」\n\n転置インデックスとは\n\n前提として、 逐次検索では実現の難しい要件を整理する\n\n大規模テキストを高速に検索する\n\nフレーズ検索を行う（ある単語とある単語が隣り合うか、 または近くに出現するかのようなパターン検索）\n\n検索結果にランキングを付けたい\n\nこれらを実現する仕組みとして転置インデックス（inverted index）がある。 \n\nドキュメントを牽引語に分割し、 牽引語を行、 ドキュメント ID を列として、 「牽引語がドキュメントに出現すれば 1 、 出現しない場合は 0」のような行列が最も簡単な転置インデックス。 \n\n「Web ページのランキングアルゴリズム PageRank はラリー・ペイジらによって開発された」\n\n↓ 牽引語に分割\n\n「Web / ページ / の / ランキング / アルゴリズム / pagerank / は / ラリー / ・ / ペイジ/ ら / によって / 開発 / さ / れ / た」\n\n（牽引後の分割方法は第 3 章を参照。 形態素解析や N-Gram などのアルゴリズムが用いられることが一般的。 ）\n\n|        | Doc1 | Doc2 | Doc3 |\n| ------ | -------- | -------- | -------- |\n| web    | 1        | 1        | 0        |\n| 検索     | 1        | 1        | 1        |\n| アルゴリズム | 1        | 0        | 1        |\n| エンジン   | 1        | 1        | 1        |\n\nこのような行列の転置インデックスを用いると、 牽引語「web」が出現しつつ、 牽引語「検索」も出現する AND 検索や、 どちらかが出現する OR 検索、 出現しないことを指定する NOT 検索が可能になる（検索処理を集合演算に落とし込めるということ。 AND 検索、 OR 検索、 NOT 検索はまとめてブール検索モデルという。 ）\n\n上のような「牽引語-ドキュメント行列」の転置インデックスでは、 大規模なテキストコーパスではほとんどの要素が 0 になってしまいメモリ効率が悪くなる。 そのため、 実用的な転置インデックスとして、 \\*\\*ターム辞書（term dictionary）とポスティングリスト（posting list）\\*\\*を用いた転置インデックスがある。 \n\nターム辞書: は牽引語を key、 ポスティングリストを value としたデータ構造。 \n\nポスティングリスト: その牽引語が出現しているドキュメント ID と出現位置のリスト。 \n\n転置インデックスを用いた検索\n\nユーザーが検索のために入力した文字列（検索クエリ）をターム（牽引語）に分割する（テキスト解析）\n\nターム辞書を引き、 検索対象のポスティングリストを見つける（辞書引き）\n\nポスティングリストを先頭から走査してまーじ（ポスティングリスト走査） ポスティングリスト走査の詳細については第 4 章を参照\n\n見つかった検索結果をランキングにして返す（ランキング）\n\nこの一連の流れはクエリ処理（query processing）と呼ぶ。 \n\nテキストデータ以外のインデックス\n\n転置インデックスを利用すると、 テキストデータの全文検索が行えることがわかりました。 しかし、 検索システムでは全文検索のみでは成り立たず、 テキスト以外のデータの検索を組み合わせる必要があります。 \n\ne.g.) EC サイトの検索では、 商品のカテゴリや価格による絞り込み（フィルタリング）やソート、 ファセット機能（6 章を参考）を組み合わせた検索が必須\n\ne.g.) ホテルの空室検索では、 予約したい日付による検索が必須\n\ne.g.) レストランや店舗検索では、 ユーザーの地理情報を考慮した検索を実現したい\n\n無償で利用可能な検索エンジンの紹介\n\nApache Lucene\n単体ではサーバーとしての機能を持たないため、 Apache Lucene をベースとした検索エンジンサーバーである、 Apache Solr や Elasticsearch を用いることが多い\n\nVespa\n\nGroonga\n\n※ 転置インデックスの特徴\n\nインメモリでもディスクベースでも、 そのハイブリットでも動作する\n\n一台のマシーンで扱いきれない大規模なデータを扱う際に、 水平分散によりスケールしやすい\n\n第 3 章 テキスト解析\n\n主題:「全文検索エンジンにおいてテキストがどのように扱われるか」を扱う\n\nテキスト解説は大きな処理として考えると次のような流れになる\n\n文字の正規化\n\nトークン化\n\nトークン列に対する各種処理\n\n※ トークン（牽引語）はテキストを構成する基本的な単語や記号であり、 タームは検索の目的でインデックスされる特定の単語やフレーズを指します。 \n\n対象となるドキュメントの言語によってトークンの決め方は異なり、 それぞれに対して独自のアルゴリズムが発展している。 \n\n日本語のテキスト解析\n\n日本語のテキスト解析には以下の二つの手法が用いられることが多い\n\n形態素解析器を用いたテキスト解析\n\n日本語の文章を人間が認識できる単語へと区切る手法。 \nOSS: MeCab\n\n文字 N-Gram を用いたテキスト解析\n\nN-Gram では入力された文章を N 文字ずつ切り出して切り出してトークンとして出力する。 \n\nその他、 Unicode 正規化、 ストップワード、 類義語の展開など、 検索エンジンの精度を高めるためのテキスト解析の工夫うが紹介されている。 \n\n第 4 章 ポスティングリストの走査とランキングのアルゴリズム\n\n主題: 「転置インデックスからドキュメントを取得する際にどのようなアルゴリズムやロジックが使われているか」\n\nこれまでの章で全文検索エンジンの主な仕組みは、 転置インデックス上でテキストをタームの集合として扱い、 ターム集合から構築したポスティングリストを走査することで目的の検索結果を取得するというところまでわかりました。 本章ではポスティングリスト走査とランキングのアルゴリズムについて解説されています。 \n\nポスティングリストの走査\n\n図での解説が必要なため、 まとめられず。 \n\nランキングを考慮したポスティングリストの走査\n\nTF(term frequency)と IDF(inverse document frequency)を考慮しながらポスティングリストを走査することで、 走査時の結果が重み付けされランキングを考慮した結果となる。 \n\nパフォーマンスの評価\n\n検索システムにおいて重視されるのレイテンシ（Latency）とスループット（throughput）\n\nクエリ処理時はレイテンシのみが重要視され、 インデクシング時はレイテンシとスループットの両方が重視される。 \n\nまた、 大量のデータを扱う検索システムでは、 ディスク I/O のレイテンシやスループットがパフォーマンスを下げる大きな要因になりうるため、 注意が必要。 \n\nよって、 検索システムのパフォーマンス試験を実施するに当たっては以下項目を見るのが良いとされる\n\nレイテンシ\n\nスループット\n\nCPU およびメモリの使用量\n\nディスク I/O の回数測定\n\n（Kubernetes 環境では）ネットワークレイテンシや DNS 名前解決などの TCP 通信\n\nキャッシュの利用\n\n検索エンジン内に実装されるキャッシュの種類\n\n検索結果のキャッシュ\n\nクエリ途中結果のキャッシュ\n具体的には、 ポスティングリストの積集合や絞り込み条件、 ソート条件がキャッシュ対象。 \n\nドキュメントのキャッシュ\nインデクシング元のテキストそのものをキャッシュし、 ユーザーに返却するスニペットの生成を高速化するキャッシュ\n\netc…\n\n検索結果のキャッシュは、 キャッシュのヒット率が低いことが多く、 不要なデータをキャッシュから追い出す処理（eviction）のオーバーヘッドがキャッシュ効率を上回ることが多い。 クエリの特性に合わせて慎重にキャッシュ戦略を選ぶ必要がある。 \n\nまた、 リアルタイム性が重視されるシステムではそおそもキャッシュを導入しない方が良い場合もあるので注意。 \n\nキャッシュを導入する前に遅いクエリを分析するしてクエリそのものを改善できないかを検討することが重要。 \n\nインデックスの冗長化と分散検索\n\n1 台の検索エンジンでは扱いきれないほど大量のデータやトラフィックを捌く必要が出てきた場合、 一部で障害が発生した...","date":"2024-10-06T00:00:00.000Z"},{"slug":"next-blog-summary","title":"ほぼ無料で構築した個人ブログで 1 円稼ぐことに成功したのでやったことをまとめる","tags":["poem","tech","next"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n本ブログの今月の収益🎉\n\nalt text\n\n今年の 7 月あたりから個人ブログの作成を少しづつ始め、 Google Adsense の審査に通り、 0→1 を達成することができたので、 やったことをまとめてみます。 \n\n⚠️ ジュニアエンジニアが自分自身の成長のために作成したブログでたまたま収益が 1 円になっただけです。 ブログでたくさん稼ぎたい方にとってはあまり参考になる内容はないと思います m\n自分自身の継続のためにも好奇心で Google Adsense の審査を出すと通ったので、 少しだけ広告を入れさせてもらっている状況です。 \n\nまとめ\n\nNext.js でブログを構築した\n\nnotion で記事を書き、 github action の定期実行から notion api 経由で md ファイルを保存することで記事を管理している\n\nnotion の画像データの URL は、 notion の仕様によるものなのかがすぐに URL が変わってリンク切れしてしまうため、 画像データは base64 エンコードして md ファイル内に含めた\n\nその他、 TOC の自動生成、 ダークモード、 rss フィードなど普通のブログにありそうな機能を実装した\n\nホスティング先は Netlify の無料プラン（Vercel の無料プランだと広告を載せられない）\n\nCMS に notion, ホスティングに Netlify という構成なので、 ドメイン取得料以外は無料\n\n個人ブログを作ろうと思った背景\n\n勉強したことのアウトプットを形として残しておきたい\n\nブログを始めると、 ネタ作りのためにも勉強意欲になる\n\nいつか転職を検討する際に役立つかも\n\nつよつよで影響力のあるエンジニアは個人ブログを持っていることが多い\n\n仕様書や技術的なドキュメントを作成する際の文章力を鍛えたい（かっこよく言えばテクニカルライティング）\n\nNext.js で何か作りたい\n\nやったこと\n\nNext.js でブログのベース機能の作成\n\ncontentlayer による md ファイルには管理\n\n公式 doc: https://contentlayer.dev/docs\n\n参考記事:\n\n設定ファイルは以下\n\nreact-markdown と react-syntax-highlighter によるレンダリング\n\n公式 doc: https://github.com/remarkjs/react-markdown\n\nreact-markdwon でのレンダリングでは base64 形式で画像を指定する data: protocol はセキュリティの観点からデフォルトでサポート対象のため、 以下のように urlTransform を上書きする必要があります\n\nref: https://github.com/remarkjs/react-markdown/issues/774\n\nTOC の追加\n\n公式 doc: https://tscanlin.github.io/tocbot/\n\n参考記事:\n\n実装は以下\n\nnext/og による OGP の画像の自動生成\n\n公式 doc: https://github.com/vercel/next.js\n\n参考:\n\n実装（style 部分は長くなるため省略）\n\nnotion と github action 定期実行で CMS 機能の作成\n\nそもそもなぜ notion なのか？ \n\nnotion ならスマホからでも気軽に書くことができ、 記事の更新を継続できそうと思ったから。 \n\n他の CMS でブログ構築を試みた経験がありますが、 ブログのためだけに CMS を開くのが億劫で更新できなくなりました。 普段使いしている notion をデータソースとするとブログの更新が継続できるかもと思い、 無理やり notion を採用しました。 \n\n実現方法\n\nnotion api はすぐにリクエスト量制限に達するので、 都度取得できません。 定期実行（1 日に 2 回）で md ファイルに変換したものをリポジトリに保存しておき、 build 時にリポジトリにある md ファイルを読み込んでブログを構築しています。 \n\n最終的なマスターデータはリポジトリに保存される md ファイルとなるので、 細かい修正などは直接 md ファイルをいじったりもしています。 \n\ngithub actions の設定 yaml\n\nrun notion fetch タスクの処理の概要\n\nnotion の画像を md ファイルに変換する際の注意点\n\nnotion の画像データの URL は、 notion の仕様によるものなのかがすぐに URL が変わってリンク切れしてしまう\n\n⇒ notion-to-md で md 変換する際に画像を base64 変換することで対応 ref: https://github.com/souvikinator/notion-to-md/pull/81/files\n\n最後に\n\nわざわざ notion api を利用せずに、 md ファイルを直接いじる運用でも良い気がしている。 \n\n以下の機能が VS Code に追加されてからは画像の扱いもかなり楽になったので、 そのうち notion api 利用は終わりそう。 \n\nまた、 （1 円ととても少ないが）収益をブログから得るにあたって、 業務で得た知識との棲み分けが難しいなと感じる。 業務で得た知識は会社に帰属する認識でブログで利用するつもりはないが、 業務きっかけて興味を持ち、 プライベートの時間に深掘った技術をどこまでアウトプットすべきなのかが難しいところ。 。 。 \n\n「業務と業務時間外の勉強の境目が曖昧」 = 「会社の資産と自分の資産が曖昧」な状態なので同業の方はどのように捉えているのかとても気になる\n","date":"2024-12-03T00:00:00.000Z"},{"slug":"next-web-push","title":"Next.js でブラウザ通知を試してみる","tags":["tech","next","PWA"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\nNext.js (app router) にて Push 通知を実現してみた記録をまとめます。 \n\nPush 通知を実現するための技術である、 サービスワーカーとその応用の PWA についても気になったので少し言及しています。 \n\n実装の大部分は以下の Next.js の公式ドキュメントに基づいています。 \n\nブラウザのサービスワーカーとは\n\n通知を受け取りユーザーに表示するためには、 ブラウザのサービスワーカー機能を利用する必要があるとのこと。 \n\n以下の記事がとても参考になる。 \n\n以下は明示的にサービスワーカースクリプトをブラウザにダウロードさせる例。 \n\nダウンロードしたサービスワーカースクリプトがバックグラウンドで実行され、 通知をキャッチしてさばくという仕組み。 \n\n一度ダウンロードするとオフラインでも実行可能。 オフラインでも動くこのような仕組みをキャッシュなどに利用するのが PWA らしい。 \n\nサービスワーカーはブラウザの実行環境と異なり、 以下の api が利用できるらしい。 ページ読み込み前から動いてくれるスクリプトなので当然だが、 DOM にアクセスできなかったり、 window オブジェクトなどは利用できないので注意が必要。 \n\n以下の chrome の service worker 設定画面（chrome://serviceworker-internals/）から確認、 unregister することができる\n\nalt text\n\nまた、 検証画面から Unregister することもできます。 画像の「Update on reload」にチェックを入れておくと、 リロードの度にサービスワーカーが更新されるようになり、 デバッグが捗ったのでおすすめです。 \n\nalt text\n\nNext.js でブラウザ通知を実装する\n\n通知を受け取るサービスワーカーを作成する\n\npublic/sw.js\n\npush イベントを event listener を作成し、 通知を作成する showNotification メソッドで端末に通知を表示しています。 \n\nサービスワーカーを登録し、 通知管理の subscription をステートに格納する\n\nregister メソッドの返り値は以下の ServiceWorkerRegistration を返す。 \n\nこちらの ServiceWorkerRegistration からプッシュ通知の許可状況へのアクセスなど、 プッシュ通知の購読を管理するための PushManager へアクセスすることができる。 こちらの PushManager から取得できる PushSubscription インスタンスはプッシュ通知を送る際に利用するので state に格納しておく。 \n\nちなみに、 サービスワーカーを登録する際はブラウザの通知権限を求める必要があり、 自分は以下のように管理しています。 \n\n通知権限の状態については以下をの公式 Doc を参考に実装しました。 \n\nまた、 参考までに ーカルでで動作確認する場合は https である必要があるので、  next dev --experimental-https を利用する必要があます。 \n\n通知を送る（server action）\n\n最後に、 取得した subscription の情報を用いて通知を実装に送信する部分を作成しました。 通知送信部分は server action を利用します。 \n\nweb-push 送信部分の詳細は以下の README.me などを参照ください。  VAPID などを事前に登録しておく必要があります。 \n\n本サイトの以下 labs にて動作している部分を確認できます。 \n\nhttps://sokes-nook.net/blog/next-web-push\n\nchrome の場合、 送信した通知の履歴等は以下から確認することができる。 \n\nchrome://gcm-internals/\n\nPWA の現状について\n\nPWA(Progressive Web Applications) は、 ウェブアプリケーションのリーチとアクセス性を持ちながら、 ネイティブモバイルアプリの機能とユーザー体験を組み合わせたものです。 \n\nアプリストアの承認を待たずに、 すぐに更新をデプロイできる\n\n単一のコードベースでクロスプラットフォームアプリケーションを作成できる\n\nホーム画面へのインストールやプッシュ通知など、 ネイティブのような機能を提供できる\n\nNext.js においては、 manifest を適切に作成してサービスワーカーを配置すると PWA として動かすことができます。 \n\n本サイトも通知権限を付与してかつ、 ブラウザのホームに追加いただくと PWA として動くようになると思います。 \n\nしかし、 実態としてブラウザ通知やサービスワーカは以下で報告されているように詐欺などに利用されるケースが多くあり、 ユーザー側が積極的に許可する状況ではありません。 肌感覚的には、 「何か謎の通知が要求されたが、 よくわからないのでとりあえず拒否。 」というユーザーが多いと感じており、 アプリほどリーチとアクセス性を実現できないと思います。 \n\n実際にプロダクトに導入してリターンが見込めるかはしっかり検討が必要な段階ということです。 \n\nしかし、 技術としてはとても素敵だなと思いますし、 日々進化していると実感しています。 最後に PWA を導入する際に参考になりそうなサイトを置いておきます。 \n\n現在 PWA ができることは以下サイトにまとめられている\n\nNext.js の公式ドキュメントにて言及されている serwist を利用した PWA を作成しているブログ\n\nserwist のフォーク元である workbox を利用している以下の一休.com の事例\n","date":"2024-09-25T00:00:00.000Z"},{"slug":"nextauth-yahoojapanid","title":"NextAuth で Yahoo!JapanID でログインするためのカスタムプロバイダーを作成する","tags":["tech","auth","OIDC"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\nNext.js を用いたアプリケーションにログイン機能を実装する際、 NextAuth を利用することが多いと思います。 メールアドレスでログインだったり、 Google でログインする方法については、 たくさんの実装例や記事があると思います。 参考になりそうな実装例を以下に置いておきます。 \n\n公式の実装例: https://github.com/nextauthjs/next-auth-example\n\n記事: https://zenn.dev/tfutada/articles/5557b780050574\n\n本記事では Yahoo!JAPAN ID を用いたログインを NextAuth で実装する方法を紹介します。 \n\nYahoo! JAPAN ID でログインを実装するための前準備\n\n前提として Yahoo! JAPAN ID でログインを提供するための ID 連携の仕組みなどは以下で説明されているので事前に確認しておくと実装がわかりやすくなると思います。 \n\n利用用途に応じてこちらのガイドラインも参照しておくとさらに安心かと思います。 デザインガイドラインも用意されています。 \n\nまた、 以下の Yahoo!デベロッパーネットワークから、 先にアプリケーションを作成し、 OAuth2.0 および OpenID Connect 用の Clinet ID を発行しておく必要があります。 \n\nこちらで発行した Clinet ID, シークレットは実装で利用するのでメモっておいてください。 \n\nredirect\\_uri に指定する値を事前に登録する必要があるので、 以下値をコールバック URL に登録しておく必要があります。 \n\nhttps://サイトのドメイン/api/auth/callback/yahoo\n\n注意点として、 Yahoo!JAPAN ID をログインに利用し、 ユーザーの名前やメールアドレスを取得したい場合は以下で案内されている申し込みフォームから審査が必要となります。 Google の場合は審査なしで表示名やメールアドレスが取得できるようになりますが、 Yahoo!JAPAN は個人情報の扱いを丁寧に行なっていることが伺えますね。 \n\nhttps://developer.yahoo.co.jp/yconnect/v2/userinfo.html\n\n実装\n\n基本的には先ほど紹介したこちらの記事の Google でログインの部分の実装と同じです。 \n\nYahoo!JAPAN ID でログインするためのカスタムプロバイダーを作成し、 適用する部分だけを切り出してみました。 \n\nyahoo-japan-id-provider.ts\n\n先ほど取得した clinet id, シークレットは以下の部分で利用しており、 環境変数に指定しておく必要があります。 \n\nprocess.env.YAHOO\\_CLIENT\\_ID,\nprocess.env.YAHOO\\_CLIENT\\_SECRET,\n\nnext-auth-options.ts\n\nログインボタンコンポーネント\n\napp/api/auth/\\[…nextauth]/route.ts\n","date":"2024-09-29T00:00:00.000Z"},{"slug":"npm-workspace-shared","title":"npm workspace を用いて共通処理を package として切り出す","tags":["tech","next","npm"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n以下のような構成でモノレポではあるものの、 それぞれ独立した node プロジェクトで運用していた状態から、 npm workspace を用いて共通処理置き場(shared プロジェクト)を作成したのでその方法を紹介します。 \n\nworkspace 移行後のプロジェクト構成\n\nそもそも npm workspace とはという方は以下の記事がすごくわかりやすかったのでおすすめです。 \n\nやったこと\n\nプロジェクトルートに workspace 用の package.json 作成\n\n全てのパッケージで利用するような dependencies はルートに指定しておくことができます。 \n\nnode のモジュール解決ではホイスティング（hoisting, 巻き上げ）という機能があり、 自分の node\\_modules に該当モジュールがない場合は親の node\\_modules に該当モジュールを探しに行くという機能による恩恵です。 \n\nshared プロジェクト作成\n\nプロジェクト構成\n\nshared/package.json\n\nshared/tsconfig.json\n\nshared/index.ts\n\nshared/date.ts\n\nプロジェクトを build しておく\n\nfrontend 側の修正\n\nshared を frontend 側の依存に追加する\n\nshared の機能を利用する\n\n注意点\n\n元々存在していた node\\_module と package-lock.json を手動で削除してからルートにて npm install を実行する必要があります。 これを実施しないと以下のようなエラーで共通処理を正しく import できないと思います。 \n\n最後に build コマンドで shared プロジェクトを一緒に build されるように修正します。 \n\n最後に\n\n全てのメソッドが shared/index.ts にまとまって一括で import する構成はなんか微妙な気がする場合は export を利用する手もあると思います。 \n","date":"2025-02-08T00:00:00.000Z"},{"slug":"pageviews-by-gadataapi","title":"Google Analytics Data API を利用して PV 数と人気記事を取得する","tags":["tech","GCP"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\n個人ブログの記事に PV 数をつける際の最に Google Analytics Data API を利用したため、 その方法を備忘録的にまとめました。 \n\nGoogle Analytics がすでにサイトに導入されていて、 ある程度のデータが GoogleAnalytics 上にあることが前提の記事になりますのでご注意ください。 \n\n事前準備: API の有効化\n\nGoogle Analytics Reporting API を有効化しておく必要があります。 \n\nGCP のコンソールからプロジェクトを選択し、 以下 API を有効化してください。 \n\nhttps://console.cloud.google.com/apis/api/analyticsdata.googleapis.com/metrics\n\n参考までに公式の quick start のリンクも貼っておきます\nhttps://developers.google.com/analytics/devguides/reporting/data/v1/quickstart-client-libraries?hl=ja\n\nGoogle Analytics Data API を叩く\n\npagePath の dimensions で、 screenPageViews の metrics を指定すると、 ページごとの view 数を取得できる。 \n\nslug に対する PV 数をもつ辞書型を返すように実装しています。 \n\nその他、 dimensions, metrics のパラメータを調整するとそれぞれのユースケースに合わせたデータが取得できるようになると思います。 パラメータを調整する際に参考になりそうなドキュメント群を置いておきます\n\n公式 Doc: https://developers.google.com/analytics/devguides/reporting/data/v1?hl=ja\n\nnode の clinet ライブラリのドキュメント: https://googleapis.dev/nodejs/analytics-data/latest/index.html\n\nクエリで使用する、 dimensions, metrics で利用できる値: https://developers.google.com/analytics/devguides/reporting/data/v1/api-schema?hl=ja#dimensions\n\nおわりに\n\n※ Google Analytics Data API は、 料金はかからないようですが、 以下ページにある Quotas を上回るとリクエストに失敗するという制限があるようです。 ご注意ください。 \nhttps://developers.google.com/analytics/devguides/reporting/data/v1/quotas?hl=ja\n\n以下、 公式 doc 以外で実装の際に参考にさせていただいた記事です🙏\n\nNode.js (TypeScript) で Google Analytics Reporting API v4 を使用する方法\n\nGoogle Analytics Reporting API での発生したリクエスト数を確認する\n\nGatsby で作ったサイトに人気記事のランキングを実装する【前編】\n","date":"2024-09-11T15:00:00.000Z"},{"slug":"remix-notion-blog-give-up","title":"remix と notion api でブログを構築しようとしたが断念した話","tags":["tech","remix","notion"],"content":"構築しようとしたブログの要件\n\nremix を使いたい\n\nエンジニアあるあるですが、 目的と手段が一致するパターンです。 remix を利用したいが作るものがなかったので個人ブログを作ることにした感じです。 \n\nデータソースは notion\n\n以前 contentful で個人ブログを始めましたが、 contentful を開くことが面倒になってしまい続かなかったという経験があるため、 CMS は利用せずに notion api を利用したいです。 \n\n無料で高パフォーマンス\n\nこちらの 3 軸でブログ作成を真面目に検討し、 作成しましたが断念したのでその記録をまとめます。 \n\n環境\n\nremix-run: 2.10\n\nreact: 19rc\n\nデプロイ先: cloudflare pages\n\nやったこと\n\nremix プロジェクト作成\n\nnotion api 取得処理\n\n取得した記事一覧、 記事詳細ページの追加\n\nEDGE 環境でのキャッシュ処理実装\n\nGoogle Analytics 設定、 記事ページのビュー数取得\n\nやってみた結果思ったこと\n\n検討、 制作を進めるにつれてこれまでに見えていなかった remix の特徴が発覚し、 notion api との相性の悪さから断念しました。 \n\nremix は Edge 環境で動作することが前提のため、 node.js 依存のライブラリを利用できない。 \ncloudflare を利用している場合、  Home - Cloudflare Workers® が Edge 環境になります\n\n@google-analytics/data を用いて GA のデータを取得しようとしたが、 node.js 依存のため api をコールする部分を自分で実装する羽目に…\n\nnode.js の fs が使えないため、 md ファイルを fs を用いて read して… といったことができない。 そもそも Edge 環境にあげられる build 後のアプリケーションに md ファイルを含めることが難しい。 \n\nEdge 環境で毎度 SSR することを売りにしており、 アンチ SSG という姿勢のライブラリのため、 SSR するごとに notion api にアクセスする必要がある。 \nnotion api は 3rps 以上リクエストするとエラーとなるため、 SSR 環境で都度取得は絶望的\nhttps://developers.notion.com/reference/request-limits\n\nそこであげられる方法として、 あらかじめ notion の内容を repository に自動で保存しておく方法ですが、 こちらは node.js が使えないため、 不可能でした。 \n\n唯一残っている方法が、 notion の内容を react コンポーネントを含まない mdx として保存する自動化プログラムを作成し、 route/ 配下 に自動で mdx ファイルを増やし続ける方法のみでした。 mdx のみ、 remix が build 時に梱包することを built-in サポートしており、 remix の奨励方法であると言えそうです。 \nhttps://remix.run/docs/en/main/guides/mdx\n\nしかし、 mdx で作る方法を採用する場合、 ブログ一覧ページを取得する方法が難しいそう。 \nwindow.\\_\\_remixManifest.routes を見ればクライアントサイドから全ての route を見ることができるが、 ssr 時点で動的に取得する方法は自分の観測範囲内ではなさそう。 別途 blogList.server.ts 等、 ファイルを作って記事一覧を管理しているブログ多そうだが、 これはやりたくない。 \n\nおわりに\n\n今回の要件の場合、 諦めて Next.js での構築に切り替えようと思います。 \nnotion api を用いて無料でブログを構築する場合、 以下の流れが良さそうなので試してみます。 \n\ngithub action で 定期的に notion を md として repository に保存\n\nNext.js の SSG で contentlayer を用いて記事一覧を操作\n\nあくまで自分が remix の特徴を知らずにとりあえず使ってみたいと先走った結果であり、 remix は素晴らしい技術だと思っています。 ただ、 自分にそれを扱う準備と技術力がなかった…\n\nProgressive Enhancement 等、 Next.js の App router に多いに影響を与えており、 考え方や哲学は今で本当に素敵だなと感じています。 \n","date":"2024-08-31T00:00:00.000Z"},{"slug":"slides-browsing","title":"スライド徘徊のすすめ","tags":["poem"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n個人的に新しい技術をキャッチアップする際、 まず最初に既存の勉強会などのスライドをスライド共有サービスから探して全体を把握することが多いです。 こちらの方法が意外とお勧めなので、 スライド徘徊を勧めるだけの記事を書いてみようと思います。 \n\nスライド徘徊のメリット\n\n概念ベースでわかりやすくまとまっており、 すぐに全体像を掴めるスライドが存在する\n\nスライドから勉強会の youtube に飛べることができることも。 勉強会の存在を知るきっかけにもスなる\n\nすでによく使う技術であっても、 スライド徘徊していると新しい使い方に出会って自プロダクトの改善のきっかけになる。 （セレンディピティ的な？ ）\n\nスライド共有サービス 3 選\n\nslideshare\n\nJava で検索した際の例:\nhttps://www.slideshare.net/search?searchfrom=header\\&q=java\\&language=ja\\&sort=RECENT\n\n全分野のスライドがあるので、 技術のキャッチアップのスライドを探そうとするとノイズが多め\n\nSpeaker Deck\n\nJava で検索した際の例:\nhttps://speakerdeck.com/search?page=1\\&q=java\\&lang=ja\n\n洗練された内容のスライドが多いイメージで、 最も徘徊していて楽しい。 \n\nDocswell\n\n日本のエンジニア向けに開発されており、 個人的にも最も UX が良いと感じている。 カジュアルな内容から技術仕様の詳細解説まで存在する。 \n\nJava でタグ検索した際の例:\nhttps://www.docswell.com/tag/Java\n\nユーザーが独自でつけたタグでソートできるのが良い感じ。 \n\njava 学習で読んでよかったスライドたち\n\nぎり文法がわかるレベルの java 理解度だった自分が、 java21 の特徴をふんだんに利用したプロジェクトにジョインする際にまず参考にさせてもらったスライドたちを列挙します。 \n\nJava 21 の概要\n\nJava21 と Kotlin の代数的データ型 & パターンマッチの紹介と本当に嬉しい使い方\n\nJava の現状 2024 夏\n\nSpring のこれまでとこれから #javasumi23\n\n※ こちらのスライド内でも紹介されている以下動画は、 ネイティブイメージなど Spring Boot 3 の機能をキャッチアップしたい人にとって感動的にわかりやすかったです。 \nB 1500 槙俊明 5 年ぶりのメジャーアップデート! Spring Framework 6 Spring Boot 3\n\n今こそ知りたい Spring DI x AOP #jsug\n\n最後に Twitter に勉強会スライドを bot 化してくれるアカウントも紹介しておきます。 \n\nhttps://x.com/tech\\_slideshare\n","date":"2024-11-04T00:00:00.000Z"},{"slug":"standalone-apache-solr-docker","title":"standalone モードの Apache Solr を Docker で動かしてみる","tags":["tech","solr"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nstandalone モードの Apache Solr を Docker で動かしてみる\n\nApache Solr は全文検索ライブラリ Apache Licene(ルシーン)を用いた検索エンジンサーバーです。 \n\n以下を参考に docker を用いた standalone モードを動かしてみたのでその記録を紹介します。 \n\n利用した docker-compose.yaml\n\n起動コマンド\n\n起動すると以下から solr の管理画面にアクセスできるようになります。 \n\nhttp://localhost:8983/solr/#/\n\nコンテナに入ってデータをインデックス（参考書籍にある techproducts をインデックスする方法はなさそう）\n\nまた、  Solritas の画面は V9 では削除されているようです。 \n\nThe VelocityResponseWriter and associated /browse UI is deprecated and will be removed in 9.0.\nThe functionality has been replaced by a 3rd party plugin available at https://github.com/erikhatcher/solritas.\n\nhttps://solr.apache.org/guide/8\\_9/velocity-search-ui.html\n\nサイドバーの core selector から books を選択し、 query を実行すると以下のように検索が成功することを確認。 \n\nalt text\n\nSolr を扱う際に役立ちそうなメモ\n\nサーチコンポーネントとサーチハンドラー\n\nSolr が提供する検索機能は、 「サーチコンポーネント」と「サーチハンドラー」によって実現されている。 \n\nクライアントからの検索リクエストはサーチハンドラが処理する。 サーチハンドラーには複数のサーチコンポーネントが登録されており、 これらのサーチコンポーネントが互いに独立してリクエストを実行し、 最終的に処理結果を出力する。 （サーチコンポーネントの種類によっては依存関係があり、 完全に独立していない場合もある。 ）\n\nSolr が提供するサーチコンポーネントの抜粋\n\n| サーチコンポーネント名 | クラス名                    | 概要                         |\n| ----------- | ----------------------- | -------------------------- |\n| query       | solr.QueryComponent     | 検索結果を返す                    |\n| facet       | solr.FacetComponent     | ファセット情報を返す                 |\n| highlight   | solr.HighlightComponent | ハイライトスニペットを返す              |\n| suggest     | solr.SuggestComponent   | 検索キーワードのサジェスチョン（入力補完）を実現する |\n| …           |                         |                            |\n\nサーチコンポーネントをサーチハンドラーに登録するには、 solrconfig.xml のリクエストハンドラ定義内にある first-components または last-components にコンポーネント名を列挙する\n\nfisrt-components に登録されたサーチコンポーネントはデフォルトで登録されているサーチコンポーネントの前に登録され、 last-components は後に登録される。 \n\n※ query, facet, highlight, 等、 デフォルトで登録されているサーチコンポーネントは、 明示的に solrconfig.xml にて登録する必要がない。 \n\nSolrCloud によるクラスタ運用\n\nSolr では、 上で紹介した standalne モードではなく、 SolrCloud モードで大規模な検索エンジン構築を行うことができる。 \n\nSolrCloud は ZooKeeper を利用して、 ノードのステータス管理、 設定ファイルの中央集中管理、 分散インデクシング、 レプリケーション、 自動フェールオーバー、 リーダーノード（マスターノード）の自動選出など、 「単一障害点（Single Point Of Failure : SPOF）」を極力なくすための仕組みが取り込まれている。 \n\n分散インデックス\n\nSolr では大量のドキュメントを複数のノードで分けて扱うことが可能。 1 ノードあたりのストレージを小さく抑えつつ、 理論的には巨大なインデックスを構築できる。 その複数のノードに分散配置されたインデックスの一つをシャード（Shard）と言う。 \n\n複数ノードでインデクシングを並列処理することにより、 単位時間あたりのインデクシングのスループットを向上させることができる。 （インデクシングのスループットは Documents Per Second :  DPS という。 ）\n\n分散検索\n\n分散インデックスを用いた Solr では検索リクエストの際にすべてのシャードに同じ検索クエリを発行し、 すべてのシャードの検索結果をマージしてクライアントへ検索結果として返却する分散検索が可能。 \n\n複数のノードで小さなインデックスを並列に検索するため、 大規模の大きなインデックス一つを検索する場合に比べて処理時間が短くなる。 （単一インデックスからの検索に対して、 分散検索ではマージ処理は別途必要になるが、 大きなインデックスに対する検索コストに比べるとマージ処理は軽い処理）\n\nレプリケーション\n\nSolr では大量の検索トラフィックや、 ハードウェア障害などによりノードがダウンした場合に備えて用意するインデックスのレプリカを作成することができる。 \n\nレプリケーションは負荷分散とクラスタ全体の検索スループット向上させることができます。 （検索スループットは Queries Per Second : QPS という。 ）\n\n参考文献\n\n⚠️注意\n\n2024 年 10 月現在はで Apache Solr 9.7.0  まで出ていますが、 本書は 2017 年 5 月出版で Apache Solar 6.3.0 までの情報しか含みません。 適宜読み替える必要があります。 \n","date":"2024-10-15T00:00:00.000Z"},{"slug":"test-style","title":"ブログ表示テスト用","tags":["tech","poem"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nH1 heading\n\nH2 heading\n\nH3 heading\n\nlist1\n\nlist2\n\nlist3\n\nnumber1\n\nnumber2\n\nnumber3\n\nネスト 1\n\nネスト 2\n\nネスト 3\n\nthis is quote message\n\nこれは太文字です\n\nこれはインラインコードです\n\nこれは\\_イタリック\\_です\n\nこれはアンダーラインです\n\nこれは取り消し線です\n\n\\[ ] TODO1\n\n\\[ ] TODO2\n\n\\[x] TODO3\n\n以下はブックマーク\n\n以下はリンク\n\nhttps://zenn.dev/mizchi/articles/remix-cloudflare-pages-supabase\n\n以下は divider\n\n💡 これは callout です\n\nTwitter Embed テスト\n\nYouTube Embed テスト\n\n本ブログの Bookmark\noepngraph-image のテストも含む\n\n以下は png 画像\nalt text\n\n以下は jpg 画像\nalt text\n\n以下は gif 画像\nalt text\n\n以下は svg 画像\nalt text\n","date":"2024-08-13T15:00:00.000Z"},{"slug":"udemy-crafting-shell","title":"Python による自作シェル講座の感想","tags":["tech","udemy","shell"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\nはじめに\n\n以下講座を受講したのでその備忘録のメモ的な記事です。 あまり読む価値はないと思います\n\n自作シェルで学ぶ Linux システムプログラミング―Python で 150 行の「シェル」を実装して Linux のしくみを学ぼう\n\nコマンドを実行する「シェル」を自作しながら、 Linux のしくみや OS に近い領域のプログラミングを学ぼう！ 「シェルって何？ コマンドの実行って何？ 標準入出力って何？ ターミナルって何？ 」といった疑問をシェルの実装レベルで解消しよう！ \n\n講座の感想\n\n実際に作ってみるととても簡単にシェルが出来上がってびっくりした。 \n\n「0, 1, 2 にそれぞれ標準入力、 標準出力、 標準エラー出力がファイルディスクリプタとして割り当てられている」くらいの記憶はありましたが、 実際にリダイレクトやハイプの簡易的な実装をこなうことその本質の理解に迫れたのが良かった。 \n\nアプリケーションレイヤーのプログラミングを行う際に直接役に立つことは少ないと思うが、 知っておいて得しかない情報ばかりだった。 \n\n以下、 講座受講中のメモ\n\nそもそもシェルとは\n\nシェルはユーザーが入力したコマンドを読み取り、 プロガラムを実行する、 特殊なプログラム。 \n\nよってシェルの中心的な処理は以下の二つに分類できる\n\nユーザーの入力を読み込んで解析する\n\n指定されたコマンド（プログラム）を実行する\n\nカーネルの機能\n\nプロセス管理\n\nOS がプロセスを順番に割り当てて、 複数のプロセスが同時に動いているかのように見せる（コアが一つなら同時に 1 つのプロセスしか動かせない）\n\ninit プロセス → 全てのプロセスの親となるプロセス。 systemd がよく init プロセスと用いられる（docker は一番親プロセスが init プロセスではないので注意）\n\ninit プロセスが sshd を起動しておいてくれるので、 Bash などのシェルが起動できる\n\nメモリ管理\n\nファイルシステム\n\netc..\n\n⇒ シェルはコマンドを実行する機能があるが、 プロセスを新しく生成することができるのはカーネルだけなので、 シェルはプロセスの実行をカーネルに依頼することになるということ。 \nこのプロセスの実行や、 その他カーネルの処理を呼び出す処理を「システムコール」という。 \n\nPython でシステムコールする場合は、 fork,  exec という 2 つのシステムコールに対応したメソッドを利用する\n\nシェルのコマンドの種類\n\n内部コマンド → echo, cd, pwd などシェル内部で実装されたコマンド。 type コマンドでそのコマンドが内部か外部かを区別できる\n\n外部コマンド → 実行するプログラムのファイル名。 プログラムの場所は PATH で指定する（フォルダが指定された場合、 そのフォルダにある実行可能プログラムが全て外部コマンドとなる）。 シェルはコマンドが内部コマンドでない場合、 PATH 一覧から該当するプログラムがないかを探す。 cat, ls などは外部コマンド\n\nlinix シグナル\n\nあるプロセスから別のプロセスに通信する方法の一種。 \n\nSIGINT\nターミナルで Ctrl + C を入力すると、 プロセスに SIGINT が送信される。 プロセスのデフォルトの挙動として、 SIGINT を受信するとプロセスの実行が終了することになっている。 \n\nSIGTERM\n\nSIGKILL\nカーネルが強制的にプロセスを停止するシグナル。 プロセスは SIGKILL ハンドラーを作成できないため、 必ずプロセスを停止することができるシグナル。 \nkill コマンドはこの SIGKILL をプロセスに送信している\n\netc…\n\nシグナルハンドリングを実装することでプロセスのデフォルトの挙動を修正することができる。 \n\nシェバン(shebang)\n\nLinux などでスクリプトを実装する際、 スクリプトの 1 行目に #!<インタプリタのパス> と書かれている部分。 \n\n\\#!/bin/bash\n\n\\#!/usr/bin/env python3\n\n…\n\nLinux におけるシェバンの解釈\n\nデフォルトではコマンドは最終的に execve システムコールに渡される\nexecve(”./script.sh”, \\[”./script.sh”], ….])\n\nシェバンを指定すると、 execve ではなく、 シェバンに指定されたインタプリタにコマンドが渡されるようになる\n/bin/bash script.sh\n\n標準入出力とリダイレクト、 パイプ\n\nリダイレクトやパイプの機能を使うことで、 標準入力、 標準出力、 標準エラー出力を切り替えることができる。 \n\nプロセスから見たファイルは、 0, 1, 2, 3, 4 といった数字が割り当てられてる（プロセス内で open したファイルに数字が払い出される）この数字をファイルディスクリプタという。 \nプロセスが open しているファイルは ls -l proc/{PID}/fd をみると確認できる。  $$ には現在の PID が格納されているので、  ls -l proc/{PID}/fd とすると現在のプロセスのファイルディスクリプタを確認できる。 \n\n標準入力、 標準出力、 標準エラーはそれぞれ、 0, 1, 2 のファイルディスクリプタのこと。 \n\nリダイレクト\n\n標準出力の切り替え: >\n\n標準エラー出力の切り替え 2>:\n\nパイプ\n\n前のコマンドの標準出力を、 次のコマンドの標準入力に受け流すことができる\n\n/dev/pts/0 とういファイルはなんなのか？ \n\n前提として、 以下ディレクトリには特殊なファイルシステムが使われている\n\n| ディレクトリ | 配置されるファイル    |\n| ------ | ------------ |\n| /dev   | デバイスファイル     |\n| /proc  | カーネルやプロセスの情報 |\n| /sys   | カーネルやデバイスの情報 |\n\n※ これらの情報はストレージ上に書き込まれているわけではない。 OS のカーネル内のオペレーション上の情報をファイルと同じような扱いにしているだけ。 ”Everything is a file” という Linux の哲学の一つ。 Linux において、 「ファイルはデータを読み書きできるもの」くらい抽象的な存在である。 \n\nシェルとターミナルの違い\n\n本来のターミナルは、 コンピュータに接続して使う、 ディスプレイとキーボードがくっついたハードウェアのことを指していた（この画面はいつも見てるターミナルの黒い画面のみということ）。 \n現在ではこの物理的なターミナルを使われることはなく、 ソフトとして入っている「ターミナルエミュレータ」というソフトウェアをターミナルと呼んでいるということ。 \n\nターミナルは起動すると /dev/pts/0 に接続され、 この /dev/pts/0 を介してシェルを扱っている。 \n\nよって、 ターミナルは入力装置で、 実際にプログラムを実行しているのはシェルという整理ができそう。 \n\nこの違いを意識すると、 clear コマンドの原理を理解できる。 \n","date":"2024-09-03T15:00:00.000Z"},{"slug":"unified-notion-bookmark","title":"unified を理解して notion-to-md の Bookmark を AST 上から扱う","tags":["tech","markdown","unified"],"content":"はじめに\n\n本ブログは 2024/09/14 現在では、 Next.js, contentlayer, markdown を用いて構築しています。 \n記事は notion で書いたものを notion api を定期実行で叩くことで markdown として取得し保存しています。 \n\nnotion api からページ内容を取得し Markdown に変換する処理は notion-to-md を用いて行なっています。 \n\nこちらライブラリでは、 notion の Bookmark を以下のような md に変換します。 \n\nデフォルトの markdown レンダラーを利用する場合、 リンクテキストが”bookmark”のリンクとして解釈されてしまいます。 \n\nこちらの bookmark を独自に解釈してブックマークとして表示する unified プラグインを作成したので紹介します。 \n\nunified について\n\nmarkdown を自分の思った通りに描画しようとした場合、 unified というエコシステムを利用することが不可避でした。 \n\nunified では mdast（Markdown の AST）を構築し、 hast （HTML の AST）に変換して、 html を出力するという手順で動いており、 AST の扱いを把握する必要がありました。 \n\nunified の理解については @janus\\_wel さんの zenn における以下連載がとても参考になりました。 \n\nunified を使う前準備\n\nunified におけるプラグインまとめ\n\nunified を使って Markdown を拡張する\n\nunified を使ってオレオレ Markdown を ReactElement に変換する\n\nこちらで解説されている内容は前提として本編が構成されているため、 前もっての一読をお勧めします。 \n\nまた、 qiita の以下記事もわかりやすかったのでお勧めです。 \n\nNext.js のための Remark / Rehype 入門\n\n実装\n\n感想\n\n正直、 \\[bookmark] で始まる md リンクを if 文で条件分岐するだけで良かったのですが、 unified が面白そうだったので試してみた感じです。 実務だったら if 文で書けって怒られそう。 \n\n以下動画を参考にすると、 async の remarkPlugin 作成もできそう？ （未検証）\nasync が使えると plugin の自由度がさらに上がるので気になる方はぜひ。 \n\n余談\n\nほとんどのプログラミング言語の処理系でも AST を利用されており、 AST についてのアレルギーをなくしておくと、 言語を深く理解したくなったときや言語レベルでのデバッグの際に役立ちそうだなーと感じました。 \n\n参考: https://www.hpcs.cs.tsukuba.ac.jp/~msato/lecture-note/comp-lecture/note1.html\n\nAST Explorer（https://astexplorer.net/ ）では、 さまざまな言語がどのように AST になるかを眺めることができて面白いです。 \n\n以下は markdown を Parser: remark でパースして生成された AST の例です。 \n\nalt text\n","date":"2024-09-14T00:00:00.000Z"},{"slug":"uv-with-vscode","title":"Python プロジェクトに uv を導入 with VSCode","tags":["tech","python","uv","VSCode"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n巷で話題の uv を用いて VSCode の開発環境を構築した記録です。 \n\nuv でモジュールをインストールした後、 モジュールのパラメータ予測、 ソースコード参照が可能な環境を作成するための手順が少なかったので記事にしました。 \n\nuv とは\n\nRust で書かれた非常に高速な Python パッケージおよびプロジェクトマネージャーです。 \n\n公式 Doc 以外にも uv については話題になっていた以下の記事群を参照すると良いと思います。 \n\nプロジェクトへの uv の導入\n\nVSCode 上で uv 環境を指定する\n\nVSCode の画面の左下にある Python のバージョンを表す数字の部分をクリックし、 「インタープリターのパスを選択」を選択。 \n\nalt text\n\nuv によって作成さらた python バイナリのパス（your/path/to/airflow/.venv/bin/python3）を入力\n\nalt text\n\n上記操作を実施するとプロジェクトで uv のインタープリタが有効になり、 モジュールが適切に import されるようになります。 \n\n画像のようにパラメータ補完や command+クリックによるソース参照機能が有効になっていることを確認できると思います。 \n\nalt text\n\n本格的にやりたい場合、 以下の devcontainer を用いた環境構築をするのが良さそう。 \n\n【おまけ 】ruff を導入してコードフォーマットする\n\nuv の開発元と同じ Astral 社が開発している Python 用の Formatter, Linter である ruff を導入して利用してみます。 \n\nuv add の際に--dev オプションをつけることで development dependency group に依存関係を追加することができます。 \n\n自分の環境では lefthook の pre-commit のフックに ruff を追加してコミット前にコードを整形するルールをプロジェクトに適用しています。 \n","date":"2024-12-26T00:00:00.000Z"}]