[{"slug":"2025-02-26-next-flexsearch","title":"flexsearchを用いてNext.jsでサイト内検索を実装する","tags":["tech","poem"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n本ブログはNext.jsでMarkdwonをソースとしたブログなのですが、サイト内検索が欲しくなったので追加しました。\n\nNextraが提供しているサイト内検索がflexsearchを利用しているとのことなので、nextraの実装を参考にしながら自分のブログにも実装してみます。\nhttps://nextra.site/docs/guide/search\n\nhttps://github.com/shuding/nextra\n\n\nビルドの際にMarkdwonから簡易的にインデックスを作成し、ブラウザで完結するサイト内検索ができれば良いなと思い色々頑張ったので、その記録を残します。\n\n<Bookmark href=\"https://tenderfeel.xsrv.jp/javascript/5711/\" />\n\n\n# Flexsearchとは\nhttps://github.com/nextapps-de/flexsearch\n\njavascriptで動作する高速の全文検索ライブラリです。Node.jsに加えてブラウザでも動作するので、ビルドの際にインデックスを作成し、ブラウザで完結するサイト内検索が可能になります。\n\n## Flexsearchの特徴\n","date":"2025-02-26T10:34:20.000Z"},{"slug":"2025-02-26-next-mdx-bundler-blog","title":"Next.jsとmarkdownのブログでcontentlayerからmdx-bundlerに移行した話","tags":["tech","next","contentlayer","mdx","markdown"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n## はじめに\n\n本ブログで利用していた[contentlayer](https://github.com/contentlayerdev/contentlayer)がメンテナンスを停止したので、[mdx-bundler](https://github.com/kentcdodds/mdx-bundler)へ移行しました。\n\n現状の記事管理ではmdxを使っているわけではないのですが、これを機に表現の幅を広げられたら嬉しいなと思い mdx-bundler を選定しました。\n\nappディレクトリ配下ではない場所でmarkdownファイルを管理しているため、[Next.js公式にあるmdxの利用方法](https://nextjs.org/docs/pages/building-your-application/configuring/mdx)はあまり参考にならず、自前でゴリゴリ実装する羽目になりましたがなんとか実現できたのでその記録を残します。\n\n[contentlayerから移行の際の注意点](#contentlayerから移行の際の注意点) にあるように、contentlayerっぽい作りをそのまま利用することはできないといのでこれから実施する人は注意してください。\n\n\n## 実装紹介\n### 記事一覧取得部分\n記事一覧はmarkdownのfrontmatterの情報のみが必要なため、mdx-bundlerは利用せず、[gray-matter](https://github.com/jonschlinkert/gray-matter)を用いて記事の情報を取得しました。\n\n```tsx\nexport const baseDir = process.env.BASE_DIR || process.cwd();\n\nexport const getPostDirPath = () =>\n  path.join(baseDir, \"../blog-contents/contents/tech-blog\");\n\nexport async function getFrontMatter(\n  slug: string\n): Promise<FrontMatter | null> {\n  try {\n    const fileContent = await readFileFromMdorMds(slug);\n    if (!fileContent) return null;\n    const { data } = matter(fileContent);\n\n    // 型アサーションで FrontMatter 型を適用\n    return data as FrontMatter;\n  } catch (error) {\n    console.error(\"Error reading Markdown file:\", error);\n    return null;\n  }\n}\n\nexport const getAllPosts = async (): Promise<FrontMatter[]> => {\n  const postDirPath = getPostDirPath();\n  const postFiles = await readdir(postDirPath);\n  const slugs = postFiles.map((file) =>\n    path.basename(file, path.extname(file))\n  );\n\n  const frontMattersPromises = slugs.map((slug) => getFrontMatter(slug));\n  const frontMatters = (await Promise.all(frontMattersPromises)).filter(\n    (post): post is FrontMatter => post !== null\n  );\n\n  return frontMatters;\n};\n```\n\n### bundleMDXの設定\nmarkdown内で画像を利用しているため、remarkMdxImagesの適用と、esbuildOptionsにて、Next.jsのpublicフォルダに画像を配置するように設定しました。\n```tsx\nimport { bundleMDX } from \"mdx-bundler\";\n\nexport const loadMDX = async (fileContent: string) => {\n  return bundleMDX<FrontMatter>({\n    source: fileContent,\n    cwd: getPostDirPath(),\n    mdxOptions(options) {\n      options.remarkPlugins = [\n        ...(options.remarkPlugins ?? []),\n        remarkGfm,\n        remarkMdxImages,\n      ];\n      options.rehypePlugins = [\n        ...(options.rehypePlugins ?? []),\n        rehypeSlug,\n      ];\n      return { ...options, providerImportSource: \"@mdx-js/react\" };\n    },\n    esbuildOptions(options) {\n      options.outdir = path.join(baseDir, \"public/images/blog/\");\n      options.loader = {\n        ...options.loader,\n        \".gif\": \"file\",\n        \".jpeg\": \"file\",\n        \".jpg\": \"file\",\n        \".png\": \"file\",\n        \".svg\": \"file\",\n        \".webp\": \"file\",\n      };\n      options.publicPath = \"/images/blog/\";\n      options.write = true;\n      return options;\n    },\n  });\n};\n\n```\n\n### 記事詳細ページ\napp/blog/[slug]/page.tsx\n```tsx\nexport const dynamic = \"error\";\nexport const dynamicParams = false;\n\nexport async function generateStaticParams() {\n  const slugs = await getSlugs();\n  return slugs.map((slug) => {\n    return { slug };\n  });\n}\n\ntype Props = {\n  params: Promise<{ slug: string }>;\n};\n\nexport default async function Page({ params }: Props) {\n  const { slug } = await params;\n  const fileContent = await readFileFromMdorMds(slug);\n  if (!fileContent) return notFound();\n\nconst mdx = await loadMDX(fileContent);\nconst { frontmatter, code } = mdx;\n\nreturn (\n    <article>\n        {/* 記事のメタデータなど, frontmatterの情報を表示するセクション */}\n    \n        <MDXComponent code={code}/>\n    </article>\n);\n```\n\napp/blog/[slug]/MDXComponent.tsx\n```tsx\n\"use client\";\nimport { useMemo } from \"react\";\nimport { getMDXComponent } from \"mdx-bundler/client\";\nimport { MDXProvider, useMDXComponents } from \"@mdx-js/react\";\nimport { Prism as SyntaxHighlighter } from \"react-syntax-highlighter\";\nimport { oneDark } from \"react-syntax-highlighter/dist/cjs/styles/prism\";\n\nconst GLOBAL_CONFIG = {\n  MdxJsReact: {\n    useMDXComponents,\n  }\n};\n\nexport function MDXComponent({ code }: { code: string }) {\n  const Component = useMemo(() => getMDXComponent(code, MDX_GLOBAL_CONFIG),[code]);\n\n  return (\n    <MDXProvider\n      components={{\n        code: ({ ...props }) => {\n          const { className, children } = props;\n          const match = /language-(\\w+)/.exec(className || \"\");\n          return match ? (\n            <SyntaxHighlighter\n              language={match[1]}\n              PreTag=\"div\"\n              {...props}\n              style={oneDark}\n            >\n              {String(children).replace(/\\n$/, \"\")}\n            </SyntaxHighlighter>\n          ) : (\n            <code className={className} {...props}>\n              {children}\n            </code>\n          );\n        },\n      }}\n    >\n      <Component />\n    </MDXProvider>\n  );\n}\n```\n[こちら](https://github.com/kentcdodds/mdx-bundler?tab=readme-ov-file#custom-components-in-downstream-files)を参考に、codeのハイライト等のクライアント側でのcustom componentsを利用できるような設定を実施しています。\n\n\n## contentlayerから移行の際の注意点\ncontentlayerの場合は、`.contentlayer`配下にmarkdownファイルの情報をまとめたjsonを作成し、ビルド生成物としてバンドルしてしまうので、SSRの際にファイルシステムへアクセスせずとも記事情報を取得することで来ていました。\n\nmdx-bundlerのみで同じような機能を実装しようとすると、SSRの際にファイルアクセスしてエラーが出るので、SSGのみにしておくか、ビルド時にファイルを読み込むようにしておく必要があるということです。\n\n自分の場合、mdファイルを読み込む必要がある `/blog/[slug]/page.tsx` については以下の設定を適用し、SSGのみにする対応としました。\n\n```\nexport const dynamic = \"error\";\nexport const dynamicParams = false;\n```\nhttps://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config\n\n⚠️ fetchでrevalidateを用いているISRは利用できません。file system参照でエラーになると思うので削除してください。自分はfetchのrevalidateしている部分があることに忘れて1日ほどハマりました。。。\n\n\n\n\n## `@next/mdx`を利用しなかった理由\nブログ記事を別のmarkdownのみのリポジトリで管理していたからです。\n`@next/mdx`の場合、Next.jsのお作法に従って mdxファイルをappフォルダ配下に配置する必要があり、ブログ記事管理とフロントエンドの実装が疎結合になるのが気に入りませんでした。\n\n参考: https://nextjs.org/docs/pages/building-your-application/configuring/mdx\n\n## `next-mdx-remote`を利用しなかった理由\nesbuildをdependencyに含めてしまって,mdx内のimportを解決してくれるのが嬉しいです。\nmarkdownで記事を管理するリポジトリにComponentを配置することができるので、記事の管理をフロントエンドの実装に依存させることなく管理できます。\n\n\n## 最後に\nmdx-bundlerでは以下のようにわかりやすい感じでmarkdownにBookmark(Linkcard)を配置することができて良い感じだなと🎉\n```\nimport { Bookmark } from \"../../components/Bookmark\";\n\n<Bookmark href=\"https://sokes-nook.net/blog/next-web-push\" siteUrl=\"https://sokes-nook.net\" />\n```\n表示結果↓\n<Bookmark href=\"https://sokes-nook.net/blog/next-web-push\" siteUrl=\"https://sokes-nook.net\" />\n\n\nmdxに移行する以前は、Bookmark(Linkcard)を表示するために以下のようななんちゃってのunifiedプラグインを作って頑張っていましたが、とてもシンプルになってとてもほっこりです。\n\n<Bookmark href=\"https://sokes-nook.net/blog/unified-notion-bookmark\" siteUrl=\"https://sokes-nook.net\" />\n\n\n\ncontentlayerとmdx-bundlerは同じ役割ではないので、厳密には移行とは言えないと注意されそうなので補足しておくと、contentlayer利用時のレンダリングには[react-markdwon](https://github.com/remarkjs/react-markdown)を利用していました。\n","date":"2025-02-26T10:44:36.000Z"},{"slug":"2025-03-01-my-dotfiles","title":"dotfilesの育成 in 2025【mise, sheldon, raycastなど】","tags":["tech","dotfiles","mise","sheldon","raycast","fzf","shell"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n## はじめに\nPCをリプレイス機会があったのでdotfilesを整理しました。\n\nある程度調査をして、自分が使いこなせる範囲の最低限整えるためのdotfilesといった感じです。  \n「rust製のツールで揃えてとにかく早い環境を作る」みたいないなこだわりも特にありません。\n\nリポジトリは以下になります。  \nhttps://github.com/nyankiti/dotfiles\n\n## 2023時点からの変化\n### mise\nあらゆる言語を管理できるツールを利用たことがなかったのでこれを機に入門しました。\nグローバルに用いる実行環境については全て[mise](https://github.com/jdx/mise)へ集約できるようにしました。\n\nそれぞれの言語で個別でPATHを通していたものが以下で集約され、.zshrcがとてもすっきりしました。\n```\neval \"$(mise activate zsh)\"\nexport PATH=\"$HOME/.local/share/mise/shims:$PATH\"\n```\n\n### zinit => Sheldon\nSheldon が早い & toml ファイルで設定を管理できると噂を聞いていたのでこれを機に入門しました。\n\n[zinitが不安なのでsheldonへ移行したらzshの起動が50msと更に速くなった](https://ktrysmt.github.io/blog/migrate-zinit-to-sheldon/) 等を見ると遅延読み込みの設定を書いていました。興味のある人はこちらも試すと良いと思います。\n\n自分は上記の遅延読み込みを入れると kube-ps1 の$PROMPTのカスタマイズが初回から読み込めないという問題があったので利用しませんでした。\n\n```\nshell = \"zsh\"\n\n# pure テーマ\n[plugins.pure]\ngithub = \"sindresorhus/pure\"\nuse = [\"async.zsh\", \"pure.zsh\"]\n\n# 構文ハイライト\n[plugins.zsh-syntax-highlighting]\ngithub = \"zsh-users/zsh-syntax-highlighting\"\n\n# 補完\n[plugins.zsh-completions]\ngithub = \"zsh-users/zsh-completions\"\n\n# オートサジェスト\n[plugins.zsh-autosuggestions]\ngithub = \"zsh-users/zsh-autosuggestions\"\n```\n[pure](https://github.com/sindresorhus/pure)は最初に入れたzsh themeで思い入れがあるのかずっと利用しています。\n\n### alfred, clipy => raycast\nraycastは1年以上使っていましたが、dotfilesに落とし込めていなかったので追加しました。\nraycastの設定のエクスポート/インポートができるようで、詳細は以下を参照ください。通常の設定に加えてスニペットなどの情報も引き継げるようです。\n<Bookmark href=\"https://www.raycast.com/changelog/1-22-0\" />\n\n個人的にはraycastはalfredの完全上位互換と捉えており、とても重宝しています。\n\n### peco => fzf\npecoで特に不満はなかったのですが、色々とfzfの方が優れている気はしていたので乗り換えました。\nといっても現状以下のコマンド履歴検索を利用している程度です。もっと色々カスタマイズできるはずなので、これから色々試していきたいです。 \n```\nfunction history_search_with_fzf() {\n    local selected_command\n    selected_command=$(history -n 1 | tac | awk '!a[$0]++' | fzf --height 40% --layout=reverse --border --inline-info)\n    if [ -n \"$selected_command\" ]; then\n        BUFFER=$selected_command\n        CURSOR=$#BUFFER\n        zle reset-prompt\n    fi\n}\nzle -N history_search_with_fzf\nbindkey '^R' history_search_with_fzf\n```\n\n利用例↓\n![alt text](<images/2025-03-01-my-dotfiles/スクリーンショット 2025-03-01 22.09.22.png>)\n\n### .gitconfig\n.gitconfigによる以下の設定をdotfilesに追加。`git pull`した時の merge strategyの指定がないぞと言われるあのワーニングへの対策です\n```\n[include]\n  path = ~/dotfiles/git/user.conf\n[color]\n\tui = true\n[core]\n\tautocrlf = false\n\tignorecase = false\n\tquotepath = false\n\tsafecrlf = true\n[init]\n\tdefaultBranch = main\n[pull]\n\trebase = false\n[push]\n\tdefault = current\n    autoSetupRemote = true\n[merge]\n  ff = false\n```\n\n## GitHub Actionsで再現性の担保\n`macos-latest`環境にて`setup.sh`と`source .zshrc`が正常終了することをGitHub Actionsで担保しました。\n```\njobs:\n  check:\n    runs-on: macos-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup.sh\n        run: bash setup.sh\n        shell: zsh {0}\n        env:\n          SHELL: /usr/bin/zsh\n      - name: Source .zshrc\n        run: source zsh/.zshrc\n        shell: zsh {0}\n        env:\n          SHELL: /usr/bin/zsh\n```\n\n## 最後に\nmise, sheldon, fzfは初めて触ることもあり、結局1日くらい格闘していました。  \nCursor, Warp等の生成AI周りのツールの環境整備もdotfilesに落とし込みたかったのですが、今回は定番どころを押さえていくだけで時間がすごいかかったのでまた別の機会にしたいと思います。\n\n今回メインで参考にさせていただいた記事です🙏\n- [2024年度 わたしのdotfilesを紹介します](https://zenn.dev/smartcamp/articles/f20a72910bde40)\n  - https://github.com/ayuukumakuma/dotfiles\n- [Macの環境をdotfilesでセットアップしてみた改](https://github.com/tsukuboshi/dotfiles)\n- [dotfilesで再構築可能なターミナル環境構築を目指してみた](https://dev.classmethod.jp/articles/dotfiles-reconstruct-termina-env/)\n\n","date":"2025-03-01T02:31:36.000Z"},{"slug":"2025-03-02-open-ai-batch-api","title":"Open AIのBatch APIを利用してembeddingを50%割引で実施する【node.js】","tags":["tech","node.js","openai","prisma"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n\n## OpenAIのBatch APIとは\n<Bookmark href=\"https://platform.openai.com/docs/guides/batch\" />\n\nOpenAIへのリクエストを非同期で処理し、コストを削減するための機能です。通常のAPIと比較して50%割引となっており、コストパフォーマンスに優れています。\n\n複数のリクエストをJSON Lines（.jsonl）形式のファイルとして書き出して送信し、最大24時間以内に結果の .jsonl ファイルを取得することができます。\n\n公式Docでは以下のようなユースケースが紹介されています。\n\n- 評価の実行\n- 大規模データセットの分類\n- コンテンツのembedding\n\n今回はRAGを組む際のデータのembeddingに利用しました。\n基本的にはリアルタイム性が必要のない利用であればどのような用途でも利用可能だと思います。\n\n### Batch APIの利用手順は以下\n\n1. **入力データの作成**: 処理したいプロンプトをJSON Lines（.jsonl）形式のファイルにまとめます。\n2. **ファイルのアップロード**: 作成した.jsonlファイルをOpenAIにアップロードします。\n3. **バッチジョブの開始**: アップロードしたファイルに対してバッチ処理を実行するジョブを作成します。\n4. **結果の取得**: 処理が完了したら、結果をダウンロードして確認します。\n\n今回はnode.jsを用いてリクエスト用のファイルを作成し、結果を受け取ってDBに保存するまでの処理を紹介しようと思います。\n\n## やったことまとめ\n\n- リクエストをまとめた.jsonlファイルを作成するnode.jsスクリプトを作成\n- openAIのBatch APIダッシュボードに.jsonlファイルをアップロード\n- 処理が完了するまで最大24時間待機\n- openAIのBatch APIダッシュボードから結果の.jsonlファイルをダウンロード\n- node.jsスクリプトで結果.jsonlファイルを読み込み、結果をpostgresに格納\n\n### なぜnode.jsを用いたか？\n\nnode.jsを利用したアプリの一環で作成したバッチ処理だったためです。prismaクライアントの生のSQLを実行するための `queryRawTyped` を利用しているので、どの言語、DBクライアントを用いても同じだと思います。\n\nPythonの環境がある場合はPythonでやってしまった方が良いと思います。\n\n### なぜopenAIクライアントの[batchs](https://github.com/openai/openai-node/blob/master/src/resources/batches.ts)を利用しないのか？\n\n最初はリクエスト/レスポンスのファイルの中身を見ながら利用したなと感じたからです。そのうちopenAIクライアント利用に移行すると思います。openAIクライアントを利用するとアプリケーション側ではbatchIdを管理するだけで良くなりそうです。\n\n### 利用モデル\n\n2025/02/22 時点で最もコスパの良さそうな`text-embedding-3-small` を利用しました。\n\n[https://openai.com/ja-JP/index/new-embedding-models-and-api-updates/](https://openai.com/ja-JP/index/new-embedding-models-and-api-updates/)\n\n## 実装紹介\n\n### リクエストをまとめた.jsonlファイルを作成するnode.jsスクリプトを作成\n\n```tsx\nimport fs from \"fs/promises\";\nimport path from \"path\";\n\nexport async function processInput() {\n  const dataForEmbedding = [\"aaaaaa\", \"bbbbbb\", \"cccccc\"];\n\n  const requests = dataForEmbedding.map((data, i) => {\n    // OpenAI は改行を空白に置換することを奨励している\n    const input = data.replace(/\\n/g, \" \");\n    return {\n      custom_id: i,\n      method: \"POST\",\n      url: \"/v1/embeddings\",\n      body: {\n        /** @see https://openai.com/ja-JP/index/new-embedding-models-and-api-updates/ */\n        model: \"text-embedding-3-small\",\n        input,\n      },\n    };\n  });\n\n  const filePath = path.resolve(\n    __dirname,\n    \"./openai-batch-data/input/requests_embedding_1.jsonl\"\n  );\n  // requestsをファイルに書き込む\n  const fileHandle = await fs.open(filePath, \"w\");\n  const stream = fileHandle.createWriteStream();\n  for (const request of requests) {\n    stream.write(JSON.stringify(request) + \"\\n\");\n  }\n  stream.end();\n  console.log(\"requests.jsonl created\");\n}\n\nprocessInput()\n```\n\n作成されたファイル\n\n```json\n{\"custom_id\":\"0\",\"method\":\"POST\",\"url\":\"/v1/embeddings\",\"body\":{\"model\":\"text-embedding-3-small\",\"input\":\"aaaaaa\"}}\n{\"custom_id\":\"1\",\"method\":\"POST\",\"url\":\"/v1/embeddings\",\"body\":{\"model\":\"text-embedding-3-small\",\"input\":\"bbbbbb\"}}\n{\"custom_id\":\"2\",\"method\":\"POST\",\"url\":\"/v1/embeddings\",\"body\":{\"model\":\"text-embedding-3-small\",\"input\":\"cccccc\"}}\n```\n\n### Batch APIダッシュボードに.jsonlファイルをアップロード\n\n[https://platform.openai.com/batches/](https://platform.openai.com/batches/)\n\n![alt text](images/2025-03-02-open-ai-batch-api/スクリーンショット_2025-02-22_15.19.26.png)\n\nembeddingに利用するので Endpointは /v1/embeddings を選択する必要があります。Completion windowは現在 24 hours しか選択できないようです。\n\nアップロード後、処理が完了するまで最大24時間待ちます。\n\nファイルの大きさやタイミングによると思いますが、自分は3時間かかることもあればが、15分で終わることもあありました。\n\n### Batch APIダッシュボードから結果の.jsonlファイルをダウンロード\n\n処理が完了すると以下のようなUIとなり「Download output」より結果をダウンロードできます\n\n![alt text](images/2025-03-02-open-ai-batch-api/スクリーンショット_2025-02-22_15.22.02.png)\n\nダウンロードされたファイル\n\n```json\n{\"id\": \"batch_req_67b613c5e9188190932e345eb13da6d1\", \"custom_id\": \"0\", \"response\": {\"status_code\": 200, \"request_id\": \"c83d0bac65b0fc758f0217d67426cf63\", \"body\": {\"object\": \"list\", \"data\": [{\"object\": \"embedding\", \"index\": 0, \"embedding\": [0.022931391, ... ], \"model\": \"text-embedding-3-small\", \"usage\": {\"prompt_tokens\": 930, \"total_tokens\": 930}}}, \"error\": null}\n{\"id\": \"batch_req_67b613c7247481909a98dd2762d6a38c\", \"custom_id\": \"1\", \"response\": {\"status_code\": 200, \"request_id\": \"941ebd3de9c6f6cafa5658c708d91c1d\", \"body\": {\"object\": \"list\", \"data\": [{\"object\": \"embedding\", \"index\": 0, \"embedding\": [-0.007332558, ...], \"model\": \"text-embedding-3-small\", \"usage\": {\"prompt_tokens\": 294, \"total_tokens\": 294}}}, \"error\": null}\n{\"id\": \"batch_req_67b613c734388190add5465b04db485a\", \"custom_id\": \"2\", \"response\": {\"status_code\": 200, \"request_id\": \"6ae9f597f1fa88e9b9238ac9a8a9638b\", \"body\": {\"object\": \"list\", \"data\": [{\"object\": \"embedding\", \"index\": 0, \"embedding\": [0.019804584, ...], \"model\": \"text-embedding-3-small\", \"usage\": {\"prompt_tokens\": 439, \"total_tokens\": 439}}}, \"error\": null}\n```\n\n### node.jsスクリプトでダウンロードした.jsonlファイルを読み込み、結果をpostgresに格納\n\n```tsx\nimport { prisma } from \"@gizilog/shared/prisma\";\nimport fs from \"fs/promises\";\nimport path from \"path\";\nimport { ebeddingBulkInsert } from \"@prisma/client/sql\";\n\ntype BatchResponseJson = {\n  id: string;\n  custom_id: string;\n  response: {\n    status_code: 200;\n    request_id: string;\n    body: {\n      id: string;\n      object: \"list\";\n      model: \"text-embedding-3-small\";\n      data: [{ object: \"embedding\"; index: number; embedding: number[] }];\n    };\n  };\n  error: null;\n};\n\nexport async function output() {\n  const filePath = path.resolve(\n    __dirname,\n    \"./openai-batch-data/output/requests_embedding_1.jsonl\"\n  );\n  const content = await fs.readFile(filePath, \"utf-8\");\n\n  const updates: { id: string; embedding: number[] }[] = [];\n\n  for (const line of content.split(\"\\n\")) {\n    try {\n      const json = JSON.parse(line) as BatchResponseJson;\n      const embedding = json.response.body.data[0].embedding;\n      updates.push({ id: json.custom_id, embedding });\n    } catch (e) {\n      console.error(e);\n      console.log(\"line:\", line);\n    }\n  }\n  const batchSize = 15; // connection数の上限までまとめてアップロードする\n  for (let i = 0; i < updates.length; i += batchSize) {\n    const batch = updates.slice(i, i + batchSize);\n    // speechid と embedding の配列を Postgres の `UNNEST` に渡す形式に変換\n    const speechIds = batch.map((update) => update.id);\n    const embeddings = batch.map((update) => JSON.stringify(update.embedding)); // JSON に変換して PostgreSQL 側で `::vector(1536)` にキャスト\n\n    await prisma.$queryRawTyped(\n      embeddingBulkInsert(speechIds, embeddings)\n    );\n  }\n\n  // 処理したoutputファイルを判別するため `.done` ファイルを配置する\n  const doneFilePath = filePath + \".done\";\n  await fs.writeFile(doneFilePath, \"\");\n  console.log(\"is successfully feeded\");\n}\n\n```\n\nembeddingBulkInsertのSQLは以下です。\n\n```sql\nUPDATE sample_table\nSET embedding = embeddings.embedding::vector(1536)\nFROM (SELECT UNNEST($1::text[]) AS id, UNNEST($2::vector(1536)[]) AS embedding) AS embeddings\nWHERE sample_table.id = embeddings.id;\n```\n\nPrismaのTypedSQLを利用する際は以下が参考になります。利用する前に`prisma generate --sql` を実行する必要があります。\n\n<Bookmark href=\"https://zenn.dev/tockn/articles/0e6eac6220e072\" />\n","date":"2025-03-01T18:36:53.000Z"},{"slug":"apache-pulsar-catchup","title":"Apache Pulsarをローカルで立ち上げ、Spring for Apache Pulsarを動作確認するメモ","tags":["tech","java","pulsar"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n# Apache Pulsarとは  \n  \n  \nリアルタイムのデータストリーミングとメッセージングを扱うためのオープンソースの分散メッセージングシステム。Apache Kafkaに比べ、柔軟性や拡張性が求められるユースケースに適しています。マルチテナントの運用やスケーラブルなストレージ管理が必要な場合に特に有利。  \n  \n  \nトピック名が階層化されておりマルチテナントしやすい（プロパティ/ネームスペース/デスティネーション でトピック名が階層化されている）  \n  \n  \nまた、ジオレプリケーション機能が備わっている。  \n  \n  \nspringプロジェクトでは、2022年からapache pulsarのサポートを開始している[https://spring.io/blog/2022/08/16/introducing-experimental-spring-support-for-apache-pulsar](https://spring.io/blog/2022/08/16/introducing-experimental-spring-support-for-apache-pulsar)  \n  \n  \n参考  \n  \n- 公式doc  \n    \n    <Bookmark href=\"https://pulsar.apache.org/docs/4.0.x/\" />\n  \n- Apache Pulsarのマネージメントサービスである[Astra Streaming](https://www.datastax.com/lp/astra-registration)を提供するDataStax者の方による記事。メッセージング及びストリーミングテクノロジーの概観から始まり、Apache Kafkaとの違いを取り上げながらApache Pulsarについてわかりやすく解説されている。  \n  \n    <Bookmark href=\"https://qiita.com/yoshiyuki_kono/items/839ca884eb52f6d0950e\" />\n  \n- Yahoo!デベロッパーネットワークが公開している以下のスライド群  \n[https://www.docswell.com/tag/Apache Pulsar](https://www.docswell.com/tag/Apache%20Pulsar)  \n- baeldungの解説  \n[https://www.baeldung.com/apache-pulsar  \n](https://www.baeldung.com/apache-pulsar)[https://www.baeldung.com/spring-boot-apache-pulsar](https://www.baeldung.com/spring-boot-apache-pulsar)  \n  \n# Apache Pulsarをローカルで立ち上げ、**Spring for Apache Pulsarを動作確認する**  \n  \n  \n### pulsarをsingle nodeで立ち上げる  \n  \n  \n公式のApache Pulsar distributionをダウンロードして解凍  \n  \n  \n```shell  \nwget https://archive.apache.org/dist/pulsar/pulsar-4.0.0/apache-pulsar-4.0.0-bin.tar.gz  \ntar xvfz apache-pulsar-4.0.0-bin.tar.gz  \n```  \n  \n  \n解凍された プロジェクトに移動し、standalone pulsarをstartする  \n  \n  \n```shell  \ncd apache-pulsar-4.0.0  \nbin/pulsar standalone  \n```  \n  \n  \n参考: [https://pulsar.apache.org/docs/4.0.x/getting-started-standalone/](https://pulsar.apache.org/docs/4.0.x/getting-started-standalone/)  \n  \n  \n### トピックを作成する  \n  \n  \n```shell  \nbin/pulsar-admin topics create persistent://public/default/my-topic  \n```  \n  \n  \n※ pulsarは自動的に存在しないトピックを作成する機能があるが、あえて明示的に前もってトピックを作成している  \n  \n  \n### spring の pulsar clientでメッセージのproduce/consumeする  \n  \n  \n[spring initializr](https://start.spring.io/) より、Dependenciesに**Spring for Apache Pulsar** を選択してプロジェクトを作成する  \n  \n  \n```java  \n@SpringBootApplication  \npublic class PulsarBootHelloWorld {  \n  \n    public static void main(String[] args) {  \n        SpringApplication.run(PulsarBootHelloWorld.class, args);  \n    }  \n  \n    @Bean  \n    ApplicationRunner runner(PulsarTemplate<String> pulsarTemplate) {  \n        return (args) -> pulsarTemplate.send(\"my-topic\", \"Hello Pulsar World!\");  \n    }  \n  \n    @PulsarListener(subscriptionName = \"hello-pulsar-sub\", topics = \"my-topic\")  \n    void listen(String message) {  \n        System.out.println(\"Message Received: \" + message);  \n    }  \n}  \n```  \n  \n  \n※ pulsarはデフォルトのlocalhost:6650で実行されている想定のため、追加の設定はしていない  \n  \n  \n以下コマンドでメッセージを手動でproduceすることで動作確認できる  \n  \n  \n```shell  \nbin/pulsar-client produce my-topic --messages 'Hello Pulsar World from CLI'  \n```  \n  \n","date":"2024-11-30T00:00:00.000Z"},{"slug":"create-kubernetes-operator","title":"k8s入門者が超シンプルなKubernetes Operatorを作ってk8sの理解を深める","tags":["kubernetes","tech"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n## はじめに  \n\nkubernetesの理解が甘い気がすると漠然と上司に相談したところ、Kubernetes Operatorの自作を勧められたので奮闘した記録です。  \n  \n  \n現在のKubernetesについてのレベル感としては「 [Kubernetes完全ガイド](https://www.amazon.co.jp/Kubernetes%E5%AE%8C%E5%85%A8%E3%82%AC%E3%82%A4%E3%83%89-%E7%AC%AC2%E7%89%88-Top-Gear-%E9%9D%92%E5%B1%B1/dp/4295009792) を辞書代わりに必死に調べながらなんとかな既にあるyamlファイルのチューニングっぽいことができる。」くらいです。  \n  \n  \napi-serverがあり、etcdという永続化層があり、kubectlするとapi-serverが叩かれているということはなんとなく知っているが実態としてどのようにpodたちが管理しているかはあまりわかっていないという状況でした。  \n  \n  \n今思うと、新卒になって買った技術書の中で最も[Kubernetes完全ガイド](https://www.amazon.co.jp/Kubernetes%E5%AE%8C%E5%85%A8%E3%82%AC%E3%82%A4%E3%83%89-%E7%AC%AC2%E7%89%88-Top-Gear-%E9%9D%92%E5%B1%B1/dp/4295009792) を開いている回数が多い気がする（開いている回数は多いが通読して理解しているわけではない🤪）  \n  \n  \n## Kubernetes Operatorとは   \n  \n  \nkubernetesが持つ制御ループという仕組みを使った拡張機能。公式Docの[Operator Pattern](https://kubernetes.io/ja/docs/concepts/extend-kubernetes/operator/)の部分に説明がある。  \n  \n\n### 制御ループ (Control Loop, Reconcile Loopとも呼ばれる)  \n  \n  \n常に制御の対象となるものを監視し、理想状態に近づける仕組み。  \n  \n- Kubernetes Controller内で使われる仕組み。（Kubernetes Controllerはリソースを監視して、登録された理想状態に近づける責務を持っている。 [https://kubernetes.io/ja/docs/concepts/architecture/controller/](https://kubernetes.io/ja/docs/concepts/architecture/controller/)）  \n- 「監視 → 分析 → アクション」のサイクルを繰り返し、現実状態（Actual State, Current State）を理想状態（Desired State）に近づける  \n- ロボット工学やオートメーション分野で使われている  \n  \n### Controllerとは  \n  \n- ControllerはController Managerの中に束ねられている  \n- Controllerは一つ以上のリソースを監視  \n- Controllerは理想状態と現実状態を近づける責務を持つ  \n- 理想状態: API serverから取得したKubernetesオブジェクトのspecフィールド  \n- 現実状態: API serverから取得できる実際の状態  \n  \nKubernetesアーキテクチャにおけるController Manager(c-m)の立ち位置を以下で参考にできます。etcdに保存されている理想状態に関する情報や、実際のNodeの情報をAPI serverを通じて取得している。  \n  \n![alt text](images/create-kubernetes-operator/components.png)\n  \n出展: [https://kubernetes.io/docs/concepts/overview/components/](https://kubernetes.io/docs/concepts/overview/components/)  \n  \n\nbuild-in controller: Controller Managerの中にデフォルトで含まれているController。Deployment, ReplicaSet, Podなどのbuild-in Resourceを管理している  \n  \n  \n### kubectlの内部の動き  \n  \n1. kuectlコマンドを実行する  \n`kubectl create deploy  test —image=nginx —replicas=1`  \n2. コマンドをAPI serverに対するPOSTリクエストに変換する  \n`POST apis/apps/v1/namaspaces/default/deployments`  \n3. API serverがdeploymentのインスタンスを作成し、etcdに保存する（API server自体は保存するだけでReplicaSetの起動等、その先の動作を行わない）  \n4. Deployment Controllerの制御ループにより、新たにdeploymentインスタンスが作成されたことを検知し、Deployment Controllerが新たなdeploymentインスタンスに対するReplicaSetを作成する  \n5. ReplicaSet Controllerの制御ループにより、新たにreplicaSetインスタンスが作成されたことを検知し、ReplicaSet Controllerが新たなreplicaSetに対するPodを作成する  \n  \n### Kubernetes Operatorとは（結論）  \n  \n  \nCustom ResourceとCustom Controllerを自分で作成し、Kuberneteのコンセプトに則って自分のロジックを実現する拡張機能。以下の2 stepで作成することができる。  \n  \n1. APIの拡張: Custom Resourceの定義・追加（ここで作成するCustom Resource DefinitionはCRDと略される）  \n2. 制御ループの追加: Custom Controllerの追加  \n  \n### Operator Hub  \n  \n  \n[https://operatorhub.io/](https://operatorhub.io/)  \n  \n  \n様々なKubernetes Operatorが公開されている。[**prometheus operator**](https://github.com/prometheus-operator/prometheus-operator/tree/main)等、皆さんがよくお世話になっているOperatorについてお調べることができる。  \n  \n  \n### Kubernetes Operatorを使う手順  \n  \n  \nCDRとCustom ControllerのdeployをすることでKubernetes Operatorが使えるようになる。  \n  \n  \nKubernetes Operatorインストール時にインストールされるリソースは以下。  \n  \n- CosutmResorceDefinition  \n- Deployment(ControllerはDeplymentで実行されることが多い)  \n- Controllerへ必要な権限を与えるためのリソース（RBAC）。ServiceAccount, Role(Cluster Role), RoleBiding(Cluster RoleBiding)  \n  \n## 簡単なKubernetes Operatorを作ってみる  \n  \n  \n### Custom Resourceを作成する  \n  \n  \nsample.crd.yaml  \n  \n  \n```yaml  \napiVersion: apiextensions.k8s.io/v1  \nkind: CustomResourceDefinition  \nmetadata:  \n  name: samples.example.com # <names.plural>.<group>  \nspec:  \n  group: example.com  \n  names:  \n    kind: Sample  \n    plural: samples  \n  scope: Namespaced  \n  versions:  \n    - name: v1alpha1  \n      served: true  \n      storage: true  \n      additionalPrinterColumns:  \n        - name: Test String  \n          jsonPath: .testString  \n          type: string  \n      schema:  \n        openAPIV3Schema:  \n          type: object  \n          properties:  \n            testString:  \n              type: string  \n  \n```  \n  \n  \nsample.yaml  \n  \n  \n```yaml  \napiVersion: example.com/v1alpha1  \nkind: Sample  \nmetadata:  \n  name: sample-resource-1  \ntestString: \"first sample resource\"  \n```  \n  \n  \n`Sample` crdを作成し、インスタンスを作成する  \n  \n  \n```shell  \n❯ kubectl apply -f sample.crd.yaml  \ncustomresourcedefinition.apiextensions.k8s.io/samples.example.com created  \n  \n❯ kubectl get crd                   \nNAME                  CREATED AT  \nsamples.example.com   2024-12-16T13:30:40Z  \n  \n❯ kubectl apply -f sample.yaml      \nsample.example.com/sample-resource-1 created  \n  \n❯ kubectl get samples     \nNAME                TEST STRING  \nsample-resource-1   first sample resource  \n  \n❯ kubectl get samples sample-resource-1 -o yaml  \napiVersion: example.com/v1alpha1  \nkind: Sample  \nmetadata:  \n  annotations:  \n    kubectl.kubernetes.io/last-applied-configuration: |  \n      {\"apiVersion\":\"example.com/v1alpha1\",\"kind\":\"Sample\",\"metadata\":{\"annotations\":{},\"name\":\"sample-resource-1\",\"namespace\":\"default\"},\"testString\":\"first sample resource\"}  \n  creationTimestamp: \"2024-12-16T13:30:49Z\"  \n  generation: 1  \n  name: sample-resource-1  \n  namespace: default  \n  resourceVersion: \"14316\"  \n  uid: a5e9cc5c-b53f-4199-8799-8f6c360b9d30  \ntestString: first sample resource  \n```  \n  \n  \n### Custom Controllerを作成する  \n  \n  \n簡単なControllerのため、1秒に一回制御ループを実行し、custom resourceのオブジェクトの一覧を表示する機能だけを持ちます。  \n  \n  \n```shell  \npackage main  \n  \nimport (  \n\t\"context\"  \n\t\"encoding/json\"  \n\t\"flag\"  \n\t\"fmt\"  \n\t\"path/filepath\"  \n\t\"time\"  \n  \n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"  \n\t\"k8s.io/apimachinery/pkg/runtime/schema\"  \n\t\"k8s.io/client-go/dynamic\"  \n\t\"k8s.io/client-go/tools/clientcmd\"  \n\t\"k8s.io/client-go/util/homedir\"  \n)  \n  \nvar gvr = schema.GroupVersionResource{  \n\tGroup:    \"example.com\",  \n\tVersion:  \"v1alpha1\",  \n\tResource: \"samples\",  \n}  \n  \ntype Sample struct {  \n\tmetav1.TypeMeta   `json:\",inline\"`  \n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`  \n  \n\tTestString string `json:\"testString\"`  \n}  \n  \ntype SampleList struct {  \n\tmetav1.TypeMeta `json:\",inline\"`  \n\tmetav1.ListMeta `json:\"metadata,omitempty\"`  \n  \n\tItems []Sample `json:\"items\"`  \n}  \n  \nfunc listSamples(client dynamic.Interface, namespace string) (*SampleList, error) {  \n\tlist, err := client.Resource(gvr).Namespace(namespace).List(context.Background(), metav1.ListOptions{})  \n\tif err != nil {  \n\t\treturn nil, err  \n\t}  \n  \n\tdata, err := list.MarshalJSON()  \n\tif err != nil {  \n\t\treturn nil, err  \n\t}  \n  \n\tvar sampleList SampleList  \n\tif err := json.Unmarshal(data, &sampleList); err != nil {  \n\t\treturn nil, err  \n\t}  \n\treturn &sampleList, nil  \n}  \n  \nfunc main() {  \n\tvar defaultKubeConfigPath string  \n\tif home := homedir.HomeDir(); home != \"\" {  \n\t\t// build kubeconfig path from $HOME dir  \n\t\tdefaultKubeConfigPath = filepath.Join(home, \".kube\", \"config\")  \n\t}  \n  \n\tkubeconfig := flag.String(\"kubeconfig\", defaultKubeConfigPath, \"kubeconfig config file\")  \n\tflag.Parse()  \n  \n\tconfig, _ := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig)  \n  \n\tclient, _ := dynamic.NewForConfig(config)  \n  \n\tfor {  \n\t\tsamples, _ := listSamples(client, \"\")  \n\t\tfor i, sample := range samples.Items {  \n\t\t\tnamespace := sample.GetNamespace()  \n\t\t\tname := sample.GetName()  \n\t\t\tfmt.Printf(\"%d\\t%s\\t%s\\n\", i, namespace, name)  \n\t\t}  \n\t\ttime.Sleep(1 * time.Second)  \n\t}  \n}  \n  \n  \n```  \n  \n  \n### Next Action  \n  \n  \n今回作成したKubernetes Operatorには以下のような課題があります。  \n  \n- custom resoruceオブジェクトが作成された場合に実施する、pod作成などのメインロジックが存在しない。  \n- ForループはAPI serverへの負荷となってしまう  \n- CRDとmain.go内で定義したSample structの整合性担保ができていない  \n- Fieldのデフォルト値やバリデーションがない  \n- Testコードがない  \n- Kubernetesクラスタ上で動かすためには、API Serverへのアクセス管理（RBAC）の考慮が必要  \n- …  \n  \n⇒ これらは全てのKubernetes Operatorを作る際の共通の課題なので、解決のためのライブラリが提供されている。⇒ [**kubebuilder**](https://github.com/kubernetes-sigs/kubebuilder)**などを利用した本格的なKubernetes Operator作成が始まる。**  \n  \n  \n## まとめ  \n  \n  \nなんとなくkubernetesの世界観がわかってきました。今のところアプリケーションレイヤーが本業なのでこれ以上は一旦踏み込めないなと感じています。業務でよく利用されている [prometheus](https://github.com/prometheus-operator/prometheus-operator)や[argo-rollouts](https://argo-rollouts.readthedocs.io/)などのkubernetes operatorのドキュメント等が読みやすくなったのがとても良いなと思います。  \n  \n  \n### 参考  \n  \n- [**オペレーターパターン**](https://kubernetes.io/ja/docs/concepts/extend-kubernetes/operator/)  \n- [**Kubernetes Operator 超入門/Kubernetes_Operator_Introduction**](https://speakerdeck.com/oracle4engineer/kubernetes-operator-introduction)  \n- [**OSC2020 Online Fukuoka Kubernetes Operator Intuition**](https://speakerdeck.com/nwiizo/osc2020-online-fukuoka-kubernetes-operator-intuition)  \n- 以下の動画は超絶おすすめです。本記事もこちらの動画の流れに沿った内容となっていますm  \n  \n\t<Bookmark href=\"https://www.udemy.com/course/kubernetes-operator-basics/\" />\n  \n  \n\trepository: [https://github.com/nakamasato/kubernetes-operator-basics/tree/main](https://github.com/nakamasato/kubernetes-operator-basics/tree/main)  \n  \n","date":"2024-12-16T00:00:00.000Z"},{"slug":"exec-py-by-uv-on-github-actions","title":"uvでプロジェクト管理しているPythonスクリプトをgithub actionsで実行する","tags":["tech","python","uv","GithubActions"],"content":"\n  \nuvでプロジェクト管理しているPythonスクリプトをGitHub Actionsで実行する環境を整えたので紹介します。  \n  \n  \nGithub Actionsは無料でちょっとしたスクリプトの定期実行にとても便利なので重宝しています。  \n  \n  \n今回はChatGPT apiを定期的に叩くタスクを想定して、OpenAIのAPI KeyをGithub Secretsに格納して参照することろまで解説します。  \n  \n  \n## プロジェクト概要  \n  \nディレクトリ構造  \n  \n  \n```shell  \n~/py-batch   \n❯ tree -L 1 -a  \n.  \n├── .python-version  \n├── .venv  \n├── main.py  \n├── pyproject.toml  \n└── uv.lock  \n```  \n  \n  \npyproject.toml  \n  \n  \n```shell  \n[project]  \nname = \"py-batch\"  \nversion = \"0.1.0\"  \nrequires-python = \">=3.9\"  \ndependencies = [  \n    \"langchain>=0.3.14\",  \n    \"openai>=1.59.6\",  \n]  \n  \n[dependency-groups]  \ndev = [  \n    \"transformers>=4.47.1\",  \n]  \n```  \n  \n  \nmain.py  \n  \n  \n```shell  \nimport os  \n  \nAPI_KEY = os.environ.get(\"API_KEY\")  \n  \nprint(\"Hello from github actions!\")  \n```  \n  \n  \n## 利用する setup テンプレート  \n  \n  \nuvの開発元であるAstral社が以下でgithub actions用のsetupを提供しているのでこちらを利用します。  \n  \n  \n[https://github.com/astral-sh/setup-uv](https://github.com/astral-sh/setup-uv)  \n  \n  \n## ワークフローyaml全体  \n  \n  \nコード全体は以下になります。  \n  \n  \n```yaml  \nname: py-batch  \non:  \n  workflow_dispatch:  \non:  \n  schedule:  \n    - cron: \"0 0 * * *\" # 毎日 0時 に実行  \njobs:  \n  command:  \n    name: exec python script  \n    runs-on: ubuntu-latest  \n    steps:  \n      - name: Checkout  \n        uses: actions/checkout@v4  \n      - name: Install the latest version of uv  \n        uses: astral-sh/setup-uv@v5  \n        with:  \n          version: \"latest\"  \n      - name: run python script  \n        env:  \n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}  \n        run: uv run main.py  \n```  \n  \n  \nactions/checkout stepでコードをcloneし、 astral-sh/setup-uv stepを入れることで簡単にuv環境を整えることができます。  \n  \n  \nastral-sh/setup-uv を用いている場合、GitHub Actions上でpython環境を整える際によく利用する [actions/setup-python](https://github.com/actions/setup-python) をstepに加える必要はないようです。  \nPythonのバージョンを指定してたい場合は、uvコマンド経由で指定することが可能です。  \n  \n  \n```yaml  \n- name: Install the latest version of uv  \n  uses: astral-sh/setup-uv@v5  \n  with:  \n    enable-cache: true  \n- name: Install Python 3.12  \n  run: uv python install 3.12  \n```  \n  \n  \n無事実行することができました  \n\n![alt text](images/exec-py-by-uv-on-github-actions/exec-py-console.png)\n  \n### 最後に  \n  \n  \n雑談ですがuvの環境を整えていて思ったこと↓  \n  \n<TweetEmbed url=\"https://x.com/soken_nowi/status/1877913565216284972\" />  \n  \n  \n以下のようにあるのでnode.jsプロジェクトにおける`npm install`的なstepを踏まなくて良いのは便利だなと  \n  \n  \n> When used in a project, the project environment will be created and updated before invoking the command.  \n  \n  \n[https://docs.astral.sh/uv/reference/cli/#uv-run](https://docs.astral.sh/uv/reference/cli/#uv-run)  \n  \n","date":"2025-01-11T00:00:00.000Z"},{"slug":"ir-system-book","title":"【読書感想】『検索システム ― 実務者のための開発改善ガイドブック』メモ、感想","tags":["bookreview","tech"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n# はじめに  \n  \n  \n検索システムは、現代のアプリケーションやウェブサービスにおいて不可欠な要素です。SNSやECサイトをはじめ、大部分のwebサービスには検索システムが実装されており、効率的な検索体験を提供することは、ユーザーの満足度や利便性に直結します。  \n  \n  \n今回は、『検索システム ― 実務者のための開発改善ガイドブック』を読み、その内容をまとめ、私が感じたことを共有したいと思います。本書は実務に即したアプローチで、検索システムの設計や改善方法について解説しており、開発者目線でとても有益な内容が詰まっています。  \n\n<Bookmark href=\"https://amzn.to/4gXQrfD\" />\n  \n# 読書時のメモ  \n  \n  \n## 第1章 イントロダクション  \n  \n  \nRDBへのクエリをそのまま検索システムとして使おうとすると生じる「キーワードの表記揺」、「ランキング」、「レイテンシ」、「フレーズ検索」について触れ、全文検索エンジンの導入となる章。  \n  \n  \n**検索の方式**  \n  \n- 逐次方式: 検索対象から文字列のマッチングでキーワードを探し出す単純な手法。unixのgrepは逐次方式の検索。  \n- 牽引方式: データベースを構築する段階で「キーワードごとに対象となるドキュメントを並べた形式のデータ」を牽引として用意し、（この牽引を**転置インデックス**と呼ぶ）転置インデックスを利用して高速にドキュメントを探す手法  \n  \n<u>転置インデックスはデータ構造の一つ。転置インデックスを格納して、クエリで問い合わせるという性質上、検索エンジンはデータベースの一種と言える。</u>  \n  \n  \n**検索機能を作成するために考慮する必要があること**  \n  \n- データを検索エンジンにどのように格納するか（**データモデリング**や**インデクシング）**  \n- ユーザーに検索機能をどのように使ってもらうか（**UI/UX**や**検索を支援するための機能**）  \n- 検索機能に関係する多様な要素をどうやって管理していくか（**プロジェクトマネジメント**）  \n  \nさらに、検索の歴史と検索システムの全体像と構成要素について前触れがある。  \n検索システムの構成要素として以下が紹介される。  \n  \n- **検索エンジン**  \n- **インデクサ**: 検索対象のデータの取得、適切な形式への加工、インデックスへの登録を担う  \n- **クエリプロセッサ**: ユーザーによって入力されたクエリの前処理を行う  \n- **クエリポストプロセッサ**: 検索結果の後加工を行う  \n- **「データ活用基盤」および「適合性コントローラー（Relevance Controller）」**: より良い検索を実現するため、検索結果に対するユーザーのフィードバック、サービス提供側のニーズに基づいて検索システムを最適化する仕組み  \n  \n（可能ならばここに図を置きたい）  \n  \n  \n2章から続くいくつかの章は「現代の検索エンジンがどのような課題を意識して実装されているか」を解説する章となる。  \n  \n  \n## 第2章 検索エンジンの仕組み  \n  \n  \n主題: 「そもそも全文検索エンジンがどのようなソフトウェアなのか、特に全文検索エンジンのデータ構造として最も広く使われている**転置インデックス**を利用した検索の大まかなフローを解説」  \n  \n  \n### 転置インデックスとは  \n  \n  \n前提として、逐次検索では実現の難しい要件を整理する  \n  \n- 大規模テキストを高速に検索する  \n- フレーズ検索を行う（ある単語とある単語が隣り合うか、または近くに出現するかのようなパターン検索）  \n- 検索結果にランキングを付けたい  \n  \nこれらを実現する仕組みとして転置インデックス（**inverted index**）がある。  \n  \n  \nドキュメントを牽引語に分割し、牽引語を行、ドキュメントIDを列として、「牽引語がドキュメントに出現すれば 1 、出現しない場合は 0」のような行列が最も簡単な転置インデックス。  \n  \n  \n「Webページのランキングアルゴリズム PageRank はラリー・ペイジらによって開発された」  \n  \n  \n↓ 牽引語に分割  \n  \n  \n「Web / ページ / の / ランキング / アルゴリズム / pagerank / は / ラリー / ・ / ペイジ/ ら / によって / 開発 / さ / れ / た」  \n  \n  \n（牽引後の分割方法は第3章を参照。形態素解析やN-Gramなどのアルゴリズムが用いられることが一般的。）  \n  \n  \n|        | **Doc1** | **Doc2** | **Doc3** |  \n| ------ | -------- | -------- | -------- |  \n| web    | 1        | 1        | 0        |  \n| 検索     | 1        | 1        | 1        |  \n| アルゴリズム | 1        | 0        | 1        |  \n| エンジン   | 1        | 1        | 1        |  \n  \n  \nこのような行列の転置インデックスを用いると、牽引語「web」が出現しつつ、牽引語「検索」も出現する AND検索や、どちらかが出現する OR検索、出現しないことを指定する NOT 検索が可能になる（検索処理を集合演算に落とし込めるということ。AND検索、OR検索、NOT検索はまとめて**ブール検索モデル**という。）  \n  \n  \n上のような「牽引語-ドキュメント行列」の転置インデックスでは、大規模なテキストコーパスではほとんどの要素が 0 になってしまいメモリ効率が悪くなる。そのため、実用的な転置インデックスとして、**ターム辞書（term dictionary）**と**ポスティングリスト（posting list）**を用いた転置インデックスがある。  \n  \n- **ターム辞書**: は牽引語をkey、ポスティングリストをvalueとしたデータ構造。  \n- **ポスティングリスト**: その牽引語が出現しているドキュメントIDと出現位置のリスト。  \n  \n```typescript  \n{  \n \"web\" : [1:[3], 2:[1], 3:[6]],  \n \"検索\" : [1:[4], 3:[1,7,10,16], 4:[14]]  \n}  \n```  \n  \n  \n### 転置インデックスを用いた検索  \n  \n1. ユーザーが検索のために入力した文字列（検索クエリ）をターム（牽引語）に分割する（**テキスト解析**）  \n2. ターム辞書を引き、検索対象のポスティングリストを見つける（**辞書引き**）  \n3. ポスティングリストを先頭から走査してまーじ（**ポスティングリスト走査**） ポスティングリスト走査の詳細については第4章を参照  \n4. 見つかった検索結果をランキングにして返す（**ランキング**）  \n  \nこの一連の流れはクエリ処理（query processing）と呼ぶ。  \n  \n  \n### テキストデータ以外のインデックス  \n  \n  \n転置インデックスを利用すると、テキストデータの全文検索が行えることがわかりました。しかし、検索システムでは全文検索のみでは成り立たず、テキスト以外のデータの検索を組み合わせる必要があります。  \n  \n  \ne.g.) ECサイトの検索では、商品のカテゴリや価格による絞り込み（フィルタリング）やソート、ファセット機能（6章を参考）を組み合わせた検索が必須  \n  \n  \ne.g.) ホテルの空室検索では、予約したい日付による検索が必須  \n  \n  \ne.g.) レストランや店舗検索では、ユーザーの地理情報を考慮した検索を実現したい  \n  \n  \n### 無償で利用可能な検索エンジンの紹介  \n  \n- Apache Lucene  \n単体ではサーバーとしての機能を持たないため、Apache Luceneをベースとした検索エンジンサーバーである、Apache SolrやElasticsearch を用いることが多い  \n- Vespa  \n- Groonga  \n  \n※ 転置インデックスの特徴  \n  \n- インメモリでもディスクベースでも、そのハイブリットでも動作する  \n- 一台のマシーンで扱いきれない大規模なデータを扱う際に、水平分散によりスケールしやすい  \n  \n## 第3章 テキスト解析  \n  \n  \n主題:「全文検索エンジンにおいてテキストがどのように扱われるか」を扱う  \n  \n  \nテキスト解説は大きな処理として考えると次のような流れになる  \n  \n- 文字の正規化  \n- トークン化  \n- トークン列に対する各種処理  \n  \n※ トークン（牽引語）はテキストを構成する基本的な単語や記号であり、タームは検索の目的でインデックスされる特定の単語やフレーズを指します。  \n  \n  \n対象となるドキュメントの言語によってトークンの決め方は異なり、それぞれに対して独自のアルゴリズムが発展している。  \n  \n  \n### 日本語のテキスト解析  \n  \n  \n日本語のテキスト解析には以下の二つの手法が用いられることが多い  \n  \n- **形態素解析器を用いたテキスト解析**  \n  \n日本語の文章を人間が認識できる単語へと区切る手法。  \nOSS: [MeCab](https://taku910.github.io/mecab/)   \n  \n- **文字N-Gramを用いたテキスト解析**  \n  \nN-Gramでは入力された文章をN文字ずつ切り出して切り出してトークンとして出力する。  \n  \n  \nその他、Unicode正規化、ストップワード、類義語の展開など、検索エンジンの精度を高めるためのテキスト解析の工夫うが紹介されている。  \n  \n  \n## 第4章 ポスティングリストの走査とランキングのアルゴリズム  \n  \n  \n主題: 「転置インデックスからドキュメントを取得する際にどのようなアルゴリズムやロジックが使われているか」  \n  \n  \nこれまでの章で全文検索エンジンの主な仕組みは、転置インデックス上でテキストをタームの集合として扱い、ターム集合から構築したポスティングリストを走査することで目的の検索結果を取得するというところまでわかりました。本章ではポスティングリスト走査とランキングのアルゴリズムについて解説されています。  \n  \n  \n### ポスティングリストの走査  \n  \n  \n図での解説が必要なため、まとめられず。  \n  \n  \n### ランキングを考慮したポスティングリストの走査  \n  \n  \nTF(term frequency)とIDF(inverse document frequency)を考慮しながらポスティングリストを走査することで、走査時の結果が重み付けされランキングを考慮した結果となる。  \n  \n  \n### パフォーマンスの評価  \n  \n  \n検索システムにおいて重視されるのレイテンシ（Latency）とスループット（throughput）  \n  \n  \nクエリ処理時はレイテンシのみが重要視され、インデクシング時はレイテンシとスループットの両方が重視される。  \n  \n  \nまた、大量のデータを扱う検索システムでは、ディスク I/O のレイテンシやスループットがパフォーマンスを下げる大きな要因になりうるため、注意が必要。  \n  \n  \nよって、検索システムのパフォーマンス試験を実施するに当たっては以下項目を見るのが良いとされる  \n  \n- レイテンシ  \n- スループット  \n- CPUおよびメモリの使用量  \n- ディスク I/O の回数測定  \n- （Kubernetes環境では）ネットワークレイテンシやDNS名前解決などのTCP通信  \n  \n### キャッシュの利用  \n  \n  \n検索エンジン内に実装されるキャッシュの種類  \n  \n- 検索結果のキャッシュ  \n- クエリ途中結果のキャッシュ  \n具体的には、ポスティングリストの積集合や絞り込み条件、ソート条件がキャッシュ対象。  \n- ドキュメントのキャッシュ  \nインデクシング元のテキストそのものをキャッシュし、ユーザーに返却するスニペットの生成を高速化するキャッシュ  \n- etc…  \n  \n検索結果のキャッシュは、キャッシュのヒット率が低いことが多く、不要なデータをキャッシュから追い出す処理（eviction）のオーバーヘッドがキャッシュ効率を上回ることが多い。**クエリの特性に合わせて慎重にキャッシュ戦略を選ぶ必要がある。**  \n  \n  \nまた、リアルタイム性が重視されるシステムではそおそもキャッシュを導入しない方が良い場合もあるので注意。  \n  \n  \nキャッシュを導入する前に**遅いクエリを分析するしてクエリそのものを改善できないかを検討**することが重要。  \n  \n  \n### インデックスの冗長化と分散検索  \n  \n  \n1台の検索エンジンでは扱いきれないほど大量のデータやトラフィックを捌く必要が出てきた場合、一部で障害が発生した場合でも処理を継続できるようシステムとデータを冗長化しておく必要が出てきます。インデックスのレプリケーション、シャーディングを検討できる。  \n  \n  \n## 第5章 検索エンジンへのデータ登録  \n  \n  \n主題: 「転置インデックスでの利用を前提として、そもそも検索エンジンに登録すべきデータをどのような手段で準備すればいいか」  \n  \n  \nECの場合、検索結果で1件として扱いたいデータをどのように考えるか  \n  \n- 1SKU（Stock Keeping Unit）で扱う: Tシャツのカラーが3パターンでサイズが3種類の合計9種類のTシャツをそれぞれ1つとして扱い、9件を検索結果にヒットさせる  \n- そのTシャツ商品を1件のデータとして扱う: 検索結果の件数はSKUの数ではなく、1件のみ。サイズ、カラーなどはその商品の属性として表示される  \n  \n不要なものは登録しない  \n  \n- 検索結果として十分な品質がないデータ  \n- 何年間も非アクティブなユーザーによるデータ  \n- 検索結果に一定期間表示されていないデータ  \n  \n検索エンジンのデータを更新するタイミング  \n  \n- リアルタイム: ECサイトで在庫の状況を反映する必要がある場合は必須  \n- バッチ更新  \n- 手動更新  \n  \nデータを更新する度に内部で小さな転置インデックスが作成されるため、リアルタイムよりバッチの方が検索効率がよくなる場合がある。また削除操作も、転置インデックス内に利用されないデータが蓄積されるため、検索効率への影響が出る可能性がある。  \n  \n  \n代表的なデータソース  \n  \n- ファイルシステム:  \n- データベース: SQLを利用して収集。同期的 or 非同期的に検索エンジンにデータを反映させるかの判断が必要。また、非正規化データを扱う場合、一つのデータが更新された場合にそれに関連するすべてのデータを検索エンジンに反映させる際に注意が必要  \n- Web: クローラーによる収集など  \n  \nファイルシステムからのコンテンツ抽出にはApache Tikaが良さそう。PDF, PowerPoint, Word等のファイルからコンテンツとメタデータを構造化データとして抽出し、Apache Solar等の検索エンジンの転置インデックスとして登録する仕組みがすでに用意されている。  \n  \n  \n## 第6章 検索インターフェースと検索クエリの処理  \n  \n  \n検索システムへのユーザーからの入力は大きく分けて以下の二つ  \n  \n- 検索クエリ: 検索したいキーワード、検索対象とするカテゴリなどのフィルタ条件、ソート指示など  \n- コンテキスト: ユーザーの行動履歴、性別、年齢、登録情報など  \n  \n検索を「ユーザーが求める情報を得るための手段」と考えると、キーワードやフレーズでの全文検索は必ずしも最適な解決方法ではない。ECサービスでウィンドウショッピングする際は全文検索よりもカテゴリによるナビゲーションの方が検索行動をフォローする重要な機能である可能性がある。  \n  \n  \n### ファセット  \n  \n  \nファセットナビゲーションとは、ユーザーが検索クエリを明確にしたり絞り込みした入できるように、検索結果のドキュメントについているメタデータを集計して視覚的に選択肢を提示する仕組みです。  \n  \n  \nファセットの役割の一つは、ユーザーの検索クエリの作成および修正をナビゲートすることです。ユーザーに対してファセットで簡単な絞り込みの手段を提供することで、少数のキーワードから検索を始め、少しずつ検索条件を絞っていくという検索体験を提供できる。  \n  \n  \nファセットを再有する場合には、選択済みの項目をパンくずリストやチェックボックスとしてページに表示しておくことや、各ファセットのマッチする件数を表示することがとても重要。ユーザーが次の行動を判断する指標となる。  \n  \n  \nファセットで集計するメタデータは、ユーザーの情報要求を明確にできるもの、検索効率を上げるようなものを慎重に選ぶ必要がある。メタデータの選択を誤るとユーザーに混乱を招くこととなる。  \n  \n  \nこれから先は完全ネタバレ要約みたいになってしまうのでメモも公開しません。ぜひ以下でお買い求めください。  \n  \n\n<Bookmark href=\"https://www.lambdanote.com/products/ir-system\" />\n  \n## 語彙  \n  \n- テキストコーパス: 検索の対象となるテキストの集合  \n- 文章サロゲート: 検索結果のタイトル、要約、メタデータなどの情報の集合。検索結果が目的の文章であるかどうかをユーザーが判断するために使う情報。Google検索したときにタイトル周辺に出る情報。  \n- スラッシング（thrashing）: スペルミスなど誤った検索クエリを解決しないまま検索を繰り返す行動。スラッシングの発生は検索システムにおけるアンチパターンであり、オートコンプリートはこれを防止する重要な役割がある  \n- クリックエントロピー: 同じ検索クエリを実行した異なる検索者が検索結果から選択したドキュメントの散らばり具合  \n- ストップワード:  頻繁に現れ、テキスト検索処理に関連する内容を もたないワードのこと。英語では \"and\", \"or\",  \"in\" などで、日本語では ”て”, “に”, “を”, “は” などがストップワードにあたる。  \n  \n# 感想  \n  \n  \n『検索システム ― 実務者のための開発改善ガイドブック』は、理論と実務の両方をバランスよくカバーしており、エンジニアとして検索システムを構築する際に非常に役立つ内容が満載でした。  \n  \n  \n本記事のメモでは言及していませんが、後半部分は検索システムのプロジェクト管理や、検索クエリの処理（機械学習のアプローチも含む）など検索体験向上の様々なアプローチが紹介されています。  \n  \n  \nより洗練された検索機能を提供できるよう、検索システムに関する理解を深めたい方には、ぜひ一読をお勧めします！  \n  \n  \nちなみにQiitaで100いいね近くでバズっている以下記事も、~~出典明記されていませんが~~、構成や具体例が一致しているためこちらの本の要約になっているようです。  \n  \n  \n<Bookmark href=\"https://qiita.com/k_yamaki/items/2bd2284a2ddee542ad4d\" />\n","date":"2024-10-06T00:00:00.000Z"},{"slug":"next-blog-summary","title":"ほぼ無料で構築した個人ブログで1円稼ぐことに成功したのでやったことをまとめる","tags":["poem","tech","next"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n本ブログの今月の収益🎉  \n  \n![alt text](images/next-blog-summary/estimated-blog-income.png)  \n  \n今年の7月あたりから個人ブログの作成を少しづつ始め、Google Adsenseの審査に通り、0→1 を達成することができたので、やったことをまとめてみます。  \n  \n  \n⚠️ ジュニアエンジニアが自分自身の成長のために作成したブログでたまたま収益が1円になっただけです。ブログでたくさん稼ぎたい方にとってはあまり参考になる内容はないと思いますm  \n自分自身の継続のためにも好奇心で Google Adsense の審査を出すと通ったので、少しだけ広告を入れさせてもらっている状況です。  \n  \n  \n# まとめ  \n  \n- Next.jsでブログを構築した  \n\t- notionで記事を書き、github actionの定期実行から notion api経由でmdファイルを保存することで記事を管理している  \n\t- notionの画像データのURLは、notionの仕様によるものなのかがすぐにURLが変わってリンク切れしてしまうため、画像データはbase64エンコードしてmdファイル内に含めた  \n\t- その他、TOCの自動生成、ダークモード、rssフィードなど普通のブログにありそうな機能を実装した  \n- ホスティング先はNetlifyの無料プラン（Vercelの無料プランだと広告を載せられない）  \n- CMSにnotion, ホスティングにNetlifyという構成なので、ドメイン取得料以外は無料  \n  \n# 個人ブログを作ろうと思った背景  \n  \n- 勉強したことのアウトプットを形として残しておきたい  \n\t- ブログを始めると、ネタ作りのためにも勉強意欲になる  \n\t- いつか転職を検討する際に役立つかも  \n- つよつよで影響力のあるエンジニアは個人ブログを持っていることが多い  \n- 仕様書や技術的なドキュメントを作成する際の文章力を鍛えたい（かっこよく言えばテクニカルライティング）  \n- Next.jsで何か作りたい  \n  \n# やったこと  \n  \n  \n## Next.jsでブログのベース機能の作成  \n  \n  \n### contentlayerによるmdファイルには管理  \n  \n  \n公式doc: [https://contentlayer.dev/docs](https://contentlayer.dev/docs)  \n  \n  \n参考記事:  \n  \n\n<Bookmark href=\"https://zenn.dev/you_5805/articles/contentlayer\" />\n  \n設定ファイルは以下  \n  \n  \n```typescript  \n// contentlayer.config.ts  \nimport { defineDocumentType, makeSource } from \"contentlayer/source-files\";  \nimport { remark } from \"remark\";  \nimport strip from \"strip-markdown\";  \nexport const Post = defineDocumentType(() => ({  \n  name: \"Post\",  \n  filePathPattern: `*.md`,  \n  fields: {  \n    title: { type: \"string\", required: true },  \n    date: { type: \"string\", required: true },  \n    updatedAt: { type: \"string\", required: false },  \n    tags: { type: \"list\", of: { type: \"string\" }, required: true },  \n    isDeleted: { type: \"boolean\", required: false },  \n    isPublished: { type: \"boolean\", required: false },  \n    publishedAt: { type: \"string\", required: false },  \n    views: { type: \"number\", required: false },  \n  },  \n  computedFields: {  \n    url: {  \n      type: \"string\",  \n      resolve: (post) => `/posts/${post._raw.flattenedPath}`,  \n    },  \n    slug: {  \n      type: \"string\",  \n      resolve: (post) => post._raw.sourceFileName.replace(/\\.md$/, \"\"),  \n    }  \n  },  \n}));  \n  \nexport default makeSource({  \n  contentDirPath: \"../contents\",  \n  documentTypes: [Post],  \n});  \n```  \n  \n  \n### react-markdownとreact-syntax-highlighterによるレンダリング  \n  \n  \n公式doc: [https://github.com/remarkjs/react-markdown](https://github.com/remarkjs/react-markdown)  \n  \n  \n```typescript  \nimport ReactMarkdown from \"react-markdown\";  \nimport remarkGfm from \"remark-gfm\";  \nimport rehypeSlug from \"rehype-slug\";  \nimport rehypeRaw from \"rehype-raw\";  \nimport { Prism as SyntaxHighlighter } from \"react-syntax-highlighter\";  \nimport { oneDark } from \"react-syntax-highlighter/dist/cjs/styles/prism\";  \n  \n  \n<ReactMarkdown  \n  className=\"post prose dark:prose-invert\"  \n  urlTransform={(value: string) => value} // base64形式の画像に対応 ref. https://github.com/remarkjs/react-markdown/issues/774  \n  rehypePlugins={[rehypeRaw, rehypeSlug]}  \n  remarkPlugins={[remarkGfm]}  \n  components={{  \n    p: Paragraph,  \n    code: ({ node, className, children, style, ref, ...props }) => {  \n      const match = /language-(\\w+)/.exec(className || \"\");  \n      return match ? (  \n        <SyntaxHighlighter  \n          language={match[1]}  \n          PreTag=\"div\"  \n          {...props}  \n          style={oneDark}  \n        >  \n          {String(children).replace(/\\n$/, \"\")}  \n        </SyntaxHighlighter>  \n      ) : (  \n        <code className={className} {...props}>  \n          {children}  \n        </code>  \n      );  \n    },  \n  }}  \n>  \n  {post.body.raw}  \n</ReactMarkdown>  \n```  \n  \n  \n[react-markdwon](https://github.com/remarkjs/react-markdown)でのレンダリングでは base64形式で画像を指定する `data:` protocol はセキュリティの観点からデフォルトでサポート対象のため、以下のようにurlTransformを上書きする必要があります  \n  \n  \nref: [https://github.com/remarkjs/react-markdown/issues/774](https://github.com/remarkjs/react-markdown/issues/774)  \n  \n  \n```typescript  \n<ReactMarkdown  \n  className=\"post prose dark:prose-invert\"  \n  urlTransform={(value: string) => value} // base64形式の画像に対応  \n/>       \n```  \n  \n  \n### TOCの追加  \n  \n  \n公式doc: [https://tscanlin.github.io/tocbot/](https://tscanlin.github.io/tocbot/)  \n  \n  \n参考記事:   \n  \n  \n<Bookmark href=\"https://zenn.dev/yyykms123/articles/2023-12-03-adding-toc-to-nextjs-blog-using-tocbot\" />\n\n  \n実装は以下  \n  \n  \n```typescript  \n\"use client\";  \n  \nimport { useEffect } from \"react\";  \nimport tocbot from \"tocbot\";  \n  \nexport const PcToc: React.FC = () => {  \n  useEffect(() => {  \n    tocbot.init({  \n      tocSelector: \".toc\",  \n      contentSelector: \".post\", // 目次を抽出したい要素のクラス名  \n      headingSelector: \"h1, h2, h3\",  \n      scrollSmoothOffset: -60,  \n      headingsOffset: 60,  \n      scrollSmoothDuration: 300,  \n    });  \n  \n    return () => tocbot.destroy();  \n  }, []);  \n  \n  return (  \n    <div className=\"sticky top-0\">  \n      <h2 className=\"text-xl border-l-4 pl-1\">目次</h2>  \n      <div className=\"toc px-0 pb-8 text-base\"></div>  \n    </div>  \n  );  \n};  \n```  \n  \n  \n### next/ogによるOGPの画像の自動生成  \n  \n  \n公式doc: [https://github.com/vercel/next.js](https://github.com/vercel/next.js)  \n  \n  \n参考:   \n  \n  \n<Bookmark href=\"https://nextjs.org/docs/app/api-reference/file-conventions/metadata/opengraph-image\" />\n  \n  \n実装（style部分は長くなるため省略）  \n  \n  \n```typescript  \nimport path from \"path\";  \nimport { ImageResponse } from \"next/og\";  \n  \nconst assetsDirectory = process.cwd() + \"/assets\";  \n  \nexport const size = {  \n  width: 1200,  \n  height: 630,  \n};  \n  \nexport const contentType = \"image/png\";  \n  \nexport default async function Image({ params }: { params: { slug: string } }) {  \n  const post = getPost(params.slug);  \n  \n  if (!post) return new Response(\"Not Found\", { status: 404 });  \n  \n  const fontInter = await fs.readFile(  \n    path.join(assetsDirectory, \"Inter-Bold.ttf\")  \n  );  \n  const fontNotSansJP = await fs.readFile(  \n    path.join(assetsDirectory, \"NotoSansJP-Bold.ttf\")  \n  );  \n  \n  return new ImageResponse(  \n    (  \n      <div lang=\"ja-JP\">  \n        <div>{post.title}</div>  \n        <div>{SITE_TITLE}</div>  \n      </div>  \n    ),  \n    {  \n      width: 1200,  \n      height: 630,  \n      fonts: [  \n        {  \n          name: \"Inter\",  \n          data: fontInter,  \n          style: \"normal\",  \n          weight: 700,  \n        },  \n        {  \n          name: \"NotoSansJP\",  \n          data: fontNotSansJP,  \n          style: \"normal\",  \n          weight: 700,  \n        },  \n      ],  \n    }  \n  );  \n}  \n  \n```  \n  \n  \n## notionとgithub action定期実行でCMS機能の作成  \n  \n  \n### そもそもなぜnotionなのか？  \n  \n  \nnotionならスマホからでも気軽に書くことができ、記事の更新を継続できそうと思ったから。  \n  \n  \n他のCMSでブログ構築を試みた経験がありますが、ブログのためだけにCMSを開くのが億劫で更新できなくなりました。普段使いしているnotionをデータソースとするとブログの更新が継続できるかもと思い、無理やりnotionを採用しました。  \n  \n  \n### 実現方法  \n  \n  \nnotion apiはすぐにリクエスト量制限に達するので、都度取得できません。定期実行（1日に2回）でmdファイルに変換したものをリポジトリに保存しておき、build時にリポジトリにあるmdファイルを読み込んでブログを構築しています。  \n  \n  \n最終的なマスターデータはリポジトリに保存されるmdファイルとなるので、細かい修正などは直接mdファイルをいじったりもしています。  \n  \n  \ngithub actionsの設定yaml  \n  \n  \n```typescript  \nname: update-contents  \non:  \n  push:  \n    branches: [main]  \n  schedule:  \n    - cron: \"0 4,14 * * *\" # 毎日 4時、14時 に実行  \n# ワークフローに書き込み権限を与える ref:https://docs.github.com/en/actions/using-jobs/assigning-permissions-to-jobs  \npermissions:  \n  contents: write  \njobs:  \n  command:  \n    name: Use Linux commands  \n    runs-on: ubuntu-latest  \n    steps:  \n      - name: Checkout  \n        uses: actions/checkout@v4  \n      - name: Use Node.js  \n        uses: actions/setup-node@v4  \n        with:  \n          node-version: 20  \n          cache: \"npm\"  \n      - name: build application  \n        working-directory: packages/gha-worker  \n        run: npm ci && npm run build  \n      - name: run notion fetch  \n        working-directory: packages/gha-worker  \n        env:  \n          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}  \n          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}  \n          GITHUB_WORKSPACE: $GITHUB_WORKSPACE  \n        run: node dist/notion-fetch/index.js  \n      - name: diff # run notion fetch タスクでファイルの更新が存在するかをgit diffで確認し、commitとpushを行う  \n        id: diff  \n        run: |  \n          git add -N .  \n          git diff --name-only --exit-code  \n        continue-on-error: true  \n      - name: update contents_last_updated.tag  \n        working-directory: packages/gha-worker  \n        run: node dist/update-tag/index.js  \n        if: steps.diff.outcome == 'failure'  \n      - name: commit and push  \n        env:  \n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  \n        run: |  \n          set -x  \n          git config --local user.email \"github-actions[bot]@users.noreply.github.com\"  \n          git config --local user.name \"github-actions[bot]\"  \n          DATETIME=$(date +\"%Y-%m-%d %H:%M:%S\")  \n          git add .  \n          git commit -m \"[$DATETIME] update notion blog content by bot\"  \n          git push origin main  \n        if: steps.diff.outcome == 'failure'  \n  \n```  \n  \n  \nrun notion fetch タスクの処理の概要  \n  \n  \n```typescript  \n  \nimport * as core from '@actions/core'  \nimport { writeFile } from 'fs/promises'  \n  \n  \nexport async function run() {  \n  try {  \n    const notionApiKey = process.env.NOTION_API_KEY ?? 'undefined'  \n    const databaseId = process.env.NOTION_DATABASE_ID ?? 'undefined'  \n    const notionClient = new NotionClient(notionApiKey)  \n  \n    // 記事一覧を取得する  \n    const posts = await notionClient.getUpdatedDatabase(databaseId)  \n  \n    // notion api のリクエスト制限を避けるため、5秒スリープする  \n    await new Promise(resolve => setTimeout(resolve, 5000))  \n  \n    const feedItems: Post[] = posts.map(postFromNotionResponse)  \n  \n    const filterdFeedItems = feedItems.filter(  \n      item => isWithin6Hours(item.lastEditedDate) && item.slug // slugが存在しない場合もfilterする  \n    )  \n  \n    const contentsPath = process.env.GITHUB_WORKSPACE + '/contents'  \n  \n    for (const feedItem of filterdFeedItems) {  \n      // 記事をmarkdown形式で取得する  \n      const mdString = await notionClient.getMdString(feedItem.id)  \n      const mdContents = attachMdMetadata(feedItem, mdString)  \n        \n      // ファイルを書き込む  \n      await writeFile(`${contentsPath}/${feedItem.slug}.md`, mdContents, 'utf8')  \n  \n      core.info(`  \n        Successfully updated below md files  \n        title: ${feedItem.title}  \n        slug: ${feedItem.slug}  \n        id: ${feedItem.id}  \n      `)  \n  \n      // notion api のリクエスト制限を避けるため、5秒スリープする  \n      await new Promise(resolve => setTimeout(resolve, 5000))  \n    }  \n  } catch (err) {  \n    console.log(err)  \n  }  \n}  \n```  \n  \n  \n### notionの画像をmdファイルに変換する際の注意点  \n  \n  \nnotionの画像データのURLは、notionの仕様によるものなのかがすぐにURLが変わってリンク切れしてしまう  \n  \n  \n⇒ [notion-to-md](https://github.com/souvikinator/notion-to-md)でmd変換する際に画像をbase64変換することで対応 ref: [https://github.com/souvikinator/notion-to-md/pull/81/files](https://github.com/souvikinator/notion-to-md/pull/81/files)  \n  \n  \n```typescript  \nimport { Client } from '@notionhq/client'  \nimport { NotionToMarkdown } from 'notion-to-md'  \n  \n  \nconst n2m = new NotionToMarkdown({  \n  notionClient: new Client({  \n      auth: notionToken,  \n      timeoutMs: 1000  \n  }),  \n  config: {  \n    convertImagesToBase64: true  \n  }  \n})  \n```  \n  \n  \n## 最後に  \n  \n  \nわざわざnotion apiを利用せずに、mdファイルを直接いじる運用でも良い気がしている。  \n  \n  \n以下の機能がVS Codeに追加されてからは画像の扱いもかなり楽になったので、そのうちnotion api利用は終わりそう。  \n  \n  \n<Bookmark href=\"https://zenn.dev/roboin/articles/1fa72705ff2e03\" />\n  \n  \nまた、（1円ととても少ないが）収益をブログから得るにあたって、業務で得た知識との棲み分けが難しいなと感じる。業務で得た知識は会社に帰属する認識でブログで利用するつもりはないが、業務きっかけて興味を持ち、プライベートの時間に深掘った技術をどこまでアウトプットすべきなのかが難しいところ。。。  \n  \n  \n「業務と業務時間外の勉強の境目が曖昧」 = 「会社の資産と自分の資産が曖昧」な状態なので同業の方はどのように捉えているのかとても気になる  \n  \n","date":"2024-12-03T00:00:00.000Z"},{"slug":"next-web-push","title":"Next.jsでブラウザ通知を試してみる","tags":["tech","next","PWA"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n# はじめに  \n\n  \nNext.js (app router) にてPush通知を実現してみた記録をまとめます。  \n  \n  \nPush通知を実現するための技術である、サービスワーカーとその応用のPWAについても気になったので少し言及しています。  \n  \n  \n実装の大部分は以下のNext.jsの公式ドキュメントに基づいています。  \n  \n<Bookmark href=\"https://nextjs.org/docs/app/building-your-application/configuring/progressive-web-apps\" />\n  \n# ブラウザのサービスワーカーとは  \n  \n  \n通知を受け取りユーザーに表示するためには、ブラウザのサービスワーカー機能を利用する必要があるとのこと。  \n  \n  \n以下の記事がとても参考になる。  \n  \n<Bookmark href=\"https://zenn.dev/s_takashi/articles/b01980461f3b21\" />\n  \n  \n以下は明示的にサービスワーカースクリプトをブラウザにダウロードさせる例。  \n  \n  \n```javascript  \nconst registration = await navigator.serviceWorker.register('/sw.js', {  \n\tscope: '/',  \n})  \n```  \n\nダウンロードしたサービスワーカースクリプトがバックグラウンドで実行され、通知をキャッチしてさばくという仕組み。  \n  \n一度ダウンロードするとオフラインでも実行可能。オフラインでも動くこのような仕組みをキャッシュなどに利用するのがPWAらしい。  \n  \nサービスワーカーはブラウザの実行環境と異なり、以下のapiが利用できるらしい。ページ読み込み前から動いてくれるスクリプトなので当然だが、DOMにアクセスできなかったり、windowオブジェクトなどは利用できないので注意が必要。  \n  \n  \n<Bookmark href=\"https://developer.mozilla.org/ja/docs/Web/API/Service_Worker_API\" />\n  \n以下のchromeのservice worker設定画面（**chrome://serviceworker-internals/**）から確認、unregisterすることができる  \n  \n![alt text](images/next-web-push/serviceworker-internals.png)\n  \nまた、検証画面からUnregisterすることもできます。画像の「Update on reload」にチェックを入れておくと、リロードの度にサービスワーカーが更新されるようになり、デバッグが捗ったのでおすすめです。  \n  \n![alt text](images/next-web-push/developer-tool.png)\n  \n# Next.jsでブラウザ通知を実装する  \n  \n  \n## 通知を受け取るサービスワーカーを作成する  \n  \n  \npublic/sw.js  \n  \n  \n```javascript  \nself.addEventListener(\"push\", function (event) {  \n  if (event.data) {  \n    const data = event.data.json();  \n    const options = {  \n      body: data.body,  \n      icon: data.icon || \"/favicon.png\",  \n      badge: \"/favicon.png\",  \n      vibrate: [100, 50, 100],  \n      data: {  \n        dateOfArrival: Date.now(),  \n        primaryKey: \"2\",  \n      },  \n    };  \n    event.waitUntil(self.registration.showNotification(data.title, options));  \n  }  \n});  \n```  \n  \n  \npushイベントをevent listenerを作成し、通知を作成する [showNotification](https://developer.mozilla.org/ja/docs/Web/API/ServiceWorkerRegistration/showNotification) メソッドで端末に通知を表示しています。  \n  \n  \n## サービスワーカーを登録し、通知管理のsubscriptionをステートに格納する  \n  \n  \n```javascript  \n  async function registerServiceWorker() {  \n    const registration = await navigator.serviceWorker.register(\"/sw.js\", {  \n      scope: \"/\",  \n      updateViaCache: \"none\",  \n    });  \n    const sub = await registration.pushManager.getSubscription();  \n    setSubscription(sub);  \n  }  \n```  \n  \n  \nregisterメソッドの返り値は以下のServiceWorkerRegistrationを返す。  \n  \n  \n<Bookmark href=\"https://developer.mozilla.org/ja/docs/Web/API/ServiceWorkerRegistration\" />\n  \n  \nこちらのServiceWorkerRegistrationからプッシュ通知の許可状況へのアクセスなど、プッシュ通知の購読を管理するための [`PushManager`](https://developer.mozilla.org/ja/docs/Web/API/PushManager) へアクセスすることができる。こちらの PushManagerから取得できる [`PushSubscription`](https://developer.mozilla.org/ja/docs/Web/API/PushSubscription) インスタンスはプッシュ通知を送る際に利用するのでstateに格納しておく。  \n  \n  \nちなみに、サービスワーカーを登録する際はブラウザの通知権限を求める必要があり、自分は以下のように管理しています。  \n  \n  \n```typescript  \n  useEffect(() => {  \n    requestPermission().then((permission) => {  \n      // 通知の権限がある場合のみ先ほどのregisterServiceWokerを実効する  \n      if (permission) registerServiceWorker();  \n    });  \n  }, []);  \n  \n  async function requestPermission() {  \n    if (  \n      \"serviceWorker\" in navigator &&  \n      \"PushManager\" in window &&  \n      \"Notification\" in window  \n    ) {  \n      // 現在の通知権限の状態をチェック  \n      if (Notification.permission === \"granted\") {  \n        return true;  \n      } else if (Notification.permission === \"default\") {  \n        // 通知権限がまだ許可されていない場合、許可をリクエスト  \n        Notification.requestPermission().then((permission) => {  \n          if (permission === \"granted\") return true;  \n        });  \n      } else {  \n        // Notification.permission === \"denied\" の場合、手動で権限を許可してもらう必要がある  \n        setAlert(  \n          \"通知権限が拒否されています。ブラウザの設定より手動で通知を許可してください\"  \n        );  \n      }  \n    } else {  \n      setAlert(\"このブラウザは通知をサポートしていません。\");  \n    }  \n    return false;  \n  }  \n```  \n  \n  \n通知権限の状態については以下をの公式Docを参考に実装しました。  \n  \n  \n<Bookmark href=\"https://developer.mozilla.org/ja/docs/Web/API/Notification/requestPermission_static\" />\n  \n  \nまた、参考までに ーカルでで動作確認する場合はhttpsである必要があるので、 `next dev --experimental-https` を利用する必要があます。  \n  \n  \n## 通知を送る（server action）  \n  \n  \n最後に、取得したsubscriptionの情報を用いて通知を実装に送信する部分を作成しました。通知送信部分は[server action](https://react.dev/reference/rsc/server-actions)を利用します。  \n  \n  \nweb-push 送信部分の詳細は以下のREADME.meなどを参照ください。 [VAPID](https://vapidkeys.com/) などを事前に登録しておく必要があります。  \n  \n  \n<Bookmark href=\"https://www.npmjs.com/package/web-push\" />\n  \n  \n```javascript  \n\"use server\";  \n  \nimport webpush, { PushSubscription } from \"web-push\";  \n  \nwebpush.setVapidDetails(  \n  \"mailto:example@gmail.com\",  \n  process.env.NEXT_PUBLIC_VAPID_PUBLIC_KEY!,  \n  process.env.VAPID_PRIVATE_KEY!  \n);  \n  \n  \nexport async function sendNotification(message: string) {  \n  if (!subscription) {  \n    throw new Error(\"No subscription available\");  \n  }  \n  \n  try {  \n    await webpush.sendNotification(  \n      subscription,  \n      JSON.stringify({  \n        title: \"Test Notification\",  \n        body: message,  \n        icon: \"/favicon.png\",  \n      })  \n    );  \n    return { success: true };  \n  } catch (error) {  \n    console.error(\"Error sending push notification:\", error);  \n    return { success: false, error: \"Failed to send notification\" };  \n  }  \n}  \n```  \n  \n  \n本サイトの以下labsにて動作している部分を確認できます。  \n  \n  \n[https://sokes-nook.net/blog/next-web-push](https://sokes-nook.net/blog/next-web-push)  \n  \n  \nchromeの場合、送信した通知の履歴等は以下から確認することができる。  \n  \n  \nchrome://gcm-internals/  \n  \n  \n## PWAの現状について  \n  \n  \nPWA(Progressive Web Applications) は、ウェブアプリケーションのリーチとアクセス性を持ちながら、ネイティブモバイルアプリの機能とユーザー体験を組み合わせたものです。  \n  \n- アプリストアの承認を待たずに、すぐに更新をデプロイできる  \n- 単一のコードベースでクロスプラットフォームアプリケーションを作成できる  \n- ホーム画面へのインストールやプッシュ通知など、ネイティブのような機能を提供できる  \n  \nNext.js においては、[manifest](https://nextjs.org/docs/app/api-reference/file-conventions/metadata/manifest)を適切に作成してサービスワーカーを配置するとPWAとして動かすことができます。  \n  \n  \n本サイトも通知権限を付与してかつ、ブラウザのホームに追加いただくとPWAとして動くようになると思います。  \n  \n  \nしかし、実態としてブラウザ通知やサービスワーカは以下で報告されているように詐欺などに利用されるケースが多くあり、ユーザー側が積極的に許可する状況ではありません。肌感覚的には、「何か謎の通知が要求されたが、よくわからないのでとりあえず拒否。」というユーザーが多いと感じており、アプリほどリーチとアクセス性を実現できないと思います。  \n  \n  \n実際にプロダクトに導入してリターンが見込めるかはしっかり検討が必要な段階ということです。  \n  \n  \n<Bookmark href=\"https://www.ipa.go.jp/security/anshin/attention/2021/mgdayori20210309.html\" />\n  \n  \nしかし、技術としてはとても素敵だなと思いますし、日々進化していると実感しています。最後にPWAを導入する際に参考になりそうなサイトを置いておきます。  \n  \n- 現在PWAができることは以下サイトにまとめられている  \n  \n  <Bookmark href=\"https://whatpwacando.today/\" />\n  \n- Next.jsの公式ドキュメントにて言及されている [serwist](https://github.com/serwist/serwist) を利用したPWAを作成しているブログ  \n  \n  <Bookmark href=\"https://javascript.plainenglish.io/building-a-progressive-web-app-pwa-in-next-js-with-serwist-next-pwa-successor-94e05cb418d7\" />\n  \n- serwistのフォーク元であるworkboxを利用している以下の一休.comの事例  \n  \n  <Bookmark href=\"https://user-first.ikyu.co.jp/entry/2019/12/02/080000\" />\n  \n","date":"2024-09-25T00:00:00.000Z"},{"slug":"nextauth-yahoojapanid","title":"NextAuthでYahoo!JapanIDでログインするためのカスタムプロバイダーを作成する","tags":["tech","auth","OIDC"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n## はじめに  \n  \n  \nNext.jsを用いたアプリケーションにログイン機能を実装する際、[NextAuth](https://next-auth.js.org/)を利用することが多いと思います。メールアドレスでログインだったり、Googleでログインする方法については、たくさんの実装例や記事があると思います。参考になりそうな実装例を以下に置いておきます。  \n  \n- 公式の実装例: [https://github.com/nextauthjs/next-auth-example](https://github.com/nextauthjs/next-auth-example)  \n- 記事: [https://zenn.dev/tfutada/articles/5557b780050574](https://zenn.dev/tfutada/articles/5557b780050574)  \n  \n本記事では Yahoo!JAPAN IDを用いたログインをNextAuthで実装する方法を紹介します。  \n  \n  \n## Yahoo! JAPAN IDでログインを実装するための前準備  \n  \n  \n前提としてYahoo! JAPAN IDでログインを提供するためのID 連携の仕組みなどは以下で説明されているので事前に確認しておくと実装がわかりやすくなると思います。  \n  \n  \n<Bookmark href=\"https://developer.yahoo.co.jp/yconnect/v2/introduction.html\" />\n  \n  \n利用用途に応じてこちらの[ガイドライン](https://developer.yahoo.co.jp/yconnect/v2/guideline.html)も参照しておくとさらに安心かと思います。[デザインガイドライン](https://developer.yahoo.co.jp/yconnect/loginbuttons.html)も用意されています。  \n  \n  \nまた、以下のYahoo!デベロッパーネットワークから、先にアプリケーションを作成し、OAuth2.0 および OpenID Connect用のClinet IDを発行しておく必要があります。  \n  \n  \n<Bookmark href=\"https://developer.yahoo.co.jp/yconnect/v2/\" />\n  \n  \nこちらで発行したClinet ID, シークレットは実装で利用するのでメモっておいてください。  \n  \n  \nredirect_uriに指定する値を事前に登録する必要があるので、以下値をコールバックURLに登録しておく必要があります。  \n  \n  \n`https://サイトのドメイン/api/auth/callback/yahoo`  \n  \n  \n注意点として、Yahoo!JAPAN IDをログインに利用し、ユーザーの名前やメールアドレスを取得したい場合は以下で案内されている申し込みフォームから審査が必要となります。Googleの場合は審査なしで表示名やメールアドレスが取得できるようになりますが、Yahoo!JAPANは個人情報の扱いを丁寧に行なっていることが伺えますね。  \n  \n  \n[https://developer.yahoo.co.jp/yconnect/v2/userinfo.html](https://developer.yahoo.co.jp/yconnect/v2/userinfo.html)  \n  \n  \n## 実装  \n  \n  \n基本的には先ほど紹介した[こちらの記事](https://zenn.dev/tfutada/articles/5557b780050574)のGoogleでログインの部分の実装と同じです。  \n  \n  \nYahoo!JAPAN IDでログインするためのカスタムプロバイダーを作成し、適用する部分だけを切り出してみました。  \n  \n  \nyahoo-japan-id-provider.ts  \n  \n  \n```typescript  \nimport { OAuthConfig } from \"next-auth/providers/oauth\";  \n  \nexport const YahooJapanIdProvider: OAuthConfig<any> = {  \n  id: \"yahoo\",  \n  name: \"yahoo\",  \n  issuer: \"https://auth.login.yahoo.co.jp/yconnect/v2\",  \n  wellKnown:  \n    \"https://auth.login.yahoo.co.jp/yconnect/v2/.well-known/openid-configuration\",  \n  version: \"2.0\",  \n  type: \"oauth\",  \n  authorization: {  \n    url: \"https://auth.login.yahoo.co.jp/yconnect/v2/authorization\",  \n    params: { scope: \"openid\" }, // 属性情報取得の審査が通った場合にスコープを追加する  \n  },  \n  token: \"https://auth.login.yahoo.co.jp/yconnect/v2/token\",  \n  userinfo: \"https://userinfo.yahooapis.jp/yconnect/v2/attribute\",  \n  idToken: true,  \n  clientId: process.env.YAHOO_CLIENT_ID,  \n  clientSecret: process.env.YAHOO_CLIENT_SECRET,  \n  checks: [\"state\"],  \n  profile: (profile: any) => {  \n    /*  \n    yahooのid tokenでは以下のようなレスポンスが返る ※ userの情報はスコープが指定できないため入っていない  \n    profile {  \n        aud: string[];  \n        exp: number;  \n        iss: 'https://auth.login.yahoo.co.jp/yconnect/v2';  \n        iat: number;  \n        sub: string;  \n        amr: string[];  \n        at_hash: string;  \n        }  \n    ref: https://developer.yahoo.co.jp/yconnect/v2/id_token.html  \n  \n    ユーザー名、メールアドレス等を取得したい場合（userInfoを叩きたい場合）は審査を経た上でスコープを指定できるようになる。  \n    ref: https://developer.yahoo.co.jp/yconnect/v2/userinfo.html  \n    */  \n    return {  \n      id: profile.sub,  \n    };  \n  },  \n};  \n```  \n  \n  \n先ほど取得したclinet id, シークレットは以下の部分で利用しており、環境変数に指定しておく必要があります。  \n  \n  \n> process.env.YAHOO_CLIENT_ID,    \n> process.env.YAHOO_CLIENT_SECRET,  \n  \n  \nnext-auth-options.ts  \n  \n  \n```typescript  \n  \nimport type { NextAuthOptions } from \"next-auth\";  \nimport { YahooJapanIdProvider } from \"./yahoo-japan-id-provider\";  \n  \nexport const nextAuthOptions: NextAuthOptions = {  \n  debug: process.env.NODE_ENV !== \"production\",  \n  session: { strategy: \"jwt\" },  \n  providers: [  \n    YahooJapanIdProvider,  \n  ],  \n  callbacks: {  \n    jwt: async ({ token, user, account, profile }) => {  \n      return token;  \n    },  \n    session: ({ session, token }) => {  \n      return {  \n        ...session,  \n        user: {  \n          ...session.user, // 属性情報取得が可能な場合はこちらにユーザー情報が入るか？？  \n          id: token.sub,  \n        },  \n      };  \n    },  \n  },  \n};  \n  \n```  \n  \n  \nログインボタンコンポーネント  \n  \n  \n```typescript  \n\"use client\";  \n  \nimport { signIn } from \"next-auth/react\";  \n  \nconst LoginButton = () => {  \n  const handleLogin = (provider: string) => async (event: React.MouseEvent) => {  \n    event.preventDefault();  \n    const result = await signIn(provider);  \n  };  \n  \n  return (  \n      <form className=\"w-full mt-6 max-w-xs space-y-6 rounded bg-white p-8 shadow-md\">  \n        <button  \n          onClick={handleLogin(\"yahoo\")}  \n          type=\"button\"  \n          className=\"w-full bg-red-500 text-white rounded-lg px-4 py-2\"  \n        >  \n          Yahoo!JapanIDでログイン  \n        </button>  \n      </form>  \n     )  \n```  \n  \n  \napp/api/auth/[…nextauth]/route.ts  \n  \n  \n```typescript  \nimport NextAuth from \"next-auth\";  \n  \nimport { nextAuthOptions } from \"../../next-auth-options\";  \n  \nconst handler = NextAuth(nextAuthOptions);  \n  \nexport { handler as GET, handler as POST };  \n```  \n  \n","date":"2024-09-29T00:00:00.000Z"},{"slug":"npm-workspace-shared","title":"npm workspaceを用いて共通処理をpackageとして切り出す","tags":["tech","next","npm"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n以下のような構成でモノレポではあるものの、それぞれ独立したnodeプロジェクトで運用していた状態から、npm workspaceを用いて共通処理置き場(sharedプロジェクト)を作成したのでその方法を紹介します。  \n  \n  \n```text\n.  \n├── backend  \n│   ├── package.json  \n│   └── package-lock.json  \n├── frontend  \n│   ├── next.config.ts  \n│   ├── package.json  \n│   └── package-lock.json  \n└── supabase  \n    ├── migrations  \n    └── schema.sql  \n```  \n  \n  \n**workspace移行後のプロジェクト構成**   \n  \n  \n```text\n.  \n├── backend  \n│   └── package.json  \n├── frontend  \n│   ├── next.config.ts  \n│   └── package.json  \n├── node_modules  \n├── package-lock.json  \n├── package.json  \n├── shared  \n│   └── package.json  \n└── supabase  \n    ├── migrations  \n    └── schema.sql  \n```  \n  \n  \nそもそもnpm workspaceとはという方は以下の記事がすごくわかりやすかったのでおすすめです。  \n  \n  \n<Bookmark href=\"https://zenn.dev/suin/scraps/20896e54419069\" />\n  \n  \n## やったこと   \n  \n  \n### プロジェクトルートにworkspace用のpackage.json作成   \n  \n  \n```json  \n{  \n  \"name\": \"your-app\",  \n  \"private\": true,  \n  \"workspaces\": [  \n    \"frontend\",  \n    \"backend\",  \n    \"shared\"  \n  ],  \n  \"scripts\": {  \n    \"build\": \"npm run build --workspaces\"  \n  },  \n  \"devDependencies\": {  \n    \"@types/node\": \"^22.13.1\",  \n    \"typescript\": \"^5.7.3\"  \n  }  \n}  \n```  \n  \n  \n全てのパッケージで利用するようなdependenciesはルートに指定しておくことができます。  \n  \n  \nnodeのモジュール解決ではホイスティング（hoisting, 巻き上げ）という機能があり、自分のnode_modulesに該当モジュールがない場合は親のnode_modulesに該当モジュールを探しに行くという機能による恩恵です。  \n  \n  \n### sharedプロジェクト作成  \n  \n  \nプロジェクト構成  \n  \n  \n```json  \n.  \n├── dist  \n├── package.json  \n├── src  \n│   ├── date.ts  \n│   └── index.ts  \n└── tsconfig.json  \n```  \n  \n  \nshared/package.json  \n  \n  \n```json  \n{  \n    \"name\": \"@your-app/shared\",  \n    \"private\": true,  \n    \"version\": \"1.0.0\",  \n    \"main\": \"dist/index.js\",  \n    \"types\": \"dist/index.d.ts\",  \n    \"scripts\": {  \n      \"build\": \"tsc\",  \n      \"watch\": \"tsc -w\"  \n    }  \n  }  \n```  \n  \n  \nshared/tsconfig.json  \n  \n  \n```json  \n{  \n  \"compilerOptions\": {  \n    \"target\": \"es2018\",  \n    \"module\": \"commonjs\",  \n    \"declaration\": true,  \n    \"outDir\": \"./dist\",  \n    \"strict\": true,  \n    \"esModuleInterop\": true,  \n    \"skipLibCheck\": true,  \n    \"forceConsistentCasingInFileNames\": true  \n  },  \n  \"include\": [\"src\"],  \n  \"exclude\": [\"node_modules\", \"dist\", \"**/*.test.ts\"]  \n}  \n  \n```  \n  \n  \nshared/index.ts  \n  \n  \n```json  \nexport * from \"./date\";  \n```  \n  \n  \nshared/date.ts  \n  \n  \n```typescript  \nexport const getDateStringBeforeN = (n: number) => {  \n  const today = new Date();  \n  const targetDate = new Date(today);  \n  targetDate.setDate(today.getDate() - n);  \n  \n  const year = targetDate.getFullYear();  \n  const month = String(targetDate.getMonth() + 1).padStart(2, \"0\");  \n  const day = String(targetDate.getDate()).padStart(2, \"0\");  \n  return `${year}-${month}-${day}`;  \n};  \n```  \n  \n  \nプロジェクトをbuildしておく  \n  \n  \n```shell  \nnpm run build -w shared  \n```  \n  \n  \n### frontend側の修正  \n  \n  \nsharedをfrontend側の依存に追加する  \n  \n  \n```shell  \nnpm i @your-app/shared -w @you-app/frontend  \n```  \n  \n  \nsharedの機能を利用する  \n  \n  \n```typescript  \nimport { getDateStringBeforeN } from \"@your-app/shared\";  \n```  \n  \n  \n**注意点**  \n  \n  \n元々存在していたnode_moduleとpackage-lock.jsonを手動で削除してからルートにて `npm install` を実行する必要があります。これを実施しないと以下のようなエラーで共通処理を正しくimportできないと思います。  \n  \n  \n```text\nModule not found: Can't resolve '@your-app/shared'  \n```  \n  \n  \n最後にbuildコマンドでsharedプロジェクトを一緒にbuildされるように修正します。  \n  \n  \n```typescript  \n  \"scripts\": {  \n    \"build\": \"npm --workspace=@you-app/shared run build && next build\"  \n  },  \n```  \n  \n  \n## 最後に  \n  \n  \n全てのメソッドがshared/index.tsにまとまって一括でimportする構成はなんか微妙な気がする場合はexportを利用する手もあると思います。  \n  \n  \n<Bookmark href=\"https://zenn.dev/makotot/articles/5edb504ef7d2e6\" />\n  \n","date":"2025-02-08T00:00:00.000Z"},{"slug":"pageviews-by-gadataapi","title":"Google Analytics Data APIを利用してPV数と人気記事を取得する","tags":["tech","GCP"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n## はじめに  \n  \n  \n個人ブログの記事にPV数をつける際の最にGoogle Analytics Data APIを利用したため、その方法を備忘録的にまとめました。  \n  \n  \nGoogle Analyticsがすでにサイトに導入されていて、ある程度のデータがGoogleAnalytics上にあることが前提の記事になりますのでご注意ください。  \n  \n  \n## 事前準備: APIの有効化  \n  \n  \nGoogle Analytics Reporting API を有効化しておく必要があります。  \n  \n  \nGCPのコンソールからプロジェクトを選択し、以下APIを有効化してください。  \n  \n  \n[https://console.cloud.google.com/apis/api/analyticsdata.googleapis.com/metrics](https://console.cloud.google.com/apis/api/analyticsdata.googleapis.com/metrics)   \n  \n  \n参考までに公式のquick startのリンクも貼っておきます  \n[https://developers.google.com/analytics/devguides/reporting/data/v1/quickstart-client-libraries?hl=ja](https://developers.google.com/analytics/devguides/reporting/data/v1/quickstart-client-libraries?hl=ja)  \n  \n  \n## Google Analytics Data APIを叩く  \n  \n  \n`pagePath`のdimensionsで、`screenPageViews`のmetricsを指定すると、ページごとのview数を取得できる。  \n  \n  \nslugに対するPV数をもつ辞書型を返すように実装しています。  \n  \n  \n```typescript  \nimport { BetaAnalyticsDataClient } from '@google-analytics/data'  \n  \nexport class GaApiClient {  \n  private propertyId: string | undefined  \n  \n  private analyticsDataClient: BetaAnalyticsDataClient  \n  \n  constructor() {  \n    this.propertyId = process.env.GA_PROPERTY_ID  \n    this.analyticsDataClient = new BetaAnalyticsDataClient({  \n      credentials: {  \n        type: 'service_account',  \n        project_id: process.env.GCP_SA_PROJECT_ID,  \n        private_key_id: process.env.GCP_SA_PRIVATE_KEY_ID,  \n        private_key: process.env.GCP_SA_PRIVATE_KEY!!.replace(/\\\\n/g, '\\n'), // 環境変数がエスケープされてしまうため、元に戻す  \n        client_email: process.env.GCP_SA_CLIENT_EMAIL,  \n        client_id: process.env.GCP_SA_CLIENT_ID,  \n        universe_domain: 'googleapis.com'  \n      }  \n    })  \n  }  \n  \n  async getPv(): Promise<{ [slug: string]: string }> {  \n    const [response] = await this.analyticsDataClient.runReport({  \n      property: `properties/${this.propertyId}`,  \n      dateRanges: [  \n        {  \n          startDate: '2024-08-25', // ブログ公開日など  \n          endDate: 'today'  \n        }  \n      ],  \n      dimensions: [{ name: 'pagePath' }],  \n      dimensionFilter: {  \n        filter: {  \n          fieldName: 'pagePath',  \n          stringFilter: {  \n            matchType: 'FULL_REGEXP',  \n            value: '/blog/([^/]+)' /* ブログページにのみに絞る */  \n          }  \n        }  \n      },  \n      metrics: [{ name: 'screenPageViews' }]  \n    })  \n  \n    const result: { [slug: string]: string } = {}  \n    response.rows?.forEach(row => {  \n      if (row && row.dimensionValues && row.metricValues) {  \n        const path = row.dimensionValues[0].value  \n        const segments =  \n          path?.split('/').filter(segment => segment !== '') ?? [] // パスをスラッシュで分割  \n  \n        // \"/blog/{slug}\" のパスを適切に分割できているかチェック  \n        if (segments.length === 2 && segments[0] === 'blog') {  \n          const slug = segments[1]  \n          const views = row.metricValues[0].value!  \n          result[slug] = views  \n        }  \n      }  \n    })  \n    return result  \n  }  \n  \n  async getPopularPostData() {  \n    const [response] = await this.analyticsDataClient.runReport({  \n      property: `properties/${this.propertyId}`,  \n      dateRanges: [  \n        {  \n          startDate: '30daysAgo', // 過去30日の人気記事を取得  \n          endDate: 'today'  \n        }  \n      ],  \n      dimensions: [  \n        {  \n          name: 'pagePath'  \n        }  \n      ],  \n      dimensionFilter: {  \n        filter: {  \n          fieldName: 'pagePath',  \n          stringFilter: {  \n            matchType: 'FULL_REGEXP',  \n            value: '/blog/([^/]+)' /* ブログページにのみに絞る */  \n          }  \n        }  \n      },  \n      metrics: [  \n        {  \n          name: 'screenPageViews'  \n        }  \n      ],  \n      orderBys: [  \n        {  \n          desc: true,  \n          metric: {  \n            metricName: 'screenPageViews'  \n          }  \n        }  \n      ],  \n      limit: 10 // 10記事までのパスを取得  \n    })  \n    console.log('popular Report result:')  \n    response.rows?.forEach(row => {  \n      console.log(  \n        row.dimensionValues && row.dimensionValues[0],  \n        row.metricValues && row.metricValues[0]  \n      )  \n    })  \n  }  \n}  \n```  \n  \n  \nその他、dimensions, metrics のパラメータを調整するとそれぞれのユースケースに合わせたデータが取得できるようになると思います。パラメータを調整する際に参考になりそうなドキュメント群を置いておきます  \n  \n- 公式Doc: [https://developers.google.com/analytics/devguides/reporting/data/v1?hl=ja](https://developers.google.com/analytics/devguides/reporting/data/v1?hl=ja)  \n- node の clinet ライブラリのドキュメント: [https://googleapis.dev/nodejs/analytics-data/latest/index.html](https://googleapis.dev/nodejs/analytics-data/latest/index.html)  \n- クエリで使用する、dimensions, metricsで利用できる値: [https://developers.google.com/analytics/devguides/reporting/data/v1/api-schema?hl=ja#dimensions](https://developers.google.com/analytics/devguides/reporting/data/v1/api-schema?hl=ja#dimensions)  \n  \n## おわりに  \n  \n  \n※ Google Analytics Data APIは、料金はかからないようですが、以下ページにあるQuotasを上回るとリクエストに失敗するという制限があるようです。ご注意ください。  \n[https://developers.google.com/analytics/devguides/reporting/data/v1/quotas?hl=ja](https://developers.google.com/analytics/devguides/reporting/data/v1/quotas?hl=ja)  \n  \n  \n以下、公式doc以外で実装の際に参考にさせていただいた記事です🙏  \n  \n- [**Node.js (TypeScript) で Google Analytics Reporting API v4 を使用する方法**](https://fwywd.com/tech/ga-popular-node-ts)  \n- [**Google Analytics Reporting APIでの発生したリクエスト数を確認する**](https://ponsuke-tarou.hatenablog.com/entry/2021/03/25/150451)  \n- [**Gatsbyで作ったサイトに人気記事のランキングを実装する【前編】**](https://komari.co.jp/column/14935/)  \n  \n<Bookmark href=\"https://fwywd.com/tech/ga-popular-node-ts\" />\n  \n  \n<Bookmark href=\"https://ponsuke-tarou.hatenablog.com/entry/2021/03/25/150451\" />\n  \n  \n<Bookmark href=\"https://komari.co.jp/column/14935/\" />\n  \n","date":"2024-09-11T15:00:00.000Z"},{"slug":"remix-notion-blog-give-up","title":"remix と notion apiでブログを構築しようとしたが断念した話","tags":["tech","remix","notion"],"content":"\n  \n## 構築しようとしたブログの要件  \n  \n- remixを使いたい  \n\t- エンジニアあるあるですが、目的と手段が一致するパターンです。remixを利用したいが作るものがなかったので個人ブログを作ることにした感じです。  \n- データソースはnotion  \n\t- 以前contentfulで個人ブログを始めましたが、contentfulを開くことが面倒になってしまい続かなかったという経験があるため、CMSは利用せずにnotion apiを利用したいです。  \n- 無料で高パフォーマンス  \n  \nこちらの3軸でブログ作成を真面目に検討し、作成しましたが断念したのでその記録をまとめます。  \n  \n  \n## 環境  \n  \n- remix-run: 2.10  \n- react: 19rc  \n- デプロイ先: cloudflare pages  \n  \n## やったこと  \n  \n- remixプロジェクト作成  \n- notion api 取得処理  \n- 取得した記事一覧、記事詳細ページの追加  \n- EDGE環境でのキャッシュ処理実装  \n- Google Analytics設定、記事ページのビュー数取得  \n  \n## やってみた結果思ったこと  \n  \n  \n検討、制作を進めるにつれてこれまでに見えていなかったremixの特徴が発覚し、notion apiとの相性の悪さから断念しました。  \n  \n- remixはEdge環境で動作することが前提のため、node.js依存のライブラリを利用できない。  \ncloudflareを利用している場合、 [Home - Cloudflare Workers®](https://workers.cloudflare.com/) がEdge環境になります  \n\t- [@google-analytics/data](https://www.npmjs.com/package/@google-analytics/data) を用いてGAのデータを取得しようとしたが、node.js依存のためapi をコールする部分を自分で実装する羽目に…  \n\t- node.jsのfsが使えないため、mdファイルをfsを用いて read して… といったことができない。そもそもEdge環境にあげられるbuild後のアプリケーションにmdファイルを含めることが難しい。  \n- Edge環境で毎度SSRすることを売りにしており、アンチSSGという姿勢のライブラリのため、SSRするごとに notion api にアクセスする必要がある。  \nnotion api は 3rps以上リクエストするとエラーとなるため、SSR環境で都度取得は絶望的  \n[https://developers.notion.com/reference/request-limits](https://developers.notion.com/reference/request-limits)  \n\t- そこであげられる方法として、あらかじめnotionの内容をrepositoryに自動で保存しておく方法ですが、こちらは node.jsが使えないため、不可能でした。  \n\t- 唯一残っている方法が、notionの内容を react コンポーネントを含まないmdxとして保存する自動化プログラムを作成し、route/ 配下 に自動でmdxファイルを増やし続ける方法のみでした。mdxのみ、remixがbuild時に梱包することをbuilt-inサポートしており、remixの奨励方法であると言えそうです。  \n\t[https://remix.run/docs/en/main/guides/mdx](https://remix.run/docs/en/main/guides/mdx)  \n\t- しかし、mdxで作る方法を採用する場合、ブログ一覧ページを取得する方法が難しいそう。  \n\t`window.__remixManifest.routes` を見ればクライアントサイドから全てのrouteを見ることができるが、ssr時点で動的に取得する方法は自分の観測範囲内ではなさそう。別途`blogList.server.ts`等、ファイルを作って記事一覧を管理しているブログ多そうだが、これはやりたくない。  \n  \n## おわりに  \n  \n  \n今回の要件の場合、諦めてNext.jsでの構築に切り替えようと思います。  \nnotion api を用いて無料でブログを構築する場合、以下の流れが良さそうなので試してみます。  \n  \n- github action で 定期的にnotion を mdとしてrepositoryに保存  \n- Next.jsのSSGで[contentlayer](https://contentlayer.dev/)を用いて記事一覧を操作  \n  \nあくまで自分がremixの特徴を知らずにとりあえず使ってみたいと先走った結果であり、remixは素晴らしい技術だと思っています。ただ、自分にそれを扱う準備と技術力がなかった…  \n  \n  \n[Progressive Enhancement](https://remix.run/docs/en/main/discussion/progressive-enhancement)等、Next.jsのApp routerに多いに影響を与えており、考え方や哲学は今で本当に素敵だなと感じています。  \n  \n","date":"2024-08-31T00:00:00.000Z"},{"slug":"slides-browsing","title":"スライド徘徊のすすめ","tags":["poem"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n個人的に新しい技術をキャッチアップする際、まず最初に既存の勉強会などのスライドをスライド共有サービスから探して全体を把握することが多いです。こちらの方法が意外とお勧めなので、スライド徘徊を勧めるだけの記事を書いてみようと思います。  \n  \n  \n## スライド徘徊のメリット  \n  \n- 概念ベースでわかりやすくまとまっており、すぐに全体像を掴めるスライドが存在する  \n- スライドから勉強会のyoutubeに飛べることができることも。勉強会の存在を知るきっかけにもスなる  \n- すでによく使う技術であっても、スライド徘徊していると新しい使い方に出会って自プロダクトの改善のきっかけになる。（セレンディピティ的な？）  \n  \n## スライド共有サービス3選  \n  \n  \n### slideshare  \n  \n  \n<Bookmark href=\"https://www.slideshare.net/\" />\n  \n  \nJavaで検索した際の例:  \n[https://www.slideshare.net/search?searchfrom=header&q=java&language=ja&sort=RECENT](https://www.slideshare.net/search?searchfrom=header&q=java&language=ja&sort=RECENT)  \n  \n  \n全分野のスライドがあるので、技術のキャッチアップのスライドを探そうとするとノイズが多め  \n  \n  \n### **Speaker Deck**  \n  \n  \n<Bookmark href=\"https://speakerdeck.com/\" />\n  \n  \nJavaで検索した際の例:  \n[https://speakerdeck.com/search?page=1&q=java&lang=ja](https://speakerdeck.com/search?page=1&q=java&lang=ja)  \n  \n  \n洗練された内容のスライドが多いイメージで、最も徘徊していて楽しい。  \n  \n  \n### **Docswell**  \n  \n  \n<Bookmark href=\"https://www.docswell.com/\" />\n  \n  \n日本のエンジニア向けに開発されており、個人的にも最もUXが良いと感じている。カジュアルな内容から技術仕様の詳細解説まで存在する。  \n  \n  \nJavaでタグ検索した際の例:  \n[https://www.docswell.com/tag/Java](https://www.docswell.com/tag/Java)  \n  \n  \nユーザーが独自でつけたタグでソートできるのが良い感じ。  \n  \n  \n## java学習で読んでよかったスライドたち  \n  \n  \nぎり文法がわかるレベルのjava理解度だった自分が、java21の特徴をふんだんに利用したプロジェクトにジョインする際にまず参考にさせてもらったスライドたちを列挙します。  \n  \n- [**Java 21の概要**](https://speakerdeck.com/kishida/outline-of-java-21)  \n- [**Java21とKotlinの代数的データ型 & パターンマッチの紹介と本当に嬉しい使い方**](https://speakerdeck.com/yuitosato/algebraic-data-type-in-java-and-kotlin-happy-use-of-pattern-match)  \n- [**Javaの現状2024夏**](https://speakerdeck.com/kishida/java-current-status-2024-summer)  \n- [**Springのこれまでとこれから #javasumi23**](https://www.docswell.com/s/MasatoshiTada/ZVV7NV-spring-current-and-future)  \n\t- ※ こちらのスライド内でも紹介されている以下動画は、ネイティブイメージなど Spring Boot 3 の機能をキャッチアップしたい人にとって感動的にわかりやすかったです。  \n\t[**B 1500 槙俊明 5年ぶりのメジャーアップデート! Spring Framework 6 Spring Boot 3**](https://www.youtube.com/watch?v=tnq4NBrlhHY)  \n- [**今こそ知りたいSpring DI x AOP #jsug**](https://www.docswell.com/s/MasatoshiTada/Z818E5-spring-di-aop-for-every-developers)  \n  \n最後にTwitterに勉強会スライドをbot化してくれるアカウントも紹介しておきます。  \n  \n  \n[https://x.com/tech_slideshare](https://x.com/tech_slideshare)  \n  \n","date":"2024-11-04T00:00:00.000Z"},{"slug":"standalone-apache-solr-docker","title":"standaloneモードのApache SolrをDockerで動かしてみる","tags":["tech","solr"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n# standaloneモードの**Apache SolrをDockerで動かしてみる**  \n  \n  \nApache Solrは全文検索ライブラリ Apache Licene(ルシーン)を用いた検索エンジンサーバーです。  \n  \n  \n以下を参考にdockerを用いたstandaloneモードを動かしてみたのでその記録を紹介します。  \n  \n  \n<Bookmark href=\"https://solr.apache.org/guide/solr/latest/deployment-guide/solr-in-docker.html\" />\n  \n  \n利用したdocker-compose.yaml  \n  \n  \n```yaml  \nversion: '3'  \nservices:  \n  solr:  \n    image: solr  \n    ports:  \n     - \"8983:8983\"  \n    volumes:  \n      - data:/var/solr  \n    command:  \n      - solr-precreate  \n      - gettingstarted  \nvolumes:  \n  data:  \n```  \n  \n  \n起動コマンド  \n  \n  \n```shell  \ndocker-compose up --build  \n```  \n  \n  \n起動すると以下からsolrの管理画面にアクセスできるようになります。  \n  \n  \n[http://localhost:8983/solr/#/](http://localhost:8983/solr/#/) \n  \n  \nコンテナに入ってデータをインデックス（[参考書籍](https://gihyo.jp/book/2017/978-4-7741-8930-7)にあるtechproductsをインデックスする方法はなさそう）  \n  \n  \n```shell  \ndocker exec -it solr-solr-1(それぞれコンテナ名) bash  \n# tutorial用に用意されているデータの確認  \nsolr@09e993203df9:/opt/solr-9.7.0$ ls example/exampledocs/  \nbooks.csv            ipod_video.xml     monitor.xml       sd500.xml         vidcard.xml  \nbooks.json           manufacturers.xml  more_books.jsonl  solr-word.pdf  \ngb18030-example.xml  mem.xml            mp500.xml         solr.xml  \nhd.xml               money.xml          post.jar          test_utf8.sh  \nipod_other.xml       monitor2.xml       sample.html       utf8-example.xml  \n  \n# books用のcoreの作成  \nsolr@09e993203df9:/opt/solr-9.7.0$ bin/solr create -c books  \n# books.jsonのインデックス  \nsolr@09e993203df9:/opt/solr-9.7.0$ bin/solr post -c books example/exampledocs/books.json  \n```  \n  \n  \nまた、 Solritasの画面は V9では削除されているようです。  \n  \n  \n> The VelocityResponseWriter and associated /browse UI is deprecated and will be removed in 9.0.    \n> The functionality has been replaced by a 3rd party plugin available at [https://github.com/erikhatcher/solritas](https://github.com/erikhatcher/solritas).  \n  \n  \n[https://solr.apache.org/guide/8_9/velocity-search-ui.html](https://solr.apache.org/guide/8_9/velocity-search-ui.html)  \n  \n  \nサイドバーのcore selectorからbooksを選択し、queryを実行すると以下のように検索が成功することを確認。  \n  \n![alt text](images/standalone-apache-solr-docker/solr-dashboard.png)  \n  \n# Solrを扱う際に役立ちそうなメモ  \n  \n  \n## サーチコンポーネントとサーチハンドラー  \n  \n  \nSolrが提供する検索機能は、「サーチコンポーネント」と「サーチハンドラー」によって実現されている。  \n  \n  \nクライアントからの検索リクエストはサーチハンドラが処理する。**サーチハンドラーには複数のサーチコンポーネントが登録されており、これらのサーチコンポーネントが互いに独立してリクエストを実行し**、最終的に処理結果を出力する。（サーチコンポーネントの種類によっては依存関係があり、完全に独立していない場合もある。）  \n  \n  \n**Solrが提供するサーチコンポーネントの抜粋**  \n  \n  \n| サーチコンポーネント名 | クラス名                    | 概要                         |  \n| ----------- | ----------------------- | -------------------------- |  \n| query       | solr.QueryComponent     | 検索結果を返す                    |  \n| facet       | solr.FacetComponent     | ファセット情報を返す                 |  \n| highlight   | solr.HighlightComponent | ハイライトスニペットを返す              |  \n| suggest     | solr.SuggestComponent   | 検索キーワードのサジェスチョン（入力補完）を実現する |  \n| …           |                         |                            |  \n  \n  \nサーチコンポーネントをサーチハンドラーに登録するには、solrconfig.xmlのリクエストハンドラ定義内にあるfirst-components または last-components にコンポーネント名を列挙する  \n  \n  \n```xml  \n<requestHandler name=\"/サーチハンドラ名\" class=\"solr.SearchHandler\">  \n  <arr name=\"first-components\">  \n    <str>サーチコンポーネント名</str>  \n    <str>サーチコンポーネント名</str>  \n  </arr>  \n  <arr name=\"last-components\">  \n    <str>サーチコンポーネント名</str>  \n    <str>サーチコンポーネント名</str>  \n  </arr>  \n</requestHandler>  \n```  \n  \n  \nfisrt-componentsに登録されたサーチコンポーネントはデフォルトで登録されているサーチコンポーネントの前に登録され、last-componentsは後に登録される。  \n  \n  \n※ query, facet, highlight, 等、デフォルトで登録されているサーチコンポーネントは、明示的にsolrconfig.xmlにて登録する必要がない。  \n  \n  \n## SolrCloudによるクラスタ運用  \n  \n  \nSolrでは、上で紹介したstandalneモードではなく、SolrCloudモードで大規模な検索エンジン構築を行うことができる。  \n  \n  \nSolrCloudはZooKeeperを利用して、ノードのステータス管理、設定ファイルの中央集中管理、分散インデクシング、レプリケーション、自動フェールオーバー、リーダーノード（マスターノード）の自動選出など、「単一障害点（Single Point Of Failure : SPOF）」を極力なくすための仕組みが取り込まれている。  \n  \n  \n### 分散インデックス  \n  \n  \nSolrでは大量のドキュメントを複数のノードで分けて扱うことが可能。1ノードあたりのストレージを小さく抑えつつ、理論的には巨大なインデックスを構築できる。その複数のノードに分散配置されたインデックスの一つをシャード（Shard）と言う。  \n  \n  \n複数ノードでインデクシングを並列処理することにより、単位時間あたりのインデクシングのスループットを向上させることができる。（インデクシングのスループットは Documents Per Second :  DPSという。）  \n  \n  \n### 分散検索  \n  \n  \n分散インデックスを用いたSolrでは検索リクエストの際にすべてのシャードに同じ検索クエリを発行し、すべてのシャードの検索結果をマージしてクライアントへ検索結果として返却する分散検索が可能。  \n  \n  \n複数のノードで小さなインデックスを並列に検索するため、大規模の大きなインデックス一つを検索する場合に比べて処理時間が短くなる。（単一インデックスからの検索に対して、分散検索ではマージ処理は別途必要になるが、大きなインデックスに対する検索コストに比べるとマージ処理は軽い処理）  \n  \n  \n### レプリケーション  \n  \n  \nSolrでは大量の検索トラフィックや、ハードウェア障害などによりノードがダウンした場合に備えて用意するインデックスのレプリカを作成することができる。  \n  \n  \nレプリケーションは負荷分散とクラスタ全体の検索スループット向上させることができます。（検索スループットはQueries Per Second : QPS という。）  \n  \n  \n## 参考文献  \n  \n  \n<Bookmark href=\"https://gihyo.jp/book/2017/978-4-7741-8930-7\" />\n  \n  \n⚠️注意  \n  \n  \n2024年10月現在はで Apache Solr 9.7.0  まで出ていますが、本書は2017年5月出版で Apache Solar 6.3.0 までの情報しか含みません。適宜読み替える必要があります。  \n  \n","date":"2024-10-15T00:00:00.000Z"},{"slug":"test-style","title":"ブログ表示テスト用","tags":["tech","poem"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n# H1 heading  \n  \n  \n## H2 heading  \n  \n  \n### H3 heading  \n  \n- list1  \n- list2  \n- list3  \n1. number1  \n2. number2  \n3. number3  \n- ネスト1  \n\t- ネスト2  \n\t\t- ネスト3  \n  \n> this is quote message  \n  \n  \n```text  \nconst a = \"aaa\"  \nconst b = \"bbb\"  \nconst list = [a, b]  \n```  \n  \n  \nこれは**太文字**です  \n  \n  \nこれは`インラインコード`です  \n  \n  \nこれは_イタリック_です  \n  \n  \nこれは<u>アンダーライン</u>です  \n  \n  \nこれは<u>取り消し線</u>です  \n  \n- [ ] TODO1  \n- [ ] TODO2  \n- [x] TODO3  \n  \n以下はブックマーク  \n    \n<Bookmark href=\"https://zenn.dev/mizchi/articles/remix-cloudflare-pages-supabase\"/>\n  \n以下はリンク\n  \n[https://zenn.dev/mizchi/articles/remix-cloudflare-pages-supabase](https://zenn.dev/mizchi/articles/remix-cloudflare-pages-supabase)  \n  \n  \n以下はdivider  \n  \n---  \n  \n> 💡 これはcalloutです  \n  \nTwitter Embedテスト  \n  \n<TweetEmbed url=\"https://twitter.com/elonmusk/status/1834213015857889706\" />\n  \n  \nYouTube Embedテスト  \n  \n<YouTubeEmbed url=\"https://www.youtube.com/watch?v=CmDv7ww6ikU\" />  \n  \n本ブログのBookmark  \noepngraph-imageのテストも含む  \n  \n  \n<Bookmark href=\"https://sokes-nook.net/blog/next-web-push\" siteUrl=\"https://sokes-nook.net\" />\n\n  \n\n以下はpng画像\n![alt text](images/test-style/onepiece01_luffy.png)\n\n以下はjpg画像\n![alt text](images/test-style/onepiece01_luffy.jpg)\n\n以下はgif画像\n![alt text](images/test-style/dog-7011_256.gif)\n\n以下はsvg画像\n![alt text](images/test-style/SVGアイコン.svg)","date":"2024-08-13T15:00:00.000Z"},{"slug":"udemy-crafting-shell","title":"Pythonによる自作シェル講座の感想","tags":["tech","udemy","shell"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n## はじめに  \n  \n  \n以下講座を受講したのでその備忘録のメモ的な記事です。あまり読む価値はないと思います  \n  \n  \n[**自作シェルで学ぶLinuxシステムプログラミング―Pythonで150行の「シェル」を実装してLinuxのしくみを学ぼう**](https://www.udemy.com/course/crafting-shell/)  \n  \n  \n> コマンドを実行する「シェル」を自作しながら、Linux のしくみや OS に近い領域のプログラミングを学ぼう！「シェルって何？コマンドの実行って何？標準入出力って何？ターミナルって何？」といった疑問をシェルの実装レベルで解消しよう！  \n  \n  \n<Bookmark href=\"https://www.udemy.com/course/crafting-shell\" />\n  \n  \n## 講座の感想  \n  \n  \n実際に作ってみるととても簡単にシェルが出来上がってびっくりした。  \n  \n  \n「0, 1, 2にそれぞれ標準入力、標準出力、標準エラー出力がファイルディスクリプタとして割り当てられている」くらいの記憶はありましたが、実際にリダイレクトやハイプの簡易的な実装をこなうことその本質の理解に迫れたのが良かった。  \n  \n  \nアプリケーションレイヤーのプログラミングを行う際に直接役に立つことは少ないと思うが、知っておいて得しかない情報ばかりだった。  \n  \n  \n---  \n  \n  \n**以下、講座受講中のメモ**  \n  \n  \n## そもそもシェルとは  \n  \n  \nシェルはユーザーが入力したコマンドを読み取り、プロガラムを実行する、特殊なプログラム。  \n  \n  \nよってシェルの中心的な処理は以下の二つに分類できる  \n  \n- ユーザーの入力を読み込んで解析する  \n- 指定されたコマンド（プログラム）を実行する  \n  \nカーネルの機能  \n  \n- プロセス管理  \n\t- OSがプロセスを順番に割り当てて、複数のプロセスが同時に動いているかのように見せる（コアが一つなら同時に1つのプロセスしか動かせない）  \n\t- initプロセス → 全てのプロセスの親となるプロセス。systemd がよくinitプロセスと用いられる（dockerは一番親プロセスがinitプロセスではないので注意）  \n\t- initプロセスが sshdを起動しておいてくれるので、Bashなどのシェルが起動できる  \n- メモリ管理  \n- ファイルシステム  \n- etc..  \n  \n⇒ シェルはコマンドを実行する機能があるが、プロセスを新しく生成することができるのはカーネルだけなので、**シェルはプロセスの実行をカーネルに依頼することになるということ**。  \nこのプロセスの実行や、その他カーネルの処理を呼び出す処理を「**システムコール**」という。  \n  \n  \nPythonでシステムコールする場合は、`fork`,  `exec` という2つのシステムコールに対応したメソッドを利用する  \n  \n  \nシェルのコマンドの種類  \n  \n- 内部コマンド → echo, cd, pwdなどシェル内部で実装されたコマンド。typeコマンドでそのコマンドが内部か外部かを区別できる  \n  \n```typescript  \nvscode ➜ /workspaces/udemy-crafting-shell $ type echo  \necho is a shell builtin  \n```  \n  \n- 外部コマンド → 実行するプログラムのファイル名。プログラムの場所はPATHで指定する（フォルダが指定された場合、そのフォルダにある実行可能プログラムが全て外部コマンドとなる）。シェルはコマンドが内部コマンドでない場合、PATH一覧から該当するプログラムがないかを探す。cat, lsなどは外部コマンド  \n  \n```typescript  \nvscode ➜ /workspaces/udemy-crafting-shell $ type cat  \ncat is /usr/bin/cat  \n```  \n  \n  \n## linixシグナル  \n  \n  \nあるプロセスから別のプロセスに通信する方法の一種。  \n  \n- SIGINT  \nターミナルで Ctrl + C を入力すると、プロセスにSIGINTが送信される。プロセスのデフォルトの挙動として、SIGINTを受信するとプロセスの実行が終了することになっている。  \n- SIGTERM  \n- SIGKILL  \nカーネルが強制的にプロセスを停止するシグナル。プロセスはSIGKILLハンドラーを作成できないため、必ずプロセスを停止することができるシグナル。  \n**kill コマンドはこのSIGKILLをプロセスに送信している**  \n- etc…  \n  \nシグナルハンドリングを実装することでプロセスのデフォルトの挙動を修正することができる。  \n  \n  \n  \n## シェバン(shebang)  \n  \n  \nLinuxなどでスクリプトを実装する際、スクリプトの1行目に `#!<インタプリタのパス>` と書かれている部分。  \n  \n- #!/bin/bash  \n- #!/usr/bin/env python3  \n- …  \n  \n**Linuxにおけるシェバンの解釈**  \n  \n  \nデフォルトではコマンドは最終的にexecveシステムコールに渡される  \n`execve(”./script.sh”, [”./script.sh”], ….])`   \n  \n  \nシェバンを指定すると、execveではなく、シェバンに指定されたインタプリタにコマンドが渡されるようになる  \n`/bin/bash script.sh`   \n  \n  \n## 標準入出力とリダイレクト、パイプ  \n  \n  \nリダイレクトやパイプの機能を使うことで、標準入力、標準出力、標準エラー出力を切り替えることができる。  \n  \n  \nプロセスから見たファイルは、0, 1, 2, 3, 4 といった数字が割り当てられてる（プロセス内でopenしたファイルに数字が払い出される）この数字を**ファイルディスクリプタ**という。  \nプロセスがopenしているファイルは `ls -l proc/{PID}/fd` をみると確認できる。 `$$` には現在のPIDが格納されているので、 `ls -l proc/{PID}/fd` とすると現在のプロセスのファイルディスクリプタを確認できる。  \n  \n  \n**標準入力、標準出力、標準エラーはそれぞれ、0, 1, 2のファイルディスクリプタのこと。**  \n  \n  \n### リダイレクト  \n  \n  \n標準出力の切り替え: `>`  \n  \n  \n```typescript  \n$ pwd > pwd.output  \n```  \n  \n  \n標準エラー出力の切り替え `2>`:   \n  \n  \n```typescript  \n$ cat nofile.txt 2> error.output  \n```  \n  \n  \n### パイプ  \n  \n  \n前のコマンドの標準出力を、次のコマンドの標準入力に受け流すことができる  \n  \n  \n```typescript  \ncat /etc/os-release | grep VERSION  \n```  \n  \n  \n### /dev/pts/0 とういファイルはなんなのか？  \n  \n  \n前提として、以下ディレクトリには特殊なファイルシステムが使われている  \n  \n  \n| ディレクトリ | 配置されるファイル    |  \n| ------ | ------------ |  \n| /dev   | デバイスファイル     |  \n| /proc  | カーネルやプロセスの情報 |  \n| /sys   | カーネルやデバイスの情報 |  \n  \n  \n※ これらの情報はストレージ上に書き込まれているわけではない。OSのカーネル内のオペレーション上の情報をファイルと同じような扱いにしているだけ。**”Everything is a file” というLinuxの哲学の一つ。Linuxにおいて、「ファイルはデータを読み書きできるもの」くらい抽象的な存在である。**  \n  \n  \n## シェルとターミナルの違い  \n  \n  \n本来のターミナルは、コンピュータに接続して使う、ディスプレイとキーボードがくっついたハードウェアのことを指していた（この画面はいつも見てるターミナルの黒い画面のみということ）。  \n現在ではこの物理的なターミナルを使われることはなく、ソフトとして入っている「ターミナルエミュレータ」というソフトウェアをターミナルと呼んでいるということ。  \n  \n  \nターミナルは起動すると /dev/pts/0 に接続され、この /dev/pts/0 を介してシェルを扱っている。  \n  \n  \nよって、ターミナルは入力装置で、実際にプログラムを実行しているのはシェルという整理ができそう。  \n  \n  \nこの違いを意識すると、clearコマンドの原理を理解できる。  \n  \n","date":"2024-09-03T15:00:00.000Z"},{"slug":"unified-notion-bookmark","title":"unifiedを理解してnotion-to-mdのBookmarkをAST上から扱う","tags":["tech","markdown","unified"],"content":"\n  \n## はじめに  \n  \n本ブログは2024/09/14現在では、Next.js, contentlayer, markdownを用いて構築しています。\n記事はnotionで書いたものをnotion apiを定期実行で叩くことでmarkdownとして取得し保存しています。  \n  \nnotion apiからページ内容を取得しMarkdownに変換する処理は [notion-to-md](https://github.com/souvikinator/notion-to-md) を用いて行なっています。  \n\nこちらライブラリでは、notionのBookmarkを以下のようなmdに変換します。  \n  \n```markdown  \n[bookmark](https://zenn.dev/januswel/articles/e4f979b875298e372070)   \n```  \n\nデフォルトのmarkdownレンダラーを利用する場合、リンクテキストが”bookmark”のリンクとして解釈されてしまいます。  \n\nこちらのbookmarkを独自に解釈してブックマークとして表示する unifiedプラグインを作成したので紹介します。  \n\n\n## unifiedについて\nmarkdownを自分の思った通りに描画しようとした場合、[unified](https://github.com/unifiedjs/unified)というエコシステムを利用することが不可避でした。  \n  \nunified では mdast（Markdown の AST）を構築し、hast （HTML の AST）に変換して、htmlを出力するという手順で動いており、ASTの扱いを把握する必要がありました。  \n  \nunifiedの理解については [@janus_wel](https://x.com/janus_wel)さんのzennにおける以下連載がとても参考になりました。  \n  \n- [**unified を使う前準備**](https://zenn.dev/januswel/articles/e4f979b875298e372070)  \n- [**unified におけるプラグインまとめ**](https://zenn.dev/januswel/articles/44801708e8c7fdd358e6)  \n- [**unified を使って Markdown を拡張する**](https://zenn.dev/januswel/articles/745787422d425b01e0c1)  \n- [**unified を使ってオレオレ Markdown を ReactElement に変換する**](https://zenn.dev/januswel/articles/c0e663c88b562bfde8ff)  \n  \nこちらで解説されている内容は前提として本編が構成されているため、前もっての一読をお勧めします。  \n  \nまた、qiitaの以下記事もわかりやすかったのでお勧めです。  \n  \n[**Next.js のための Remark / Rehype 入門**](https://qiita.com/sankentou/items/f8eadb5722f3b39bbbf8)  \n  \n## 実装  \n  \n  \n```typescript  \nimport { unified } from \"unified\";  \nimport remarkParse from \"remark-parse\";  \nimport remark2rehype from \"remark-rehype\";  \nimport rehypeStringify from \"rehype-stringify\";  \nimport { Node, Parent } from \"unist\";  \nimport { VFileCompatible } from \"vfile\";  \nimport { visit } from \"unist-util-visit\";  \nimport { Paragraph } from \"mdast\";  \nimport { Handler } from \"mdast-util-to-hast\";  \n  \nconst testMdString = `  \n# test markdown  \n  \n以下はnotionのブックマーク    \n  \n[bookmark](https://zenn.dev/mizchi/articles/remix-cloudflare-pages-supabase)    \n    \n`;  \n  \nfunction isObject(target: unknown): target is { [key: string]: unknown } {  \n  return typeof target === \"object\" && target !== null;  \n}  \n  \n// https://github.com/syntax-tree/unist#parent  \nfunction isParent(node: unknown): node is Parent {  \n  return isObject(node) && Array.isArray(node.children);  \n}  \n  \nconst notionBookmarkPlugin = () => {  \n  return (tree: Node, _file: VFileCompatible) => {  \n    visit(  \n      tree,  \n      \"paragraph\",  \n      (node: Paragraph, index: number, parent: Parent | undefined) => {  \n        if (!isParent(parent)) {  \n          return;  \n        }  \n  \n        if (node.children[0].type === \"link\") {  \n          const linkNode = node.children[0];  \n          if (linkNode.children[0].type === \"text\") {  \n            const textNode = linkNode.children[0];  \n            if (textNode.value === \"bookmark\") {  \n              // my-bookmarkという新たなtypeのnodeを作成する  \n              parent.children[index] = {  \n                type: \"my-bookmark\",  \n                //@ts-ignore  \n                children: [{ type: \"text\", value: linkNode.url }],  \n                data: { linkUrl: linkNode.url },  \n              };  \n            }  \n          }  \n        }  \n      }  \n    );  \n  };  \n};  \n  \n// @ts-ignore  \n// my-bookmark typeのnodeをhtmlへ変換する際のhandler  \nconst myBookmarkHandler: Handler = (state, node, parent) => {  \n  return {  \n    type: \"element\",  \n    tagName: \"a\",  \n    properties: {  \n      className: \"\",  \n      href: node.data.linkUrl,  \n      target: \"_blank\",  \n      rel: \"noreferrer\",  \n    },  \n    children: [  \n      {  \n        type: \"element\",  \n        tagName: \"div\",  \n        properties: {  \n          className: \"\",  \n        },  \n        children: [  \n          {  \n            type: \"element\",  \n            tagName: \"div\",  \n            properties: { className: \"\" },  \n            children: [  \n              {  \n                type: \"text\",  \n                value: \"BookMark時の内容\",  \n              },  \n            ],  \n          },  \n        ],  \n      },  \n    ],  \n  };  \n};  \n  \nconst processor = unified()  \n  .use(remarkParse) // parse: markdown to mdast  \n  .use(notionBookmarkPlugin)  \n  // @ts-ignore  \n  .use(remark2rehype, {  \n    handlers: {  \n      \"my-bookmark\": myBookmarkHandler,  \n    },  \n  }) // mdast to hast  \n  .use(rehypeStringify); // compile: hast to html  \n  \nconst Unified: React.FC = async () => {  \n  const htmlString = processor.processSync(testMdString).toString();  \n  return (  \n    <div className=\"prose\" dangerouslySetInnerHTML={{ __html: htmlString }} />  \n  );  \n};  \n  \nexport default Unified;  \n  \n```  \n  \n  \n## 感想  \n  \n  \n正直、`[bookmark]` で始まるmdリンクをif文で条件分岐するだけで良かったのですが、unifiedが面白そうだったので試してみた感じです。実務だったらif文で書けって怒られそう。  \n  \n  \n以下動画を参考にすると、asyncのremarkPlugin作成もできそう？（未検証）  \nasyncが使えるとpluginの自由度がさらに上がるので気になる方はぜひ。  \n  \n<YouTubeEmbed url=\"https://www.youtube.com/watch?v=NmXw8yMTjys&t=978s&ab_channel=KentC.Dodds\" />\n  \n  \n## 余談  \n  \n  \nほとんどのプログラミング言語の処理系でもASTを利用されており、ASTについてのアレルギーをなくしておくと、言語を深く理解したくなったときや言語レベルでのデバッグの際に役立ちそうだなーと感じました。  \n  \n  \n```text  \n言語処理系は、大きく分けて、次のような部分からなる。  \n  \n1. 字句解析(lexical analysis): 文字列を言語の要素（トークン、token）の列に分解する。  \n2. 構文解析(syntax analysis): token列を意味を反映した構造に変換。こ の構造は、しばしば、木構造で表現されるので、抽象構文木（abstract syntax tree）と呼ばれる。ここまでの言語を認識する部分を言語のparserと 呼ぶ。  \n3. 意味解析(semantics analysis): 構文木の意味を解析する。インタプリ ターでは、ここで意味を解析し、それに対応した動作を行う。コンパイラでは、 この段階で内部的なコード、中間コードに変換する。  \n4. 最適化(code optimization): 中間コードを変形して、効率のよいプログ ラムに変換する。  \n5. コード生成(code generation): 内部コードをオブジェクトプログラムの 言語に変換し、出力する。例えば、ここで、中間コードよりターゲットの計算 機のアセンブリ言語に変換する。  \n```  \n  \n  \n参考: [https://www.hpcs.cs.tsukuba.ac.jp/~msato/lecture-note/comp-lecture/note1.html](https://www.hpcs.cs.tsukuba.ac.jp/~msato/lecture-note/comp-lecture/note1.html)  \n  \n  \nAST Explorer（[https://astexplorer.net/](https://astexplorer.net/) ）では、さまざまな言語がどのようにASTになるかを眺めることができて面白いです。  \n  \n\n以下はmarkdownをParser: [remark](https://remark.js.org/) でパースして生成されたASTの例です。  \n  \n![alt text](images/unified-notion-bookmark/astexplorer.png)    ","date":"2024-09-14T00:00:00.000Z"},{"slug":"uv-with-vscode","title":"Pythonプロジェクトにuvを導入 with VSCode","tags":["tech","python","uv","VSCode"],"content":"import { Bookmark } from \"../../components/Bookmark\";\n  \n巷で話題の[uv](https://docs.astral.sh/uv/)を用いてVSCodeの開発環境を構築した記録です。  \n  \n  \nuv でモジュールをインストールした後、モジュールのパラメータ予測、ソースコード参照が可能な環境を作成するための手順が少なかったので記事にしました。  \n  \n  \n## uvとは  \nRust で書かれた非常に高速な Python パッケージおよびプロジェクトマネージャーです。  \n  \n<Bookmark href=\"https://docs.astral.sh/uv/\" />\n  \n  \n公式Doc以外にもuvについては話題になっていた以下の記事群を参照すると良いと思います。  \n  \n  \n<Bookmark href=\"https://roboin.io/article/2024/02/19/rust-based-python-package-manager-uv/\" />\n  \n  \n<Bookmark href=\"https://gihyo.jp/article/2024/03/monthly-python-2403\" />\n  \n  \n<Bookmark href=\"https://qiita.com/kissy24/items/0c091bb5f12d697131ae\" />\n  \n  \n## プロジェクトへのuvの導入  \n\n```shell  \n❯ cd path/to/your/project  \n❯ uv init  \nInitialized project `your-project-name`  \n// .venvの生成するため、syncを実行する  \n❯ uv sync  \nCreating virtual environment at: .venv  \n❯ tree -L 1 -a  \n.  \n├── .env  \n├── .gitignore  \n├── .python-version  \n├── .venv  \n├── README.md  \n├── hello.py  \n├── pyproject.toml  \n└── uv.lock  \n  \n  \n// 任意のモジュールをプロジェクトに追加する  \n❯ uv add apache-airflow  \n```  \n  \n## VSCode上でuv環境を指定する  \n  \n  \nVSCodeの画面の左下にあるPythonのバージョンを表す数字の部分をクリックし、「インタープリターのパスを選択」を選択。  \n  \n![alt text](images/uv-with-vscode/vscode-py-version.png)    \n  \nuvによって作成さらたpythonバイナリのパス（your/path/to/airflow/.venv/bin/python3）を入力  \n  \n![alt text](images/uv-with-vscode/interpreter-path.png)  \n  \n上記操作を実施するとプロジェクトでuvのインタープリタが有効になり、モジュールが適切にimportされるようになります。  \n  \n  \n画像のようにパラメータ補完やcommand+クリックによるソース参照機能が有効になっていることを確認できると思います。  \n  \n![alt text](images/uv-with-vscode/vscode-code-completion.png)\n\n  \n本格的にやりたい場合、以下のdevcontainerを用いた環境構築をするのが良さそう。  \n  \n    \n<Bookmark href=\"https://zenn.dev/dena/articles/python_env_with_uv\" />\n  \n  \n## 【おまけ 】ruffを導入してコードフォーマットする  \n  \n  \nuvの開発元と同じ Astral 社が開発している Python 用の Formatter, Linter である[ ruff](https://github.com/astral-sh/ruff/tree/main)を導入して利用してみます。  \n  \n  \n```shell  \n❯ uv add --dev ruff  \n# 配下にあるpythonファイルをフォーマットする  \n❯ ruff format .  \n```  \n  \n  \nuv addの際に`--dev` オプションをつけることでdevelopment dependency groupに依存関係を追加することができます。  \n  \n  \n自分の環境では [lefthook](https://github.com/evilmartians/lefthook)のpre-commitのフックにruffを追加してコミット前にコードを整形するルールをプロジェクトに適用しています。  \n  \n","date":"2024-12-26T00:00:00.000Z"}]